@misc{hivemind,
  title = {{H}ivemind: a {L}ibrary for {D}ecentralized {D}eep {L}earning},
  author = {Learning{@}home team},
  year = 2020,
  howpublished = {\url{https://github.com/learning-at-home/hivemind}}
}

@inproceedings{ryabinin2020crowdsourced,
  title = {Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts},
  author = {Ryabinin, Max and Gusev, Anton},
  year = 2020,
  booktitle = {Advances in Neural Information Processing Systems},
  volume = 33,
  url = {https://proceedings.neurips.cc/paper/2020/file/25ddc0f8c9d3e22e03d3076f98d83cb2-Paper.pdf}
}

@inproceedings{ryabinin2021moshpit,
  title = {Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices},
  author = {Ryabinin, Max and Gorbunov, Eduard and Plokhotnyuk, Vsevolod and Pekhimenko, Gennady},
  year = 2021,
  booktitle = {Advances in Neural Information Processing Systems},
  volume = 34,
  url = {https://proceedings.neurips.cc/paper/2021/file/97275a23ca44226c9964043c8462be96-Paper.pdf}
}

@article{diskin2021distributed,
  title={Distributed Deep Learning In Open Collaborations},
  author={Diskin, Michael and Bukhtiyarov, Alexey and Ryabinin, Max and Saulnier, Lucile and Sinitsin, Anton and Popov, Dmitry and Pyrkin, Dmitry V and Kashirin, Maxim and Borzunov, Alexander and Villanova del Moral, Albert and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={7879--7897},
  year={2021}
}

@inproceedings{borzunov2022training,
  title={Training Transformers Together},
  author={Borzunov, Alexander and Ryabinin, Max and Dettmers, Tim and Lhoest, Quentin and Saulnier, Lucile and Diskin, Michael and Jernite, Yacine},
  booktitle={NeurIPS 2021 Competitions and Demonstrations Track},
  pages={335--342},
  year={2022},
  organization={PMLR}
}

@article{ryabinin2023swarm,
  title={SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient},
  author={Ryabinin, Max and Dettmers, Tim and Diskin, Michael and Borzunov, Alexander},
  journal={arXiv preprint arXiv:2301.11913},
  year={2023}
}

@inproceedings{li2014scaling,
  title={Scaling distributed machine learning with the parameter server},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={11th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 14)},
  pages={583--598},
  year={2014}
}

@article{lian2017can,
  title={Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent},
  author={Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{sergeev2018horovod,
  title={Horovod: fast and easy distributed deep learning in TensorFlow},
  author={Sergeev, Alexander and Del Balso, Mike},
  journal={arXiv preprint arXiv:1802.05799},
  year={2018}
}

@INPROCEEDINGS{lee2017deepspotcloud,
  author={Lee, Kyungyong and Son, Myungjun},
  booktitle={2017 IEEE 10th International Conference on Cloud Computing (CLOUD)}, 
  title={DeepSpotCloud: Leveraging Cross-Region GPU Spot Instances for Deep Learning}, 
  year={2017},
  volume={},
  number={},
  pages={98-105},
  doi={10.1109/CLOUD.2017.21}
}

@inproceedings {yang2023skypilot,
author = {Zongheng Yang and Zhanghao Wu and Michael Luo and Wei-Lin Chiang and Romil Bhardwaj and Woosuk Kwon and Siyuan Zhuang and Frank Sifei Luan and Gautam Mittal and Scott Shenker and Ion Stoica},
title = {{SkyPilot}: An Intercloud Broker for Sky Computing},
booktitle = {20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
year = {2023},
isbn = {978-1-939133-33-5},
address = {Boston, MA},
pages = {437--455},
url = {https://www.usenix.org/conference/nsdi23/presentation/yang-zongheng},
publisher = {USENIX Association},
month = apr,
}

@inproceedings{das2020sagemaker,
author = {Das, Piali and Ivkin, Nikita and Bansal, Tanya and Rouesnel, Laurence and Gautier, Philip and Karnin, Zohar and Dirac, Leo and Ramakrishnan, Lakshmi and Perunicic, Andre and Shcherbatyi, Iaroslav and Wu, Wilton and Zolic, Aida and Shen, Huibin and Ahmed, Amr and Winkelmolen, Fela and Miladinovic, Miroslav and Archembeau, Cedric and Tang, Alex and Dutt, Bhaskar and Grao, Patricia and Venkateswar, Kumar},
title = {Amazon SageMaker Autopilot: A White Box AutoML Solution at Scale},
year = {2020},
isbn = {9781450380232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399579.3399870},
doi = {10.1145/3399579.3399870},
booktitle = {Proceedings of the Fourth International Workshop on Data Management for End-to-End Machine Learning},
articleno = {2},
numpages = {7},
location = {Portland, OR, USA},
series = {DEEM'20}
}

@inproceedings{yang2022schedulingml,
author = {Yang, Sheng and Khuller, Samir and Choudhary, Sunav and Mitra, Subrata and Mahadik, Kanak},
title = {Scheduling ML Training on Unreliable Spot Instances},
year = {2022},
isbn = {9781450391634},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492323.3495594},
doi = {10.1145/3492323.3495594},
booktitle = {Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing Companion},
articleno = {29},
numpages = {8},
keywords = {scheduling, spot instances, machine learning training},
location = {Leicester, United Kingdom},
series = {UCC '21}
}

@article{elster2022nvidia,
  title={Nvidia hopper gpu and grace cpu highlights},
  author={Elster, Anne C and Haugdahl, Tor A},
  journal={Computing in Science \& Engineering},
  volume={24},
  number={2},
  pages={95--100},
  year={2022},
  publisher={IEEE}
}

@manual{lambdaweb,
  title = {{LambdaLabs}},
  month = {May},
  year = {2023},
  note = {Accessed: 19 May 2023, \url{lambdalabs.com}}
}

@manual{gcweb,
  title = {{Google Cloud}},
  month = {May},
  year = {2023},
  note = {Accessed: 19 May 2023, \url{cloud.google.com}}
}

@manual{awsweb,
  title = {{Amazon AWS}},
  month = {May},
  year = {2023},
  note = {Accessed: 19 May 2023, \url{aws.amazon.com}}
}

@manual{azureweb,
  title = {{Microsoft Azure}},
  month = {May},
  year = {2023},
  note = {Accessed: 19 May 2023, \url{portal.azure.com}}
}

@manual{vastaiweb,
  title = {{vast.ai}},
  month = {May},
  year = {2023},
  note = {Accessed: 19 May 2023, \url{vast.ai}}
}

@article{portella2019statistical,
  title={Statistical analysis of Amazon EC2 cloud pricing models},
  author={Portella, Gustavo and Rodrigues, Genaina N and Nakano, Eduardo and Melo, Alba CMA},
  journal={Concurrency and Computation: Practice and Experience},
  volume={31},
  number={18},
  pages={e4451},
  year={2019},
  publisher={Wiley Online Library}
}

@article{mattson2020mlperf,
  title={Mlperf training benchmark},
  author={Mattson, Peter and Cheng, Christine and Diamos, Gregory and Coleman, Cody and Micikevicius, Paulius and Patterson, David and Tang, Hanlin and Wei, Gu-Yeon and Bailis, Peter and Bittorf, Victor and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={336--349},
  year={2020}
}

@article{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@ONLINE{wikidump,
    author = "Wikimedia Foundation",
    title  = "Wikimedia Downloads",
    url    = "https://dumps.wikimedia.org",
    month = {May},
    year = {2023},
}

@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11976--11986},
  year={2022}
}

@inproceedings{he2015deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@misc{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{conneau2020unsupervised,
      title={Unsupervised Cross-lingual Representation Learning at Scale}, 
      author={Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and Francisco Guzm√°n and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov},
      year={2020},
      eprint={1911.02116},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@INPROCEEDINGS{aizman2019webdataset,
  author={Aizman, Alex and Maltby, Gavin and Breuel, Thomas},
  booktitle={2019 IEEE International Conference on Big Data (Big Data)}, 
  title={High Performance I/O For Large Scale Deep Learning}, 
  year={2019},
  volume={},
  number={},
  pages={5965-5967},
  doi={10.1109/BigData47090.2019.9005703}
}

@misc{ren2021zerooffload,
      title={ZeRO-Offload: Democratizing Billion-Scale Model Training}, 
      author={Jie Ren and Samyam Rajbhandari and Reza Yazdani Aminabadi and Olatunji Ruwase and Shuangyan Yang and Minjia Zhang and Dong Li and Yuxiong He},
      year={2021},
      eprint={2101.06840},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@book{10.5555/541880,
author = {Hwang, Kai},
title = {Advanced Computer Architecture: Parallelism,Scalability,Programmability},
year = {1992},
isbn = {0070316228},
publisher = {McGraw-Hill Higher Education},
edition = {1st},
abstract = {From the Publisher:This book deals with advanced computer architecture and parallel programming techniques. The material is suitable for use as a textbook in a one-semester graduate or senior course,offered by Computer Science,Computer Engineering,Electrical Engineering,or Industrial Engineering programs.}
}

@misc{rfc1072,
    series =    {Request for Comments},
    number =    1072,
    howpublished =  {RFC 1072},
    publisher = {RFC Editor},
    doi =       {10.17487/RFC1072},
    url =       {https://www.rfc-editor.org/info/rfc1072},
        author =    {},
    title =     {{TCP extensions for long-delay paths}},
    pagetotal = 16,
    year =      1988,
    month =     oct,
    abstract =  {This RFC proposes a set of extensions to the TCP protocol to provide efficient operation over a path with a high bandwidth*delay product. These extensions are not proposed as an Internet standard at this time. Instead, they are intended as a basis for further experimentation and research on transport protocol performance.},
}

@article{wortsman2023stable,
  title={Stable and low-precision training for large-scale vision-language models},
  author={Wortsman, Mitchell and Dettmers, Tim and Zettlemoyer, Luke and Morcos, Ari and Farhadi, Ali and Schmidt, Ludwig},
  journal={arXiv preprint arXiv:2304.13013},
  year={2023}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{li2020pytorch,
  title={Pytorch distributed: Experiences on accelerating data parallel training},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
  journal={arXiv preprint arXiv:2006.15704},
  year={2020}
}

@inproceedings{rasley2020deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}

@incollection{maymounkov2002kademlia,
  title={Kademlia: A peer-to-peer information system based on the xor metric},
  author={Maymounkov, Petar and Mazieres, David},
  booktitle={Peer-to-Peer Systems: First InternationalWorkshop, IPTPS 2002 Cambridge, MA, USA, March 7--8, 2002 Revised Papers},
  pages={53--65},
  year={2002},
  publisher={Springer}
}

@misc{dettmers20168bit,
      title={8-Bit Approximations for Parallelism in Deep Learning}, 
      author={Tim Dettmers},
      year={2016},
      eprint={1511.04561},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@INPROCEEDINGS{9975369,
  author={Lee, Sungjae and Hwang, Jaeil and Lee, Kyungyong},
  booktitle={2022 IEEE International Symposium on Workload Characterization (IISWC)}, 
  title={SpotLake: Diverse Spot Instance Dataset Archive Service}, 
  year={2022},
  volume={},
  number={},
  pages={242-255},
  doi={10.1109/IISWC55918.2022.00029}
}