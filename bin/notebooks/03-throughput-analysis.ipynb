{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "29dce416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG_PATH:../../artifacts/wandb\n",
      "NETWORK_LOG_PATH:../../artifacts/networking/logs\n",
      "FIGURE_PATH:../../paper/figures\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "import itertools\n",
    "wandb.login()\n",
    "pd.set_option('display.max_rows', 200)\n",
    "plotting_context = \"paper\"\n",
    "default_palette = \"colorblind\"\n",
    "#granularity_palette = sns.color_palette('coolwarm', 2)\n",
    "granularity_palette = sns.color_palette([\"#3fad82\", \"#B28ECF\"], 2)\n",
    "local_sps_palette = sns.color_palette(\"Set2\", 3)\n",
    "cv_palette = sns.color_palette(\"YlOrRd\", 5)\n",
    "nlp_palette = sns.color_palette(\"YlGnBu\", 2)\n",
    "nlp_ext_palette = sns.color_palette(\"YlGnBu\", 4)\n",
    "cv_nlp_palette = sns.color_palette([\"#c90823\", \"#225da8\"], 2)\n",
    "font_scale = 1.5\n",
    "sns.set(font_scale=font_scale, context=plotting_context)\n",
    "sns.set(rc={\"figure.dpi\":300, 'savefig.dpi':300})\n",
    "WANDB_ENTITY = \"kubework\"\n",
    "WANDB_PROJECT = \"paper-2023\"\n",
    "\n",
    "def get_env(var):\n",
    "    val = os.getenv(var)\n",
    "    if val == None:\n",
    "        raise ValueError(f\"Environment variable {var} is not set. Please follow the README.md instructions and start the notebook with all environment variables set.\")\n",
    "    else:\n",
    "        print(f\"{var}:{val}\")\n",
    "    return val\n",
    "\n",
    "LOG_PATH=get_env(\"LOG_PATH\")\n",
    "NETWORK_PATH=get_env(\"NETWORK_LOG_PATH\")\n",
    "FIGURE_PATH=get_env(\"FIGURE_PATH\")\n",
    "\n",
    "def save_figure(name, local_fig_dir, file_type='pdf'):\n",
    "    '''Save matplotlib figures at a local directory based on the full hardcoded path\n",
    "    :param name: str - name of the pdf figure (e.g., \"throughput\")\n",
    "    :param local_fig_dir: str - plots are saved under a subdirectory based on the pipeline (e.g., \"image-pipeline\")\n",
    "    '''\n",
    "    figure_path   = FIGURE_PATH\n",
    "    full_fig_dir  = figure_path + \"/\" + local_fig_dir\n",
    "    full_fig_path = full_fig_dir + \"/\" + name + \".\" + file_type\n",
    "    dpi = 300\n",
    "    plt.savefig(full_fig_path, dpi=dpi, bbox_inches = \"tight\")\n",
    "    \n",
    "def save_fig(name, file_type='pdf'):\n",
    "    save_figure(name, local_fig_dir=\"misc\", file_type=file_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7a276d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "def get_runs(run_name: str):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    entity = WANDB_ENTITY\n",
    "    project = WANDB_PROJECT\n",
    "    path = entity + \"/\" + project\n",
    "    key = \"config.run_name\"\n",
    "    filters = { key: run_name }\n",
    "    return api.runs(path=path, filters=filters)\n",
    "\n",
    "def get_history_non_cached(run, keys):\n",
    "    history = run.scan_history(keys=keys)\n",
    "    output = []\n",
    "    for row in history:\n",
    "        row_output = {}\n",
    "        for key in keys:\n",
    "            row_output[key] = row[key]\n",
    "        output.append(row_output)\n",
    "    return pd.DataFrame(output)\n",
    "\n",
    "def get_history(run, cache=True):\n",
    "    run_name = run.config.get(\"run_name\")\n",
    "    host_name = run.config.get(\"host\")\n",
    "    history_run_path = f\"{LOG_PATH}/{run_name}-{host_name}.csv\"\n",
    "    if cache and os.path.isfile(history_run_path):\n",
    "        return pd.read_csv(history_run_path)\n",
    "    else:\n",
    "        history = run.scan_history()\n",
    "        dataframe_content = []\n",
    "        for row in history:\n",
    "            dataframe_content.append(row)\n",
    "        df = pd.DataFrame(dataframe_content)\n",
    "        df.to_csv(history_run_path, index=False)\n",
    "        return df\n",
    "\n",
    "def get_run_by_prefix(runs, prefix):\n",
    "    prefixed_runs = []\n",
    "    for run in runs:\n",
    "        if run.name.startswith(prefix):\n",
    "            prefixed_runs.append(run)\n",
    "    print(f\"> Found {len(prefixed_runs)} runs with the prefix '{prefix}'.\")\n",
    "    return prefixed_runs\n",
    "\n",
    "def get_run_by_infix(runs, infix):\n",
    "    infixed_runs = []\n",
    "    for run in runs:\n",
    "        if infix in run.name:\n",
    "            infixed_runs.append(run)\n",
    "    print(f\"> Found {len(infixed_runs)} runs with the infix '{infix}'.\")\n",
    "    return infixed_runs\n",
    "\n",
    "def get_clean_history(run, filter_by=\"hivemind\", cache=True):\n",
    "    \"\"\"Tries to fix the two async wandb threads that logged together.\n",
    "    :filter_by:\n",
    "        - \"hivemind\": tries to fix the history\n",
    "        - \"hardware\": returns the history without fixing, as we don't need perfect sync here\n",
    "    \"\"\"\n",
    "    history_df = get_history(run, cache=cache)\n",
    "    if \"baseline\" in run.config.get(\"run_name\"):\n",
    "        additional_keys = ['01_general/step_based_sps']\n",
    "    else:\n",
    "        additional_keys = ['03_hivemind/num_peers',\n",
    "                           '03_hivemind/global_epoch']\n",
    "    if filter_by == \"hivemind\":\n",
    "        keys = ['01_general/locally_processed_samples',\n",
    "                '01_general/lr',\n",
    "                '01_general/step',\n",
    "                #'01_general/minibatch_loss', # loss.item() was too often None for NLP baseline, non NaN\n",
    "                '01_general/dataset_iteration_count',\n",
    "                '02_timing/dataload_time_s',\n",
    "                '02_timing/dataload_cuda_move_time_s',\n",
    "                '02_timing/forward_time_s',\n",
    "                '02_timing/backward_time_s',\n",
    "                '02_timing/loss_calc_time_s',\n",
    "                '02_timing/opt_step_time_s',\n",
    "                '02_timing/step_time_s', \n",
    "                '_step',\n",
    "                '_runtime',\n",
    "                ] + additional_keys\n",
    "        filtered_history_df = history_df[keys]\n",
    "        return merge_rows(df=filtered_history_df, ignore_cols=['_step','_runtime'])\n",
    "    elif filter_by == \"hardware\":\n",
    "        keys = [\n",
    "            'memory/total_memory_sys_MB',\n",
    "            'memory/used_memory_sys_MB',\n",
    "            'memory/available_memory_sys_MB',\n",
    "            'memory/used_memory_sys_percent',\n",
    "            'process/memory/virtual_memory_size_proc_MB',\n",
    "            'process/memory/text_resident_set_proc_MB', \n",
    "            'process/memory/shared_memory_proc_MB',\n",
    "            'process/memory/dirty_pages_proc_count',\n",
    "            'process/memory/data_resident_set_proc_MB',\n",
    "            'process/memory/resident_set_size_proc_MB',\n",
    "            'process/memory/lib_memory_proc_MB',\n",
    "            'process/voluntary_proc_ctx_switches',\n",
    "            'process/involuntary_proc_ctx_switches',\n",
    "            'bandwidth/disk_read_sys_bandwidth_MBs',\n",
    "            'bandwidth/disk_write_sys_bandwidth_MBs',\n",
    "            'bandwidth/net_sent_sys_bandwidth_Mbits',\n",
    "            'bandwidth/net_recv_sys_bandwidth_Mbits',\n",
    "            'disk/time/disk_busy_time_sys_s',\n",
    "            'disk/time/disk_write_time_sys_s',\n",
    "            'disk/time/disk_read_time_sys_s',\n",
    "            'disk/counter/disk_read_sys_count',\n",
    "            'disk/counter/disk_write_sys_count',\n",
    "            'cpu/interrupts/ctx_switches_count',\n",
    "            'cpu/interrupts/soft_interrupts_count',\n",
    "            'cpu/logical_core_count',\n",
    "            'cpu/interrupts/interrupts_count',\n",
    "            'cpu/load/avg_sys_load_one_min_percent',\n",
    "            'cpu/load/avg_sys_load_five_min_percent',\n",
    "            'cpu/load/avg_sys_load_fifteen_min_percent',\n",
    "            '_step',\n",
    "            '_runtime',\n",
    "        ]\n",
    "        return history_df[keys]\n",
    "    else:\n",
    "        raise ValueError(f\"filter_by: {filter_by} not known.\")\n",
    "\n",
    "def merge_rows(df, ignore_cols):\n",
    "    \"\"\"Merges rows with interleaved NaNs.\n",
    "    rowA = [1,     2, NaN,   4, NaN]\n",
    "    rowB = [NaN, NaN,   3, NaN,   5]\n",
    "    => result [1, 2, 3, 4, 5]\n",
    "    \n",
    "    :df: pd.DataFrame to be merged\n",
    "    :ignore_cols: list(), keys in this list are merged by using the max(valA, valB) value, e.g. \"_step\"\n",
    "    :returns: pd.DataFrame which is merged\n",
    "    \"\"\"\n",
    "    # 1. Drop all full NaN rows\n",
    "    temp_df = df.dropna(how=\"all\")\n",
    "    # 2. Iterate over all the rows by keeping a temp row which is filled successivley\n",
    "    clean_rows = []\n",
    "    prev_row = None\n",
    "    cleaned_row_counter = 0\n",
    "    for ix, row in temp_df.iterrows():\n",
    "        row_dict = dict(row)\n",
    "        # 2.1 If the temporary row is commited, start with the new row\n",
    "        if prev_row == None:\n",
    "            # 2.1.1 If the new row has NaN values, set it \"to be completed\"\n",
    "            if any_values_are_nan(row_dict):\n",
    "                prev_row = row_dict\n",
    "            else:\n",
    "            # 2.1.2 If the new row is complete, commit it\n",
    "                clean_rows.append(row_dict)\n",
    "        # 2.2 If we have a row to be completed, try to fill the values with the next row\n",
    "        else:\n",
    "            temp_row = merge_dicts(dictA=prev_row, dictB=row, ignore_cols=ignore_cols)\n",
    "            # 2.2.1 If there are still NaNs, keep it for next processing\n",
    "            if any_values_are_nan(temp_row):\n",
    "                prev_row = temp_row\n",
    "            # 2.2.2 If no NaNs exist anymore, commit and prepare for next row\n",
    "            else:\n",
    "                clean_rows.append(temp_row)\n",
    "                prev_row = None\n",
    "    return pd.DataFrame(clean_rows)\n",
    "\n",
    "def merge_dicts(dictA, dictB, ignore_cols):\n",
    "    \"\"\"Merges two dicts together by filling the NaN values.\n",
    "    If not in :ignore_cols: and both values are non NaN, an exception is thrown.\n",
    "    Both dicts have the same keys.\n",
    "    :dictA: dict()\n",
    "    :dictB: dict()\n",
    "    :ignore_cols: list(), the value of these keys will be decided by a max(a,b)\n",
    "    :returns: dict()\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for key in dictA:     \n",
    "        if key in ignore_cols and \\\n",
    "            not math.isnan(dictA[key]) and \\\n",
    "            not math.isnan(dictB[key]):\n",
    "            result[key] = max(dictA[key], dictB[key])\n",
    "        else:\n",
    "            try:\n",
    "                result[key] = return_non_nan(dictA[key], dictB[key])\n",
    "            except ValueError:\n",
    "                print(f\"Values ({dictA[key]}) and ({dictB[key]}) for key ({key}) are neither NaN, picking max.\")\n",
    "                result[key] = max(dictA[key], dictB[key])\n",
    "    return result\n",
    "\n",
    "def any_values_are_nan(dictionary):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return any([math.isnan(dictionary[key]) for key in dictionary])\n",
    "        \n",
    "def return_non_nan(a, b):\n",
    "    if math.isnan(a) and math.isnan(b):\n",
    "        return a\n",
    "    if math.isnan(a) and not math.isnan(b):\n",
    "        return b\n",
    "    if not math.isnan(a) and math.isnan(b):\n",
    "        return a\n",
    "    if not math.isnan(a) and not math.isnan(b):\n",
    "        raise ValueError(f\"Tuple (a,b) of ({a}, {b}) are neither NaN.\")\n",
    "\n",
    "def round_bar_value_multi(ax, decimals=2):\n",
    "    for containers in ax.containers:\n",
    "        for container in containers:\n",
    "            container.set_height(round(container.get_height(), decimals))\n",
    "        ax.bar_label(containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "744e41e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_runs(run_names):\n",
    "    baseline_runs = [get_runs(run_name=run_name)[0] for run_name in run_names]\n",
    "    baseline_dfs = []\n",
    "    keys = [\"01_general/step_based_sps\"]\n",
    "    for run in baseline_runs:\n",
    "        baseline_df = get_clean_history(run = run, filter_by=\"hivemind\")\n",
    "        baseline_df = baseline_df[keys]\n",
    "        baseline_df[\"name\"] = \"-\".join(run.config.get(\"run_name\").split(\".\")[0].split(\"-\")[:2])\n",
    "        if \"torchvision\" in run.config.get(\"run_name\"):\n",
    "            baseline_df[\"model\"] = run.config.get(\"run_name\").split(\".\")[-1]\n",
    "        if \"roberta\" in run.config.get(\"run_name\"):\n",
    "            baseline_df[\"model\"] = run.config.get(\"run_name\").split(\"-\")[-1]\n",
    "        baseline_df[\"TBS\"] = run.config.get(\"batch_size_per_step\") * run.config.get(\"gradient_accumulation_steps\")\n",
    "        baseline_df = baseline_df.rename(columns={\"01_general/step_based_sps\": \"samples_per_sec\"})\n",
    "        baseline_dfs.append(baseline_df)\n",
    "    return pd.concat(objs=baseline_dfs)\n",
    "\n",
    "def get_hivemind_runs(run_names, drop_first_epoch=True):\n",
    "    \n",
    "    hivemind_176_add = False\n",
    "    # adding manually as W&B API does not see this run because it was deleted and restored\n",
    "    if \"hivemind-176\" in run_names:\n",
    "        run_names.remove(\"hivemind-176\")\n",
    "        hivemind_176_add = True\n",
    "        \n",
    "    hivemind_runs = [get_run_by_prefix(\n",
    "                        runs = get_runs(run_name = run_name),\n",
    "                        prefix = \"trainmonitor\")[0]\n",
    "                     for run_name in run_names]\n",
    "    if hivemind_176_add:\n",
    "        # trainmonitor-176\n",
    "        hivemind_runs.append(api.run(path=\"kubework/paper-2023/1e4zlzpd\"))\n",
    "    hivemind_dfs = []\n",
    "    \n",
    "    keys = [\"03_hivemind/optimistic_total_samples_per_second\",\n",
    "            \"03_hivemind/epoch_based_sps\"]\n",
    "    \n",
    "    for run in hivemind_runs:\n",
    "        hivemind_df = get_history(run = run)\n",
    "        hivemind_df = hivemind_df[keys]\n",
    "        hivemind_df[\"name\"] = run.config.get(\"run_name\")\n",
    "        hivemind_df[\"model\"] = run.config.get(\"model\").split(\".\")[-1]\n",
    "        hivemind_df[\"TBS\"] = run.config.get(\"target_batch_size\")\n",
    "        hivemind_df = hivemind_df.rename(columns={\"03_hivemind/epoch_based_sps\": \"samples_per_sec\"})\n",
    "        hivemind_df = hivemind_df.rename(columns={\"03_hivemind/optimistic_total_samples_per_second\": \"local samples_per_sec\"})\n",
    "        if drop_first_epoch:\n",
    "            # get index of the first epoch value\n",
    "            first_index = hivemind_df.query(\"samples_per_sec.notna()\").index[0]\n",
    "            # get index of last epoch value\n",
    "            last_index = hivemind_df.query(\"samples_per_sec.notna()\").index[-1]\n",
    "            # start the dataframe from the next value\n",
    "            hivemind_df = hivemind_df[first_index+1:last_index+1]\n",
    "        hivemind_dfs.append(hivemind_df)\n",
    "    return pd.concat(objs=hivemind_dfs)\n",
    "\n",
    "def rename_models(df):\n",
    "    df = df.replace(\"resnet18\", \"ResNet18\")\n",
    "    df = df.replace(\"resnet50\", \"ResNet50\")\n",
    "    df = df.replace(\"resnet152\", \"ResNet152\")\n",
    "    df = df.replace(\"wide_resnet101_2\", \"WideResNet101_2\")\n",
    "    df = df.replace(\"convnext_large\", \"ConvNextLarge\")\n",
    "    df = df.replace(\"roberta_mlm_base\", \"RoBERTaBase\")\n",
    "    df = df.replace(\"roberta_mlm_large\", \"RoBERTaLarge\")\n",
    "    df = df.replace(\"roberta_mlm_xlm\", \"RoBERTaXLM\")\n",
    "    return df\n",
    "\n",
    "def compute_local_step_cutoff(df, debug=False):\n",
    "    \"\"\"We get a unimodal time series with 3 outliers (sync steps)\n",
    "    Returning a cutoff where all of the outliers are included.\n",
    "    \"\"\"\n",
    "    median = df.median()\n",
    "    std = df.std()\n",
    "    cutoff = median + 2*std # 95% of all data should be included here\n",
    "    if debug:\n",
    "        print(f\">> Found {len(df.index[df >= cutoff].tolist())} sync steps\")\n",
    "    return cutoff\n",
    "\n",
    "def get_aggregated_step_timings(df, agg = \"sum\", clean = True, debug=False):\n",
    "    \n",
    "    # 02_timing/dataload_time_s - unimodal with one startup outlier\n",
    "    # 02_timing/dataload_cuda_move_time_s - unimodal\n",
    "    # 02_timing/forward_time_s - unimodal\n",
    "    # 02_timing/loss_calc_time_s - unimodal\n",
    "    # 02_timing/backward_time_s - unimodal\n",
    "    # 02_timing/opt_step_time_s - bi/tri-modal\n",
    "    # 02_timing/step_time_s - bi/tri-modal\n",
    "    \n",
    "    agg_dict = {}\n",
    "    # opt_step_time_data.plot.hist()    \n",
    "    # opt.step() is basically a no-op\n",
    "    \n",
    "    # detect the outliers (sync-steps) and decide on a cutoff\n",
    "    # which includes all of them (the fastest sync step)\n",
    "    NON_LOCAL_STEP_CUTOFF_S = compute_local_step_cutoff(df[\"02_timing/opt_step_time_s\"], debug=debug)\n",
    "        \n",
    "    # exclude the first hivemind epoch from logging\n",
    "    if clean:\n",
    "        averaging_ix = df.index[df[\"02_timing/opt_step_time_s\"] >= NON_LOCAL_STEP_CUTOFF_S].tolist()\n",
    "        #assert(len(averaging_ix) >= 2), f\"At least 2x averaging indices should be included: {averaging_ix}\"\n",
    "        first_averaging_ix = averaging_ix[0] + 1 # starting from the next step\n",
    "        cleaned_df = df[first_averaging_ix:]\n",
    "    else:\n",
    "        cleaned_df = df\n",
    "    \n",
    "    opt_step_time_data = cleaned_df[\"02_timing/opt_step_time_s\"]\n",
    "    \n",
    "    if agg == \"sum\":\n",
    "        agg_dict[\"dataload_s\"] = cleaned_df[\"02_timing/dataload_time_s\"].sum()\n",
    "        agg_dict[\"cuda_move_s\"] = cleaned_df[\"02_timing/dataload_cuda_move_time_s\"].sum()\n",
    "        agg_dict[\"forward_s\"] = cleaned_df[\"02_timing/forward_time_s\"].sum()\n",
    "        agg_dict[\"loss_calc_s\"] = cleaned_df[\"02_timing/loss_calc_time_s\"].sum()\n",
    "        agg_dict[\"backward_s\"] = cleaned_df[\"02_timing/backward_time_s\"].sum()\n",
    "        agg_dict[\"opt_step_local_s\"] = opt_step_time_data[opt_step_time_data < NON_LOCAL_STEP_CUTOFF_S].sum()\n",
    "        agg_dict[\"opt_step_sync_s\"] = opt_step_time_data[opt_step_time_data >= NON_LOCAL_STEP_CUTOFF_S].sum()\n",
    "    elif agg == \"mean\":\n",
    "        agg_dict[\"dataload_s\"] = cleaned_df[\"02_timing/dataload_time_s\"].mean()\n",
    "        agg_dict[\"cuda_move_s\"] = cleaned_df[\"02_timing/dataload_cuda_move_time_s\"].mean()\n",
    "        agg_dict[\"forward_s\"] = cleaned_df[\"02_timing/forward_time_s\"].mean()\n",
    "        agg_dict[\"loss_calc_s\"] = cleaned_df[\"02_timing/loss_calc_time_s\"].mean()\n",
    "        agg_dict[\"backward_s\"] = cleaned_df[\"02_timing/backward_time_s\"].mean()\n",
    "        agg_dict[\"opt_step_local_s\"] = opt_step_time_data[opt_step_time_data < NON_LOCAL_STEP_CUTOFF_S].mean()\n",
    "        agg_dict[\"opt_step_sync_s\"] = opt_step_time_data[opt_step_time_data >= NON_LOCAL_STEP_CUTOFF_S].mean()\n",
    "         \n",
    "    # wrap in lists for pandas df creation\n",
    "    for key in agg_dict.keys(): agg_dict[key] = [agg_dict[key]]\n",
    "    \n",
    "    return pd.DataFrame(agg_dict)\n",
    "\n",
    "\n",
    "def hivemind_run_step_breakdown(run_name: str, agg = \"sum\", debug=False):\n",
    "    # filter all runs by workers\n",
    "    hivemind_runs = get_run_by_infix(\n",
    "        runs = get_runs(run_name = run_name),\n",
    "        infix = \"bee\")\n",
    "    \n",
    "    aggregated_dfs = []\n",
    "    \n",
    "    # aggregate gpu runs\n",
    "    for gpu_run in hivemind_runs:\n",
    "        gpu_agg_df = get_aggregated_step_timings(\n",
    "            df=get_clean_history(\n",
    "                run=gpu_run, filter_by=\"hivemind\"),\n",
    "            agg=agg,\n",
    "            debug=debug)\n",
    "        gpu_agg_df[\"name\"] = gpu_run.config.get(\"host\")\n",
    "        aggregated_dfs.append(gpu_agg_df)\n",
    "    \n",
    "    return pd.concat(objs=aggregated_dfs)\n",
    "\n",
    "def calc_granularity(df):\n",
    "    '''Higher is better'''\n",
    "    full_calc_time_key = \"dl_calc_time_s\"\n",
    "    clean_calc_time_key = \"calc_time_s\"\n",
    "    communication_time_key = \"comm_time_s\"\n",
    "    \n",
    "    df[full_calc_time_key] = df[\"dataload_s\"] + df[\"cuda_move_s\"] + df[\"forward_s\"] + \\\n",
    "                           df[\"loss_calc_s\"] + df[\"backward_s\"] + df[\"opt_step_local_s\"]\n",
    "    df[clean_calc_time_key] = df[\"cuda_move_s\"] + df[\"forward_s\"] + df[\"loss_calc_s\"] + \\\n",
    "                              df[\"backward_s\"] + df[\"opt_step_local_s\"]\n",
    "    df[communication_time_key] = df[\"opt_step_sync_s\"]\n",
    "    df[\"granularity\"] = round(df[clean_calc_time_key][0] / df[communication_time_key][0],2)\n",
    "    return df\n",
    "\n",
    "def hivemind_step_granularity(run_name, debug=False):\n",
    "    df = hivemind_run_step_breakdown(run_name=run_name, agg=\"sum\", debug=debug)\n",
    "    return calc_granularity(df=df)\n",
    "\n",
    "def plot_granularity_all_nodes(run_name):\n",
    "    sns.set(palette=default_palette)\n",
    "    temp_df = hivemind_step_granularity(run_name=run_name)\n",
    "    ax = temp_df[[\"calc_time_s\",\"comm_time_s\",\"name\"]].set_index('name').plot(kind='bar', stacked=True)\n",
    "    plt.ylabel(\"Time in Seconds\")\n",
    "    plt.xlabel(\"\")\n",
    "    \n",
    "def get_granularity_cumulated(run_name, name, debug=False, plot=False):\n",
    "    df = hivemind_step_granularity(run_name=run_name, debug=debug)\n",
    "    mean_df = df.mean(numeric_only=True).to_frame().T\n",
    "    mean_df[\"name\"] = name\n",
    "    if plot:\n",
    "        sns.set(palette=default_palette)\n",
    "        ax = mean_df[[\"calc_time_s\",\"comm_time_s\",\"name\"]].set_index('name').plot(kind='bar', stacked=True)\n",
    "        plt.ylabel(\"Time in Seconds\")\n",
    "        plt.xlabel(\"\")\n",
    "    return mean_df\n",
    "\n",
    "def transform_to_compare_local_sps(df):\n",
    "    local_sps_key = \"local samples_per_sec\"\n",
    "    global_sps_key = \"samples_per_sec\"\n",
    "    \n",
    "    temp_dict = {\n",
    "        \"name\": df[\"name\"],\n",
    "        \"model\": df[\"model\"],\n",
    "        \"TBS\": df[\"TBS\"],\n",
    "        \"gpu_count\": df[\"gpu_count\"],\n",
    "        \"gpu_type\": df[\"gpu_type\"],\n",
    "        \"sps\": [],\n",
    "        \"sps_type\": []\n",
    "    }\n",
    "    for ix, row in df.iterrows():\n",
    "        if not math.isnan(row[global_sps_key]):\n",
    "            temp_dict[\"sps\"].append(row[global_sps_key])\n",
    "            temp_dict[\"sps_type\"].append(\"hivemind global\")\n",
    "        else:\n",
    "            temp_dict[\"sps\"].append(row[local_sps_key])\n",
    "            temp_dict[\"sps_type\"].append(\"hivemind local\")\n",
    "    return pd.DataFrame(temp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e8014",
   "metadata": {},
   "source": [
    "# CV Model Suitability Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e648f562",
   "metadata": {},
   "source": [
    "### CV A10 Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3558a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_a10_baseline_df = get_baseline_runs(run_names=[\n",
    "    \"baseline-161-torchvision.models.resnet18\", #\"baseline-0-torchvision.models.resnet18\",\n",
    "    \"baseline-162-torchvision.models.resnet18\", #\"baseline-1-torchvision.models.resnet18\",\n",
    "    \"baseline-163-torchvision.models.resnet18\", #\"baseline-2-torchvision.models.resnet18\",\n",
    "    \"baseline-164-torchvision.models.resnet50\", #\"baseline-3-torchvision.models.resnet50\",\n",
    "    \"baseline-165-torchvision.models.resnet50\", #\"baseline-4-torchvision.models.resnet50\",\n",
    "    \"baseline-166-torchvision.models.resnet50\", #\"baseline-5-torchvision.models.resnet50\",\n",
    "    \"baseline-6-torchvision.models.resnet152\",\n",
    "    \"baseline-7-torchvision.models.resnet152\",\n",
    "    \"baseline-8-torchvision.models.resnet152\",\n",
    "    \"baseline-9-torchvision.models.wide_resnet101_2\",\n",
    "    \"baseline-10-torchvision.models.wide_resnet101_2\",\n",
    "    \"baseline-11-torchvision.models.wide_resnet101_2\",\n",
    "    \"baseline-12-torchvision.models.convnext_large\",\n",
    "    \"baseline-191-torchvision.models.convnext_large\",\n",
    "    \"baseline-192-torchvision.models.convnext_large\",\n",
    "    ])\n",
    "cv_a10_baseline_df = rename_models(df = cv_a10_baseline_df)\n",
    "cv_a10_baseline_df[\"gpu_count\"] = int(1)\n",
    "cv_a10_baseline_df[\"gpu_type\"] = \"A10\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58043586",
   "metadata": {},
   "source": [
    "### CV 2xA10, 3xA10, 4xA10, 8xA10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f66a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_a10_2x_runs_df = get_hivemind_runs(run_names=[\n",
    "    \"hivemind-282\", # 8k rn18\n",
    "    \"hivemind-285\", # 8k rn50\n",
    "    \"hivemind-286\", # 8k rn152\n",
    "    \"hivemind-291\", # 8k wrn101\n",
    "    \"hivemind-292\", # 8k conv\n",
    "    \"hivemind-283\", # 16k rn18\n",
    "    \"hivemind-287\", # 16k rn50\n",
    "    \"hivemind-288\", # 16k rn152\n",
    "    \"hivemind-293\", # 16k wrn101\n",
    "    \"hivemind-294\", # 16k conv\n",
    "    \"hivemind-284\", # 32k rn18\n",
    "    \"hivemind-289\", # 32k rn50\n",
    "    \"hivemind-290\", # 32k rn152\n",
    "    \"hivemind-295\", # 32k wrn101\n",
    "    \"hivemind-296\", # 32k conv\n",
    "    ],\n",
    "    drop_first_epoch=True\n",
    ")\n",
    "cv_a10_2x_runs_df = rename_models(df = cv_a10_2x_runs_df)\n",
    "cv_a10_2x_runs_df[\"gpu_count\"] = int(2)\n",
    "cv_a10_2x_runs_df[\"gpu_type\"] = \"A10\"\n",
    "cv_a10_2x_runs_df[\"local_sps_normalized\"] = cv_a10_2x_runs_df[\"local samples_per_sec\"] / cv_a10_2x_runs_df[\"gpu_count\"]\n",
    "\n",
    "cv_a10_3x_runs_df = get_hivemind_runs(run_names=[\n",
    "    \"hivemind-306\",\n",
    "    \"hivemind-307\",\n",
    "    \"hivemind-308\",\n",
    "    \"hivemind-309\",\n",
    "    \"hivemind-310\"\n",
    "    ],\n",
    "    drop_first_epoch=True\n",
    ")\n",
    "cv_a10_3x_runs_df = rename_models(df = cv_a10_3x_runs_df)\n",
    "cv_a10_3x_runs_df[\"gpu_count\"] = int(3)\n",
    "cv_a10_3x_runs_df[\"gpu_type\"] = \"A10\"\n",
    "cv_a10_3x_runs_df[\"local_sps_normalized\"] = cv_a10_3x_runs_df[\"local samples_per_sec\"] / cv_a10_3x_runs_df[\"gpu_count\"]\n",
    "\n",
    "cv_a10_4x_runs_df = get_hivemind_runs(run_names=[\n",
    "    \"hivemind-315\",\n",
    "    \"hivemind-316\",\n",
    "    \"hivemind-317\",\n",
    "    \"hivemind-318\",\n",
    "    \"hivemind-319\"\n",
    "    ],\n",
    "    drop_first_epoch=True\n",
    ")\n",
    "cv_a10_4x_runs_df = rename_models(df = cv_a10_4x_runs_df)\n",
    "cv_a10_4x_runs_df[\"gpu_count\"] = int(4)\n",
    "cv_a10_4x_runs_df[\"gpu_type\"] = \"A10\"\n",
    "cv_a10_4x_runs_df[\"local_sps_normalized\"] = cv_a10_4x_runs_df[\"local samples_per_sec\"] / cv_a10_4x_runs_df[\"gpu_count\"]\n",
    "\n",
    "cv_a10_8x_runs_df = get_hivemind_runs(run_names=[\n",
    "    \"hivemind-325\",\n",
    "    \"hivemind-326\",\n",
    "    \"hivemind-327\",\n",
    "    \"hivemind-328\",\n",
    "    \"hivemind-330\"\n",
    "    ],\n",
    "    drop_first_epoch=True\n",
    ")\n",
    "cv_a10_8x_runs_df = rename_models(df = cv_a10_8x_runs_df)\n",
    "cv_a10_8x_runs_df[\"gpu_count\"] = int(8)\n",
    "cv_a10_8x_runs_df[\"gpu_type\"] = \"A10\"\n",
    "cv_a10_8x_runs_df[\"local_sps_normalized\"] = cv_a10_8x_runs_df[\"local samples_per_sec\"] / cv_a10_8x_runs_df[\"gpu_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afff4398",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cv_a10_runs_df = pd.concat(objs=[\n",
    "    cv_a10_baseline_df,\n",
    "    cv_a10_2x_runs_df,\n",
    "    cv_a10_3x_runs_df,\n",
    "    cv_a10_4x_runs_df,\n",
    "    cv_a10_8x_runs_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90a5b77d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_a10_TBS_arg_df = pd.concat(objs=[\n",
    "    cv_a10_baseline_df,\n",
    "    cv_a10_2x_runs_df\n",
    "])\n",
    "\n",
    "sns.set(palette=cv_palette)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "ax = sns.barplot(\n",
    "    data=cv_a10_baseline_df,\n",
    "    x=\"TBS\",\n",
    "    y=\"samples_per_sec\",\n",
    "    hue=\"model\",\n",
    "    errorbar=('ci', 95),\n",
    "    errwidth=1.5,\n",
    "    capsize=0.03)\n",
    "#plt.title(\"1xA10\")\n",
    "plt.ylim([0,2200])\n",
    "plt.xlabel(\"Minibatch Size\")\n",
    "plt.ylabel(\"Samples per Second\")\n",
    "#sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.legend(title=\"Model\", fontsize=9, title_fontsize=12, bbox_to_anchor=(0.425,0.58))\n",
    "round_bar_value_multi(ax, decimals=0)\n",
    "save_fig(\"cv_1xa10_all-tbs_baseline\")\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "ax = sns.barplot(\n",
    "    data=cv_a10_2x_runs_df,\n",
    "    x=\"TBS\",\n",
    "    y=\"samples_per_sec\",\n",
    "    hue=\"model\",\n",
    "    errorbar=('ci', 95),\n",
    "    errwidth=1.5,\n",
    "    capsize=0.03)\n",
    "#plt.title(\"2xA10\")\n",
    "plt.ylim([0,2200])\n",
    "plt.ylabel(\"Samples per Second\")\n",
    "#sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.legend(title=\"Model\", fontsize=9, title_fontsize=12, bbox_to_anchor=(0.425,0.58))\n",
    "round_bar_value_multi(ax, decimals=0)\n",
    "save_fig(\"cv_2xa10_all-tbs_hivemind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4a779bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_TBS = 32768 #8192\n",
    "cv_a10_baseline_temp_df = cv_a10_baseline_df.query(f\"TBS=={local_TBS}\")\n",
    "cv_a10_baseline_temp_df[\"sps_type\"] = \"baseline\"\n",
    "cv_a10_baseline_temp_df = cv_a10_baseline_temp_df.rename(columns={\"samples_per_sec\": \"sps\"})\n",
    "\n",
    "cv_a10_2x_runs_local_sps_df = transform_to_compare_local_sps(df=cv_a10_2x_runs_df.query(f\"TBS=={local_TBS}\"))\n",
    "cv_a10_1x_2x_runs_local_sps_df = pd.concat(objs=[cv_a10_baseline_temp_df, cv_a10_2x_runs_local_sps_df])\n",
    "cv_a10_1x_2x_runs_local_sps_df[\"sps_norm\"] = cv_a10_1x_2x_runs_local_sps_df[\"sps\"] / cv_a10_1x_2x_runs_local_sps_df[\"gpu_count\"]\n",
    "\n",
    "sns.set(palette=local_sps_palette)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "ax = sns.barplot(\n",
    "    data=cv_a10_1x_2x_runs_local_sps_df,\n",
    "    x=\"model\",\n",
    "    y=\"sps_norm\",\n",
    "    hue=\"sps_type\",\n",
    "    errorbar=('ci', 95),\n",
    "    errwidth=1.5,\n",
    "    capsize=0.03)\n",
    "#plt.title(\"2xA10\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Samples per Second\")\n",
    "plt.ylim([0,2000])\n",
    "plt.legend(title=\"Throughput Type\")\n",
    "#sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "round_bar_value_multi(ax, decimals=0)\n",
    "plt.legend(title=\"Throughput Type\", fontsize=9, title_fontsize=10)\n",
    "plt.xticks(rotation=10)\n",
    "save_fig(f\"cv_2xa10_{local_TBS}_hivemind_local_sps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3404b4b",
   "metadata": {},
   "source": [
    "# CV Hivemind Penalty Slowdown Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c73fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in cv_a10_1x_2x_runs_local_sps_df[\"model\"].unique():\n",
    "    baseline_speed = cv_a10_1x_2x_runs_local_sps_df.query(f\"model=='{model}' and gpu_count==1 and \\\n",
    "                                                             sps_type=='baseline'\")[\"sps\"].mean()\n",
    "    local_speed    = cv_a10_1x_2x_runs_local_sps_df.query(f\"model=='{model}' and gpu_count==2 and \\\n",
    "                                                             sps_type=='hivemind local'\")[\"sps_norm\"].mean()\n",
    "    global_speed   = cv_a10_1x_2x_runs_local_sps_df.query(f\"model=='{model}' and gpu_count==2 and \\\n",
    "                                                             sps_type=='hivemind global'\")[\"sps_norm\"].mean()\n",
    "    local_slowdown = local_speed / baseline_speed\n",
    "    global_baseline_slowdown = global_speed / baseline_speed\n",
    "    global_local_slowdown = global_speed / local_speed\n",
    "    \n",
    "    #print(f\"{model} Local  reaches only {round(local_slowdown,2)} of baseline speed.\")    \n",
    "    print(f\"{model} Global reaches only {round(global_baseline_slowdown,2)} of baseline speed.\")   \n",
    "    #print(f\"{model} Global reaches only {round(global_local_slowdown,2)} of local speed.\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd287f31",
   "metadata": {},
   "source": [
    "### Analyzing Granularity Of 2xA10 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49fb76bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "debug = False\n",
    "rn18_2xA10_8192_df    = get_granularity_cumulated(run_name=\"hivemind-282\", name=\"RN18 2xA10 8192\", debug=debug)\n",
    "rn50_2xA10_8192_df    = get_granularity_cumulated(run_name=\"hivemind-285\", name=\"RN50 2xA10 8192\", debug=debug)\n",
    "rn101_2xA10_8192_df   = get_granularity_cumulated(run_name=\"hivemind-286\",  name=\"RN152 2xA10 8192\", debug=debug)\n",
    "wdrn101_2xA10_8192_df = get_granularity_cumulated(run_name=\"hivemind-291\",  name=\"WRN101 2xA10 8192\", debug=debug)\n",
    "conv_2xA10_8192_df    = get_granularity_cumulated(run_name=\"hivemind-292\",  name=\"CONV 2xA10 8192\", debug=debug)\n",
    "\n",
    "rn18_2xA10_16384_df    = get_granularity_cumulated(run_name=\"hivemind-283\", name=\"RN18 2xA10 16384\", debug=debug)\n",
    "rn50_2xA10_16384_df    = get_granularity_cumulated(run_name=\"hivemind-287\", name=\"RN50 2xA10 16384\", debug=debug)\n",
    "rn101_2xA10_16384_df   = get_granularity_cumulated(run_name=\"hivemind-288\",  name=\"RN152 2xA10 16384\", debug=debug)\n",
    "wdrn101_2xA10_16384_df = get_granularity_cumulated(run_name=\"hivemind-293\",  name=\"WRN101 2xA10 16384\", debug=debug)\n",
    "conv_2xA10_16384_df    = get_granularity_cumulated(run_name=\"hivemind-294\",  name=\"CONV 2xA10 16384\", debug=debug)\n",
    "\n",
    "rn18_2xA10_32768_df    = get_granularity_cumulated(run_name=\"hivemind-284\", name=\"RN18 2xA10 32768\", debug=debug)\n",
    "rn50_2xA10_32768_df    = get_granularity_cumulated(run_name=\"hivemind-289\", name=\"RN50 2xA10 32768\", debug=debug)\n",
    "rn101_2xA10_32768_df   = get_granularity_cumulated(run_name=\"hivemind-290\",  name=\"RN152 2xA10 32768\", debug=debug)\n",
    "wdrn101_2xA10_32768_df = get_granularity_cumulated(run_name=\"hivemind-295\",  name=\"WRN101 2xA10 32768\", debug=debug)\n",
    "conv_2xA10_32768_df    = get_granularity_cumulated(run_name=\"hivemind-296\",  name=\"CONV 2xA10 32768\", debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "301e72cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_granulartiy_values(ax, df, decimals=2):\n",
    "    for (p_ix, p) in enumerate(ax.patches):\n",
    "        if p_ix >= (len(ax.patches) / 2):\n",
    "            p_ix = p_ix - (len(ax.patches) / 2)\n",
    "            _x = p.get_x() + p.get_width() / 2\n",
    "            _y = p.get_y() + p.get_height() + 5\n",
    "            if decimals == 2:\n",
    "                value = '{:.2f}'.format(df.iloc[int(p_ix)][\"granularity\"])\n",
    "            elif decimals == 1:\n",
    "                value = '{:.1f}'.format(df.iloc[int(p_ix)][\"granularity\"])\n",
    "            ax.text(_x, _y, value, ha=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f462d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_combined_granularity_df = pd.concat(objs=[\n",
    "    rn18_2xA10_8192_df,\n",
    "    rn18_2xA10_16384_df,\n",
    "    rn18_2xA10_32768_df,\n",
    "    \n",
    "    rn50_2xA10_8192_df,\n",
    "    rn50_2xA10_16384_df,\n",
    "    rn50_2xA10_32768_df,\n",
    "    \n",
    "    rn101_2xA10_8192_df,\n",
    "    rn101_2xA10_16384_df,\n",
    "    rn101_2xA10_32768_df,\n",
    "    \n",
    "    wdrn101_2xA10_8192_df,\n",
    "    wdrn101_2xA10_16384_df,\n",
    "    wdrn101_2xA10_32768_df,\n",
    "    \n",
    "    conv_2xA10_8192_df,\n",
    "    conv_2xA10_16384_df,\n",
    "    conv_2xA10_32768_df\n",
    "])\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "plt.figure(figsize=(8,2))\n",
    "ax = cv_combined_granularity_df[[\"calc_time_s\",\"comm_time_s\",\"name\"]].set_index('name').plot(kind='bar', stacked=True, figsize=(8,3))\n",
    "plt.ylabel(\"Time in Seconds\")\n",
    "plt.xlabel(\"TBS\")\n",
    "plt.legend(labels=[\"Calculation\", \"Communication\"])\n",
    "plt.ylim(0,400)\n",
    "ax.axvline(2.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(5.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(8.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(11.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "labels = [\"\".join(label.split(\" \")[-1]) for label in labels]\n",
    "ax.set_xticklabels(labels)\n",
    "ax.text(0.5, 150, \"RN18\", fontsize=15)\n",
    "ax.text(3.3, 150, \"RN50\", fontsize=15)\n",
    "ax.text(6.2, 250, \"RN152\", fontsize=15)\n",
    "ax.text(9, 250, \"WRN101\", fontsize=15)\n",
    "ax.text(12, 250, \"CONV\", fontsize=15)\n",
    "show_granulartiy_values(ax=ax, df=cv_combined_granularity_df, decimals=1)\n",
    "save_fig(\"cv_2xa10_all-tbs_granularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "669d3297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter small models out\n",
    "temp_df = full_cv_a10_runs_df#.query(\"model=='ResNet152' or model=='WideResNet101_2' or model=='ConvNextLarge'\")\n",
    "# filter local sps out\n",
    "temp_df = temp_df.query(\"samples_per_sec.notna()\")\n",
    "# filter out non-32k TBS\n",
    "temp_df = temp_df.query(\"TBS==32768\")\n",
    "\n",
    "sns.set(palette=cv_palette)\n",
    "plt.figure(figsize=(8,4))\n",
    "ax = sns.lineplot(\n",
    "    data=temp_df,\n",
    "    x=\"gpu_count\",\n",
    "    y=\"samples_per_sec\",\n",
    "    hue=\"model\",\n",
    "    style=\"model\",\n",
    "    #errorbar=('sd', 1),\n",
    "    dashes=True, err_style=\"band\",\n",
    "    markers=True,\n",
    "    linewidth=2.0,\n",
    "    markersize=10.0,\n",
    ")\n",
    "#ax.set(yscale='log')\n",
    "#plt.title(\"CV Model Suitability Study\")\n",
    "plt.xlabel(\"A10 Count\")\n",
    "plt.ylabel(\"Samples per Second\")\n",
    "plt.legend(title=\"Model\")\n",
    "#sns.move_legend(ax, \"upper right\", bbox_to_anchor=(1.33, 1.03))\n",
    "save_fig(\"cv_multi-a10_scalability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a90c79b",
   "metadata": {},
   "source": [
    "## CV Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3fae125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_throughput(model, gpu_count):\n",
    "    TBS = 32768\n",
    "    temp_df = full_cv_a10_runs_df.query(f\"samples_per_sec.notna() and TBS=={TBS} and model=='{model}' and gpu_count=={gpu_count}\")\n",
    "    return temp_df[\"samples_per_sec\"].mean()\n",
    "\n",
    "for model in full_cv_a10_runs_df[\"model\"].unique():\n",
    "    print(f\"Model {model}:\")\n",
    "    baseline_sps = get_mean_throughput(model=model, gpu_count=1)\n",
    "    for gpu_count in [2,3,4,8]:\n",
    "        hivemind_sps = get_mean_throughput(model=model, gpu_count=gpu_count)\n",
    "        print(f\" - Speedup on {gpu_count} GPUS: {round(hivemind_sps / baseline_sps,2)}x, Norm: {round(hivemind_sps / baseline_sps / gpu_count,2)} ({round(hivemind_sps,1)}, {round(baseline_sps,1)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9cc4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "rn18_3xA10_32768_df    = get_granularity_cumulated(run_name=\"hivemind-306\", name=\"RN18 3xA10 32768\", debug=debug)\n",
    "rn50_3xA10_32768_df    = get_granularity_cumulated(run_name=\"hivemind-307\", name=\"RN50 3xA10 32768\", debug=debug)\n",
    "rn101_3xA10_32768_df   = get_granularity_cumulated(run_name=\"hivemind-308\", name=\"RN152 3xA10 32768\", debug=debug)\n",
    "wdrn101_3xA10_32768_df = get_granularity_cumulated(run_name=\"hivemind-309\", name=\"WRN101 3xA10 32768\", debug=debug)\n",
    "conv_3xA10_32768_df    = get_granularity_cumulated(run_name=\"hivemind-310\", name=\"CONV 3xA10 32768\", debug=debug)\n",
    "\n",
    "rn18_4xA10_32768_df    = get_granularity_cumulated(run_name=\"hivemind-315\", name=\"RN18 4xA10 32768\", debug=debug)\n",
    "rn50_4xA10_32768_df    = get_granularity_cumulated(run_name=\"hivemind-316\", name=\"RN50 4xA10 32768\", debug=debug)\n",
    "rn101_4xA10_32768_df   = get_granularity_cumulated(run_name=\"hivemind-317\", name=\"RN152 4xA10 32768\", debug=debug)\n",
    "wdrn101_4xA10_32768_df = get_granularity_cumulated(run_name=\"hivemind-318\", name=\"WRN101 4xA10 32768\", debug=debug)\n",
    "conv_4xA10_32768_df    = get_granularity_cumulated(run_name=\"hivemind-319\", name=\"CONV 4xA10 32768\", debug=debug)\n",
    "\n",
    "rn18_8xA10_32768_df    = get_granularity_cumulated(run_name=\"hivemind-325\", name=\"RN18 8xA10 32768\", debug=debug)\n",
    "rn50_8xA10_32768_df    = get_granularity_cumulated(run_name=\"hivemind-326\", name=\"RN50 8xA10 32768\", debug=debug)\n",
    "rn101_8xA10_32768_df   = get_granularity_cumulated(run_name=\"hivemind-327\", name=\"RN152 8xA10 32768\", debug=debug)\n",
    "wdrn101_8xA10_32768_df = get_granularity_cumulated(run_name=\"hivemind-328\", name=\"WRN101 8xA10 32768\", debug=debug)\n",
    "conv_8xA10_32768_df    = get_granularity_cumulated(run_name=\"hivemind-330\", name=\"CONV 8xA10 32768\", debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edd2a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(rn18_2xA10_32768_df)\n",
    "display(rn18_3xA10_32768_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130724e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ce7ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_full_a10_granulartiy_df = pd.concat(objs=[\n",
    "    rn18_2xA10_32768_df,\n",
    "    rn18_3xA10_32768_df,\n",
    "    rn18_4xA10_32768_df,\n",
    "    rn18_8xA10_32768_df,\n",
    "    rn50_2xA10_32768_df,\n",
    "    rn50_3xA10_32768_df,\n",
    "    rn50_4xA10_32768_df,\n",
    "    rn50_8xA10_32768_df,\n",
    "    rn101_2xA10_32768_df,\n",
    "    rn101_3xA10_32768_df,\n",
    "    rn101_4xA10_32768_df,\n",
    "    rn101_8xA10_32768_df,\n",
    "    wdrn101_2xA10_32768_df,\n",
    "    wdrn101_3xA10_32768_df,\n",
    "    wdrn101_4xA10_32768_df,\n",
    "    wdrn101_8xA10_32768_df,\n",
    "    conv_2xA10_32768_df,\n",
    "    conv_3xA10_32768_df,\n",
    "    conv_4xA10_32768_df,\n",
    "    conv_8xA10_32768_df\n",
    "])\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "ax = cv_full_a10_granulartiy_df[[\"calc_time_s\",\"comm_time_s\",\"name\"]].set_index('name').plot(kind='bar', stacked=True, figsize=(8,3))\n",
    "plt.ylabel(\"Time in Seconds\")\n",
    "plt.xlabel(\"A10 Count\")\n",
    "plt.legend(labels=[\"Calculation\", \"Communication\"])\n",
    "plt.ylim(0,400)\n",
    "ax.axvline(3.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(7.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(11.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(15.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "labels = [\" \".join(label.split(\" \")[1:2])[0] for label in labels]\n",
    "ax.set_xticklabels(labels, rotation=0)\n",
    "show_granulartiy_values(ax=ax, df=cv_full_a10_granulartiy_df, decimals=1)\n",
    "ax.text(0.9, 150, \"RN18\", fontsize=15)\n",
    "ax.text(4.6, 150, \"RN50\", fontsize=15)\n",
    "ax.text(8.5, 250, \"RN152\", fontsize=15)\n",
    "ax.text(12.2, 250, \"WRN101\", fontsize=15)\n",
    "ax.text(17, 300, \"CONV\", fontsize=15)\n",
    "save_fig(\"cv_full-a10_all-tbs_granularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72667a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_full_a10_granulartiy_df = pd.concat(objs=[\n",
    "    rn18_2xA10_32768_df,\n",
    "    rn18_3xA10_32768_df,\n",
    "    rn18_4xA10_32768_df,\n",
    "    rn18_8xA10_32768_df,\n",
    "    rn50_2xA10_32768_df,\n",
    "    rn50_3xA10_32768_df,\n",
    "    rn50_4xA10_32768_df,\n",
    "    rn50_8xA10_32768_df,\n",
    "    rn101_2xA10_32768_df,\n",
    "    rn101_3xA10_32768_df,\n",
    "    rn101_4xA10_32768_df,\n",
    "    rn101_8xA10_32768_df,\n",
    "    wdrn101_2xA10_32768_df,\n",
    "    wdrn101_3xA10_32768_df,\n",
    "    wdrn101_4xA10_32768_df,\n",
    "    wdrn101_8xA10_32768_df,\n",
    "    conv_2xA10_32768_df,\n",
    "    conv_3xA10_32768_df,\n",
    "    conv_4xA10_32768_df,\n",
    "    conv_8xA10_32768_df\n",
    "])\n",
    "cv_full_a10_granulartiy_df[\"comm_time_s_norm\"] = cv_full_a10_granulartiy_df[\"comm_time_s\"] / 3\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "plt.figure(figsize=(8,3))\n",
    "ax = cv_full_a10_granulartiy_df[[\"comm_time_s_norm\",\"name\"]].set_index('name').plot(kind='bar', stacked=True)\n",
    "plt.ylabel(\"Time in Seconds\")\n",
    "plt.xlabel(\"\")\n",
    "#plt.legend(labels=[\"Calculation\", \"Communication\"])\n",
    "plt.ylim(0,25)\n",
    "ax.axvline(3.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(7.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(11.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(15.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "round_bar_value_multi(ax, decimals=2)\n",
    "\n",
    "#labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "#labels = [\" \".join(label.split(\" \")[0:2]) for label in labels]\n",
    "#ax.set_xticklabels(labels)\n",
    "#show_granulartiy_values(ax=ax, df=cv_full_a10_granulartiy_df, decimals=1)\n",
    "#save_fig(\"cv_full-a10_all-tbs_granularity\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9681764",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.concat(objs=[\n",
    "    rn18_8xA10_32768_df,\n",
    "    rn50_8xA10_32768_df,\n",
    "    rn101_8xA10_32768_df,\n",
    "    wdrn101_8xA10_32768_df,\n",
    "    conv_8xA10_32768_df\n",
    "])\n",
    "\n",
    "def to_agg(name, time_s, time_type):\n",
    "    agg_dict[\"name\"].append(name)\n",
    "    agg_dict[\"time_µ\"].append(time_s * 1000 * 1000)\n",
    "    agg_dict[\"time_type\"].append(time_type)\n",
    "\n",
    "\n",
    "\n",
    "for name in temp_df[\"name\"].unique():\n",
    "    row = temp_df.query(f\"name == '{name}'\")\n",
    "    new_name = name.split(\" \")[0]\n",
    "    if new_name == \"RN18\":\n",
    "        param_size = 11.7 * 10**6\n",
    "    elif new_name == \"RN50\":\n",
    "        param_size = 25.6 * 10**6\n",
    "    elif new_name == \"RN152\":\n",
    "        param_size = 60.2 * 10**6\n",
    "    elif new_name == \"WRN101\":\n",
    "        param_size = 126.9 * 10**6\n",
    "    elif new_name == \"CONV\":\n",
    "        param_size = 197.8 * 10**6\n",
    "    print(name,param_size)\n",
    "    to_agg(name=new_name, time_s=row[\"calc_time_s\"].item() / param_size, time_type=\"Calculation\")\n",
    "    to_agg(name=new_name, time_s=row[\"comm_time_s\"].item() / param_size, time_type=\"Communication\")\n",
    "\n",
    "agg_df = pd.DataFrame(agg_dict)\n",
    "#agg_df\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=agg_df,\n",
    "    x=\"name\",\n",
    "    y=\"time_µ\",\n",
    "    hue=\"time_type\",\n",
    "    color=\"0.3\", s=200, style=\"time_type\", zorder=2)\n",
    "\n",
    "plt.ylabel(\"Time in Microseconds per Parameter\")\n",
    "plt.xlabel(\"\")\n",
    "plt.legend(title=\"Type\")\n",
    "#save_fig(\"cv_8xA10_baseline_calc_vs_comm_plot\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1dc12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.concat(objs=[\n",
    "    rn18_8xA10_32768_df,\n",
    "    rn50_8xA10_32768_df,\n",
    "    rn101_8xA10_32768_df,\n",
    "    wdrn101_8xA10_32768_df,\n",
    "    conv_8xA10_32768_df\n",
    "])\n",
    "\n",
    "agg_dict = {\n",
    "    \"name\": [],\n",
    "    \"time_µ\": [],\n",
    "    \"time_type\": []\n",
    "}\n",
    "\n",
    "def to_agg(name, time_s, time_type):\n",
    "    agg_dict[\"name\"].append(name)\n",
    "    agg_dict[\"time_µ\"].append(time_s * 1000 * 1000)\n",
    "    agg_dict[\"time_type\"].append(time_type)\n",
    "\n",
    "for name in temp_df[\"name\"].unique():\n",
    "    row = temp_df.query(f\"name == '{name}'\")\n",
    "    new_name = name.split(\" \")[0]\n",
    "    if new_name == \"RN18\":\n",
    "        param_size = 11.7 * 10**6\n",
    "    elif new_name == \"RN50\":\n",
    "        param_size = 25.6 * 10**6\n",
    "    elif new_name == \"RN152\":\n",
    "        param_size = 60.2 * 10**6\n",
    "    elif new_name == \"WRN101\":\n",
    "        param_size = 126.9 * 10**6\n",
    "    elif new_name == \"CONV\":\n",
    "        param_size = 197.8 * 10**6\n",
    "    print(name,param_size)\n",
    "    to_agg(name=new_name, time_s=row[\"calc_time_s\"].item() / param_size, time_type=\"Calculation\")\n",
    "    to_agg(name=new_name, time_s=row[\"comm_time_s\"].item() / param_size, time_type=\"Communication\")\n",
    "\n",
    "agg_df = pd.DataFrame(agg_dict)\n",
    "#agg_df\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=agg_df,\n",
    "    x=\"name\",\n",
    "    y=\"time_µ\",\n",
    "    hue=\"time_type\",\n",
    "    color=\"0.3\", s=200, style=\"time_type\", zorder=2)\n",
    "\n",
    "plt.ylabel(\"Time in Microseconds per Parameter\")\n",
    "plt.xlabel(\"\")\n",
    "plt.legend(title=\"Type\")\n",
    "#save_fig(\"cv_8xA10_baseline_calc_vs_comm_plot\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab9d86e",
   "metadata": {},
   "source": [
    "# NLP Model Suitability Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31731936",
   "metadata": {},
   "source": [
    "### NLP A10 Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d6e02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_a10_baseline_df = get_baseline_runs(run_names=[\n",
    "    \"baseline-167-roberta_mlm_base\", # \"baseline-15-roberta_mlm_base\",\n",
    "    \"baseline-168-roberta_mlm_base\", #\"baseline-16-roberta_mlm_base\",\n",
    "    \"baseline-169-roberta_mlm_base\", #\"baseline-17-roberta_mlm_base\",\n",
    "    \"baseline-18-roberta_mlm_large\",\n",
    "    \"baseline-19-roberta_mlm_large\",\n",
    "    \"baseline-20-roberta_mlm_large\",\n",
    "    \"baseline-21-roberta_mlm_xlm\",\n",
    "    \"baseline-22-roberta_mlm_xlm\",\n",
    "    \"baseline-23-roberta_mlm_xlm\",\n",
    "    ])\n",
    "nlp_a10_baseline_df = rename_models(df = nlp_a10_baseline_df)\n",
    "nlp_a10_baseline_df[\"gpu_count\"] = int(1)\n",
    "nlp_a10_baseline_df[\"gpu_type\"] = \"A10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbeb1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_a10_baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0852d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_a10_2x_runs_df = get_hivemind_runs(run_names=[\n",
    "    \"hivemind-297\",\n",
    "    \"hivemind-298\",\n",
    "    \"hivemind-299\",\n",
    "    \"hivemind-300\", \n",
    "    \"hivemind-301\",\n",
    "    \"hivemind-302\",\n",
    "    \"hivemind-303\",\n",
    "    \"hivemind-303\",\n",
    "    \"hivemind-304\",\n",
    "    \"hivemind-305\",\n",
    "    ],\n",
    "    drop_first_epoch=True\n",
    ")\n",
    "nlp_a10_2x_runs_df = rename_models(df = nlp_a10_2x_runs_df)\n",
    "nlp_a10_2x_runs_df[\"gpu_count\"] = int(2)\n",
    "nlp_a10_2x_runs_df[\"gpu_type\"] = \"A10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6576bc21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "574e5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_a10_baseline_df.query(\"model=='RoBERTaBase' and TBS==8192\").describe()\n",
    "print(f\"RoBERTaBaes slowdown vs baseline: {round(1 - (354 / (990 + 382)),2)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f7e8f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_a10_TBS_arg_df = pd.concat(objs=[\n",
    "    nlp_a10_baseline_df,\n",
    "    nlp_a10_2x_runs_df\n",
    "])\n",
    "\n",
    "nlp_palette = sns.color_palette(\"YlGnBu\", 3)\n",
    "sns.set(palette=nlp_palette)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "ax = sns.barplot(\n",
    "    data=nlp_a10_baseline_df,\n",
    "    x=\"TBS\",\n",
    "    y=\"samples_per_sec\",\n",
    "    hue=\"model\",\n",
    "    errorbar=('ci', 95),\n",
    "    errwidth=1.5,\n",
    "    capsize=0.03)\n",
    "#plt.title(\"1xA10\")\n",
    "plt.ylim([0,1600])\n",
    "#sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.legend(title=\"Model\", fontsize=9, title_fontsize=12, bbox_to_anchor=(0.7,0.98))\n",
    "plt.xlabel(\"Minibatch Size\")\n",
    "plt.ylabel(\"Samples per Second\")\n",
    "round_bar_value_multi(ax, decimals=0)\n",
    "save_fig(\"nlp_1xa10_all-tbs_baseline\")\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "ax = sns.barplot(\n",
    "    data=nlp_a10_2x_runs_df,\n",
    "    x=\"TBS\",\n",
    "    y=\"samples_per_sec\",\n",
    "    hue=\"model\",\n",
    "    errorbar=('ci', 95),\n",
    "    errwidth=1.5,\n",
    "    capsize=0.03)\n",
    "#plt.title(\"2xA10\")\n",
    "plt.ylim([0,1600])\n",
    "plt.ylabel(\"Samples per Second\")\n",
    "plt.legend(title=\"Model\", fontsize=9, title_fontsize=12, bbox_to_anchor=(0.7,0.98))\n",
    "#sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "round_bar_value_multi(ax, decimals=0)\n",
    "save_fig(\"nlp_2xa10_all-tbs_hivemind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5dba02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_TBS = 32768\n",
    "nlp_a10_baseline_temp_df = nlp_a10_baseline_df.query(f\"TBS=={local_TBS}\")\n",
    "nlp_a10_baseline_temp_df[\"sps_type\"] = \"baseline\"\n",
    "nlp_a10_baseline_temp_df = nlp_a10_baseline_temp_df.rename(columns={\"samples_per_sec\": \"sps\"})\n",
    "\n",
    "nlp_a10_2x_runs_local_sps_df = transform_to_compare_local_sps(df=nlp_a10_2x_runs_df.query(f\"TBS=={local_TBS}\"))\n",
    "nlp_a10_1x_2x_runs_local_sps_df = pd.concat(objs=[nlp_a10_baseline_temp_df, nlp_a10_2x_runs_local_sps_df])\n",
    "nlp_a10_1x_2x_runs_local_sps_df[\"sps_norm\"] = nlp_a10_1x_2x_runs_local_sps_df[\"sps\"] / nlp_a10_1x_2x_runs_local_sps_df[\"gpu_count\"]\n",
    "\n",
    "sns.set(palette=local_sps_palette)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "ax = sns.barplot(\n",
    "    data=nlp_a10_1x_2x_runs_local_sps_df,\n",
    "    x=\"model\",\n",
    "    y=\"sps_norm\",\n",
    "    hue=\"sps_type\",\n",
    "    errorbar=('ci', 95),\n",
    "    errwidth=1.5,\n",
    "    capsize=0.03)\n",
    "#plt.title(\"2xA10\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Samples per Second\")\n",
    "plt.ylim([0,1500])\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "#plt.legend(prop={'size': 6})\n",
    "round_bar_value_multi(ax, decimals=0)\n",
    "plt.legend(title=\"Throughput Type\", fontsize=9, title_fontsize=10)\n",
    "#plt.xticks(rotation=10)\n",
    "save_fig(f\"nlp_2xa10_{local_TBS}_hivemind_local_sps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c9c309",
   "metadata": {},
   "source": [
    "# NLP Hivemind Penalty Slowdown Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50cfbb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in nlp_a10_1x_2x_runs_local_sps_df[\"model\"].unique():\n",
    "    baseline_speed = nlp_a10_1x_2x_runs_local_sps_df.query(f\"model=='{model}' and gpu_count==1 and \\\n",
    "                                                             sps_type=='baseline'\")[\"sps\"].mean()\n",
    "    local_speed    = nlp_a10_1x_2x_runs_local_sps_df.query(f\"model=='{model}' and gpu_count==2 and \\\n",
    "                                                             sps_type=='hivemind local'\")[\"sps_norm\"].mean()\n",
    "    global_speed   = nlp_a10_1x_2x_runs_local_sps_df.query(f\"model=='{model}' and gpu_count==2 and \\\n",
    "                                                             sps_type=='hivemind global'\")[\"sps_norm\"].mean()\n",
    "    local_slowdown = local_speed / baseline_speed\n",
    "    global_baseline_slowdown = global_speed / baseline_speed\n",
    "    global_local_slowdown = global_speed / local_speed\n",
    "\n",
    "    \n",
    "    #print(f\"{model} Local  reaches only {round(local_slowdown,2)} of baseline speed.\")    \n",
    "    print(f\"{model} Global reaches only {round(global_local_slowdown,2)} of local speed.\")    \n",
    "    print(f\"{model} Global reaches only {round(global_baseline_slowdown,2)} of baseline speed.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e7d26f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_a10_1x_2x_runs_local_sps_df.query(\"model=='RoBERTaXLM' and gpu_count==2 and sps_type=='hivemind global'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a5f6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "rBse_2xA10_8192_df = get_granularity_cumulated(run_name=\"hivemind-297\", name=\"RBse 2xA10 8192\", debug=debug)\n",
    "rLrg_2xA10_8192_df = get_granularity_cumulated(run_name=\"hivemind-298\", name=\"RLrg 2xA10 8192\", debug=debug)\n",
    "rXLM_2xA10_8192_df = get_granularity_cumulated(run_name=\"hivemind-299\", name=\"RXLM 2xA10 8192\", debug=debug)\n",
    "\n",
    "rBse_2xA10_16384_df = get_granularity_cumulated(run_name=\"hivemind-300\", name=\"RBse 2xA10 16384\", debug=debug)\n",
    "rLrg_2xA10_16384_df = get_granularity_cumulated(run_name=\"hivemind-301\", name=\"RLrg 2xA10 16384\", debug=debug)\n",
    "rXLM_2xA10_16384_df = get_granularity_cumulated(run_name=\"hivemind-302\", name=\"RXLM 2xA10 16384\", debug=debug)\n",
    "\n",
    "rBse_2xA10_32768_df = get_granularity_cumulated(run_name=\"hivemind-303\", name=\"RBse 2xA10 32768\", debug=debug)\n",
    "rLrg_2xA10_32768_df = get_granularity_cumulated(run_name=\"hivemind-304\", name=\"RLrg 2xA10 32768\", debug=debug)\n",
    "rXLM_2xA10_32768_df = get_granularity_cumulated(run_name=\"hivemind-305\", name=\"RXLM 2xA10 32768\", debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70345dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_combined_granularity_df = pd.concat(objs=[\n",
    "    rBse_2xA10_8192_df,\n",
    "    rBse_2xA10_16384_df,\n",
    "    rBse_2xA10_32768_df,\n",
    "    rLrg_2xA10_8192_df,\n",
    "    rLrg_2xA10_16384_df,\n",
    "    rLrg_2xA10_32768_df,\n",
    "    rXLM_2xA10_8192_df,\n",
    "    rXLM_2xA10_16384_df,\n",
    "    rXLM_2xA10_32768_df,\n",
    "])\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "ax = nlp_combined_granularity_df[[\"calc_time_s\",\"comm_time_s\",\"name\"]].set_index('name').plot(kind='bar', stacked=True, figsize=(8,3))\n",
    "plt.ylabel(\"Time in Seconds\")\n",
    "plt.xlabel(\"TBS\")\n",
    "plt.ylim(0,400)\n",
    "plt.legend(labels=[\"Calculation\", \"Communication\"])\n",
    "ax.axvline(2.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(5.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.text(0.5, 150, \"RBse\", fontsize=15)\n",
    "ax.text(3.7, 150, \"RLrg\", fontsize=15)\n",
    "ax.text(6.5, 150, \"RXLM\", fontsize=15)\n",
    "labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "labels = [\"\".join(label.split(\" \")[-1]) for label in labels]\n",
    "ax.set_xticklabels(labels)\n",
    "show_granulartiy_values(ax=ax, df=nlp_combined_granularity_df, decimals=1)\n",
    "save_fig(\"nlp_2xa10_all-tbs_granularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eff6dfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_a10_3x_runs_df = get_hivemind_runs(run_names=[\n",
    "    \"hivemind-311\",\n",
    "    \"hivemind-312\",\n",
    "    \"hivemind-313\",\n",
    "    ],\n",
    "    drop_first_epoch=True\n",
    ")\n",
    "nlp_a10_3x_runs_df = rename_models(df = nlp_a10_3x_runs_df)\n",
    "nlp_a10_3x_runs_df[\"gpu_count\"] = int(3)\n",
    "nlp_a10_3x_runs_df[\"gpu_type\"] = \"A10\"\n",
    "\n",
    "nlp_a10_4x_runs_df = get_hivemind_runs(run_names=[\n",
    "    \"hivemind-320\",\n",
    "    \"hivemind-321\",\n",
    "    \"hivemind-322\",\n",
    "    ],\n",
    "    drop_first_epoch=True\n",
    ")\n",
    "nlp_a10_4x_runs_df = rename_models(df = nlp_a10_4x_runs_df)\n",
    "nlp_a10_4x_runs_df[\"gpu_count\"] = int(4)\n",
    "nlp_a10_4x_runs_df[\"gpu_type\"] = \"A10\"\n",
    "\n",
    "nlp_a10_8x_runs_df = get_hivemind_runs(run_names=[\n",
    "    \"hivemind-331\",\n",
    "    \"hivemind-332\",\n",
    "    \"hivemind-333\"\n",
    "    ],\n",
    "    drop_first_epoch=True\n",
    ")\n",
    "nlp_a10_8x_runs_df = rename_models(df = nlp_a10_8x_runs_df)\n",
    "nlp_a10_8x_runs_df[\"gpu_count\"] = int(8)\n",
    "nlp_a10_8x_runs_df[\"gpu_type\"] = \"A10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21432a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_nlp_a10_runs_df = pd.concat(objs=[\n",
    "    nlp_a10_baseline_df,\n",
    "    nlp_a10_2x_runs_df,\n",
    "    nlp_a10_3x_runs_df,\n",
    "    nlp_a10_4x_runs_df,\n",
    "    nlp_a10_8x_runs_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7130371",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_nlp_a10_runs_df.query(\"model=='RoBERTaBase' and gpu_count==2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af9aa408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter small models out\n",
    "temp_df = full_nlp_a10_runs_df#.query(\"model=='RoBERTaLarge' or model=='RoBERTaXLM'\")\n",
    "# filter local sps out\n",
    "temp_df = temp_df.query(\"samples_per_sec.notna()\")\n",
    "# filter out non-32k TBS\n",
    "temp_df = temp_df.query(\"TBS==32768\")\n",
    "\n",
    "nlp_palette = sns.color_palette(\"YlGnBu\", 3)\n",
    "pal = sns.color_palette(palette=nlp_palette)\n",
    "sns.set(palette=pal)\n",
    "plt.figure(figsize=(8,4))\n",
    "ax = sns.lineplot(\n",
    "    data=temp_df,\n",
    "    x=\"gpu_count\",\n",
    "    y=\"samples_per_sec\",\n",
    "    hue=\"model\",\n",
    "    style=\"model\",\n",
    "    #errorbar=('sd', 1),\n",
    "    dashes=True, err_style=\"band\",\n",
    "    markers=True,\n",
    "    linewidth=2.0,\n",
    "    markersize=10.0\n",
    ")\n",
    "#plt.title(\"NLP Model Suitability Study\")\n",
    "plt.ylabel(\"Samples per Second\")\n",
    "plt.xlabel(\"A10 Count\")\n",
    "plt.legend(title=\"Model\")\n",
    "#sns.move_legend(ax, \"upper right\", bbox_to_anchor=(1.29, 1.025))\n",
    "save_fig(\"nlp_multi-a10_scalability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab70dd5",
   "metadata": {},
   "source": [
    "## NLP Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddbdf88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_throughput(model, gpu_count):\n",
    "    TBS = 32768\n",
    "    temp_df = full_nlp_a10_runs_df.query(f\"samples_per_sec.notna() and TBS=={TBS} and model=='{model}' and gpu_count=={gpu_count}\")\n",
    "    return temp_df[\"samples_per_sec\"].mean()\n",
    "\n",
    "for model in full_nlp_a10_runs_df[\"model\"].unique():\n",
    "    print(f\"Model {model}:\")\n",
    "    baseline_sps = get_mean_throughput(model=model, gpu_count=1)\n",
    "    for gpu_count in [2,3,4,8]:\n",
    "        hivemind_sps = get_mean_throughput(model=model, gpu_count=gpu_count)\n",
    "        print(f\" - Speedup on {gpu_count} GPUS: {round(hivemind_sps / baseline_sps,2)}x, Norm: {round(hivemind_sps / baseline_sps / gpu_count,2)} ({round(hivemind_sps,1)}, {round(baseline_sps,1)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "314a6ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "debug = False\n",
    "rBse_3xA10_32768_df = get_granularity_cumulated(run_name=\"hivemind-311\", name=\"RBse 3xA10 32768\", debug=debug)\n",
    "rLrg_3xA10_32768_df = get_granularity_cumulated(run_name=\"hivemind-312\",  name=\"RLrg 3xA10 32768\", debug=debug)\n",
    "rXLM_3xA10_32768_df = get_granularity_cumulated(run_name=\"hivemind-313\",  name=\"RXLM 3xA10 32768\", debug=debug)\n",
    "\n",
    "rBse_4xA10_32768_df = get_granularity_cumulated(run_name=\"hivemind-320\", name=\"RBse 4xA10 32768\", debug=debug)\n",
    "rLrg_4xA10_32768_df = get_granularity_cumulated(run_name=\"hivemind-321\",  name=\"RLrg 4xA10 32768\", debug=debug)\n",
    "rXLM_4xA10_32768_df = get_granularity_cumulated(run_name=\"hivemind-322\",  name=\"RXLM 4xA10 32768\", debug=debug)\n",
    "\n",
    "rBse_8xA10_32768_df = get_granularity_cumulated(run_name=\"hivemind-331\", name=\"RBse 8xA10 32768\", debug=debug)\n",
    "rLrg_8xA10_32768_df = get_granularity_cumulated(run_name=\"hivemind-332\",  name=\"RLrg 8xA10 32768\", debug=debug)\n",
    "rXLM_8xA10_32768_df = get_granularity_cumulated(run_name=\"hivemind-333\",  name=\"RXLM 8xA10 32768\", debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6900379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_full_a10_granulartiy_df = pd.concat(objs=[\n",
    "    rBse_2xA10_32768_df,\n",
    "    rBse_3xA10_32768_df,\n",
    "    rBse_4xA10_32768_df,\n",
    "    rBse_8xA10_32768_df,\n",
    "    rLrg_2xA10_32768_df,\n",
    "    rLrg_3xA10_32768_df,\n",
    "    rLrg_4xA10_32768_df,\n",
    "    rLrg_8xA10_32768_df,\n",
    "    rXLM_2xA10_32768_df,\n",
    "    rXLM_3xA10_32768_df,\n",
    "    rXLM_4xA10_32768_df,\n",
    "    rXLM_8xA10_32768_df,\n",
    "])\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "ax = nlp_full_a10_granulartiy_df[[\"calc_time_s\",\"comm_time_s\",\"name\"]].set_index('name').plot(kind='bar', stacked=True, figsize=(8,3))\n",
    "plt.ylabel(\"Time in Seconds\")\n",
    "plt.xlabel(\"A10 Count\")\n",
    "plt.legend(labels=[\"Calculation\", \"Communication\"])\n",
    "plt.ylim(0,400)\n",
    "ax.axvline(3.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(7.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.text(1, 150, \"RBse\", fontsize=15)\n",
    "ax.text(5.05, 150, \"RLrg\", fontsize=15)\n",
    "ax.text(8.9, 200, \"RXLM\", fontsize=15)\n",
    "labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "labels = [\"\".join(label.split(\" \")[1:2])[0] for label in labels]\n",
    "ax.set_xticklabels(labels, rotation=0)\n",
    "show_granulartiy_values(ax=ax, df=nlp_full_a10_granulartiy_df, decimals=1)\n",
    "save_fig(\"nlp_full-a10_all-tbs_granularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8b64e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_full_a10_granulartiy_df = pd.concat(objs=[\n",
    "    rBse_2xA10_32768_df,\n",
    "    rBse_3xA10_32768_df,\n",
    "    rBse_4xA10_32768_df,\n",
    "    rBse_8xA10_32768_df,\n",
    "    rLrg_2xA10_32768_df,\n",
    "    rLrg_3xA10_32768_df,\n",
    "    rLrg_4xA10_32768_df,\n",
    "    rLrg_8xA10_32768_df,\n",
    "    rXLM_2xA10_32768_df,\n",
    "    rXLM_3xA10_32768_df,\n",
    "    rXLM_4xA10_32768_df,\n",
    "    rXLM_8xA10_32768_df,\n",
    "])\n",
    "nlp_full_a10_granulartiy_df[\"comm_time_s_norm\"] = nlp_full_a10_granulartiy_df[\"comm_time_s\"] / 3\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "plt.figure(figsize=(8,3))\n",
    "ax = nlp_full_a10_granulartiy_df[[\"comm_time_s_norm\",\"name\"]].set_index('name').plot(kind='bar', stacked=True)\n",
    "plt.ylabel(\"Time in Seconds\")\n",
    "plt.xlabel(\"\")\n",
    "#plt.legend(labels=[\"Calculation\", \"Communication\"])\n",
    "plt.ylim(0,25)\n",
    "ax.axvline(3.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(7.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "round_bar_value_multi(ax, decimals=2)\n",
    "#show_granulartiy_values(ax=ax, df=nlp_full_a10_granulartiy_df, decimals=1)\n",
    "#save_fig(\"nlp_full-a10_all-tbs_granularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9242873",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.concat(objs=[\n",
    "    rBse_8xA10_32768_df,\n",
    "    rLrg_8xA10_32768_df,\n",
    "    rXLM_8xA10_32768_df,\n",
    "])\n",
    "\n",
    "agg_dict = {\n",
    "    \"name\": [],\n",
    "    \"time_µ\": [],\n",
    "    \"time_type\": []\n",
    "}\n",
    "\n",
    "def to_agg(name, time_s, time_type):\n",
    "    agg_dict[\"name\"].append(name)\n",
    "    agg_dict[\"time_µ\"].append(time_s * 1000 * 1000)\n",
    "    agg_dict[\"time_type\"].append(time_type)\n",
    "\n",
    "for name in temp_df[\"name\"].unique():\n",
    "    row = temp_df.query(f\"name == '{name}'\")\n",
    "    new_name = name.split(\" \")[0]\n",
    "    if new_name == \"RBse\":\n",
    "        param_size = 124.7 * 10**6\n",
    "    elif new_name == \"RLrg\":\n",
    "        param_size = 355.4 * 10**6\n",
    "    elif new_name == \"RXLM\":\n",
    "        param_size = 560.1 * 10**6\n",
    "    to_agg(name=new_name, time_s=row[\"calc_time_s\"].item() / param_size, time_type=\"Calculation\")\n",
    "    to_agg(name=new_name, time_s=row[\"comm_time_s\"].item() / param_size, time_type=\"Communication\")\n",
    "\n",
    "agg_df = pd.DataFrame(agg_dict)\n",
    "#agg_df\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=agg_df,\n",
    "    x=\"name\",\n",
    "    y=\"time_µ\",\n",
    "    hue=\"time_type\",\n",
    "    color=\"0.3\", s=200, style=\"time_type\", zorder=2)\n",
    "\n",
    "plt.ylabel(\"Time in Microseconds per Parameter\")\n",
    "plt.xlabel(\"\")\n",
    "plt.legend(title=\"Type\")\n",
    "#save_fig(\"nlp_8xA10_baseline_calc_vs_comm_plot\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a3e9c",
   "metadata": {},
   "source": [
    "# Geo-distributed Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b89a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_us_1xt4_df = get_baseline_runs(run_names=[\"baseline-75-torchvision.models.convnext_large\"])\n",
    "cv_us_1xt4_df = rename_models(df = cv_us_1xt4_df)\n",
    "cv_us_1xt4_df[\"gpu_count\"] = int(1)\n",
    "cv_us_1xt4_df[\"gpu_type\"] = \"CV A-1\"\n",
    "\n",
    "cv_us_2xt4_df = get_hivemind_runs(run_names=[\"hivemind-81\"], drop_first_epoch=True)\n",
    "cv_us_2xt4_df = rename_models(df = cv_us_2xt4_df)\n",
    "cv_us_2xt4_df[\"gpu_count\"] = int(2)\n",
    "cv_us_2xt4_df[\"gpu_type\"] = \"CV A-2\"\n",
    "\n",
    "cv_us_3xt4_df = get_hivemind_runs(run_names=[\"hivemind-354\"], drop_first_epoch=True)\n",
    "cv_us_3xt4_df = rename_models(df = cv_us_3xt4_df)\n",
    "cv_us_3xt4_df[\"gpu_count\"] = int(3)\n",
    "cv_us_3xt4_df[\"gpu_type\"] = \"CV A-3\"\n",
    "\n",
    "cv_us_4xt4_df = get_hivemind_runs(run_names=[\"hivemind-77\"], drop_first_epoch=True)\n",
    "cv_us_4xt4_df = rename_models(df = cv_us_4xt4_df)\n",
    "cv_us_4xt4_df[\"gpu_count\"] = int(4)\n",
    "cv_us_4xt4_df[\"gpu_type\"] = \"CV A-4\"\n",
    "\n",
    "cv_us_6xt4_df = get_hivemind_runs(run_names=[\"hivemind-114\"], drop_first_epoch=True)\n",
    "cv_us_6xt4_df = rename_models(df = cv_us_6xt4_df)\n",
    "cv_us_6xt4_df[\"gpu_count\"] = int(6)\n",
    "cv_us_6xt4_df[\"gpu_type\"] = \"CV A-6\"\n",
    "\n",
    "cv_us_8xt4_df = get_hivemind_runs(run_names=[\"hivemind-79\"], drop_first_epoch=True)\n",
    "cv_us_8xt4_df = rename_models(df = cv_us_8xt4_df)\n",
    "cv_us_8xt4_df[\"gpu_count\"] = int(8)\n",
    "cv_us_8xt4_df[\"gpu_type\"] = \"CV A-8\"\n",
    "\n",
    "nlp_us_1xt4_df = get_baseline_runs(run_names=[\"baseline-76-roberta_mlm_xlm\"])\n",
    "nlp_us_1xt4_df = rename_models(df = nlp_us_1xt4_df)\n",
    "nlp_us_1xt4_df[\"gpu_count\"] = int(1)\n",
    "nlp_us_1xt4_df[\"gpu_type\"] = \"NLP A-1\"\n",
    "\n",
    "nlp_us_2xt4_df = get_hivemind_runs(run_names=[\"hivemind-82\"], drop_first_epoch=True)\n",
    "nlp_us_2xt4_df = rename_models(df = nlp_us_2xt4_df)\n",
    "nlp_us_2xt4_df[\"gpu_count\"] = int(2)\n",
    "nlp_us_2xt4_df[\"gpu_type\"] = \"NLP A-2\"\n",
    "\n",
    "nlp_us_3xt4_df = get_hivemind_runs(run_names=[\"hivemind-356\"], drop_first_epoch=True)\n",
    "nlp_us_3xt4_df = rename_models(df = nlp_us_3xt4_df)\n",
    "nlp_us_3xt4_df[\"gpu_count\"] = int(3)\n",
    "nlp_us_3xt4_df[\"gpu_type\"] = \"NLP A-3\"\n",
    "\n",
    "nlp_us_4xt4_df = get_hivemind_runs(run_names=[\"hivemind-78\"], drop_first_epoch=True)\n",
    "nlp_us_4xt4_df = rename_models(df = nlp_us_4xt4_df)\n",
    "nlp_us_4xt4_df[\"gpu_count\"] = int(4)\n",
    "nlp_us_4xt4_df[\"gpu_type\"] = \"NLP A-4\"\n",
    "\n",
    "nlp_us_6xt4_df = get_hivemind_runs(run_names=[\"hivemind-115\"], drop_first_epoch=True)\n",
    "nlp_us_6xt4_df = rename_models(df = nlp_us_6xt4_df)\n",
    "nlp_us_6xt4_df[\"gpu_count\"] = int(6)\n",
    "nlp_us_6xt4_df[\"gpu_type\"] = \"NLP A-6\"\n",
    "\n",
    "nlp_us_8xt4_df = get_hivemind_runs(run_names=[\"hivemind-80\"], drop_first_epoch=True)\n",
    "nlp_us_8xt4_df = rename_models(df = nlp_us_8xt4_df)\n",
    "nlp_us_8xt4_df[\"gpu_count\"] = int(8)\n",
    "nlp_us_8xt4_df[\"gpu_type\"] = \"NLP A-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9dc86b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_us_t4_runs_df = pd.concat(objs=[\n",
    "    cv_us_1xt4_df,\n",
    "    cv_us_2xt4_df,\n",
    "    cv_us_3xt4_df,\n",
    "    cv_us_4xt4_df,\n",
    "    cv_us_6xt4_df,\n",
    "    cv_us_8xt4_df,\n",
    "    nlp_us_1xt4_df,\n",
    "    nlp_us_2xt4_df,\n",
    "    nlp_us_3xt4_df,\n",
    "    nlp_us_4xt4_df,\n",
    "    nlp_us_6xt4_df,\n",
    "    nlp_us_8xt4_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15ec57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter local sps out\n",
    "temp_df = full_us_t4_runs_df.query(\"samples_per_sec.notna()\")\n",
    "\n",
    "sns.set(palette=cv_nlp_palette)\n",
    "plt.figure(figsize=(8,3))\n",
    "ax = sns.lineplot(\n",
    "    data=temp_df,\n",
    "    x=\"gpu_count\", y=\"samples_per_sec\", hue=\"model\", style=\"model\",\n",
    "    errorbar=('se', 1), dashes=True,\n",
    "    markers=True,\n",
    "    linewidth=2.0,\n",
    "    markersize=10.0\n",
    ")\n",
    "#plt.title(\"Geo-distributed Performance US-only\")\n",
    "plt.xlabel(\"T4 Count\")\n",
    "plt.ylabel(\"Samples per Second\")\n",
    "plt.legend(title=\"Model\")\n",
    "plt.ylim(0,700)\n",
    "plt.xlim(0,10)\n",
    "#sns.move_legend(ax, \"upper left\", bbox_to_anchor=(0, -0.15))\n",
    "save_fig(\"geo-distributed-performance-US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9091dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "cv_2xUS_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-81\",  name=\"CV A-2\", debug=debug)\n",
    "cv_2xUS_T4_gran_df[\"gpu_count\"] = 2\n",
    "cv_2xUS_T4_gran_df[\"model\"] = \"ConvNextLarge\"\n",
    "cv_3xUS_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-354\",  name=\"CV A-3\", debug=debug)\n",
    "cv_3xUS_T4_gran_df[\"gpu_count\"] = 3\n",
    "cv_3xUS_T4_gran_df[\"model\"] = \"ConvNextLarge\"\n",
    "cv_4xUS_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-77\",  name=\"CV A-4\", debug=debug)\n",
    "cv_4xUS_T4_gran_df[\"gpu_count\"] = 4\n",
    "cv_4xUS_T4_gran_df[\"model\"] = \"ConvNextLarge\"\n",
    "cv_6xUS_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-114\", name=\"CV A-6\", debug=debug)\n",
    "cv_6xUS_T4_gran_df[\"gpu_count\"] = 6\n",
    "cv_6xUS_T4_gran_df[\"model\"] = \"ConvNextLarge\"\n",
    "cv_8xUS_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-79\",  name=\"CV A-8\", debug=debug)\n",
    "cv_8xUS_T4_gran_df[\"gpu_count\"] = 8\n",
    "cv_8xUS_T4_gran_df[\"model\"] = \"ConvNextLarge\"\n",
    "\n",
    "nlp_2xUS_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-82\",  name=\"NLP A-2\", debug=debug)\n",
    "nlp_2xUS_T4_gran_df[\"gpu_count\"] = 2\n",
    "nlp_2xUS_T4_gran_df[\"model\"] = \"RobertaXLM\"\n",
    "nlp_3xUS_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-356\", name=\"NLP A-3\", debug=debug)\n",
    "nlp_3xUS_T4_gran_df[\"gpu_count\"] = 3\n",
    "nlp_3xUS_T4_gran_df[\"model\"] = \"RobertaXLM\"\n",
    "nlp_4xUS_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-78\",  name=\"NLP A-4\", debug=debug)\n",
    "nlp_4xUS_T4_gran_df[\"gpu_count\"] = 4\n",
    "nlp_4xUS_T4_gran_df[\"model\"] = \"RobertaXLM\"\n",
    "nlp_6xUS_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-115\", name=\"NLP A-6\", debug=debug)\n",
    "nlp_6xUS_T4_gran_df[\"gpu_count\"] = 6\n",
    "nlp_6xUS_T4_gran_df[\"model\"] = \"RobertaXLM\"\n",
    "nlp_8xUS_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-80\",  name=\"NLP A-8\", debug=debug)\n",
    "nlp_8xUS_T4_gran_df[\"gpu_count\"] = 8\n",
    "nlp_8xUS_T4_gran_df[\"model\"] = \"RobertaXLM\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5ff0ab",
   "metadata": {},
   "source": [
    "## US (A) Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34249250",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_mean_throughput(model, gpu_count):\n",
    "    TBS = 32768\n",
    "    temp_df = full_us_t4_runs_df.query(f\"samples_per_sec.notna() and TBS=={TBS} and model=='{model}' and gpu_count=={gpu_count}\")\n",
    "    return temp_df[\"samples_per_sec\"].mean()\n",
    "\n",
    "for model in full_us_t4_runs_df[\"model\"].unique():\n",
    "    print(f\"Model {model}:\")\n",
    "    baseline_sps = get_mean_throughput(model=model, gpu_count=1)\n",
    "    for gpu_count in [2,3,4,6,8]:\n",
    "        hivemind_sps = get_mean_throughput(model=model, gpu_count=gpu_count)\n",
    "        print(f\" - Speedup on {gpu_count} GPUS: {round(hivemind_sps / baseline_sps,2)}x, Norm: {round(hivemind_sps / baseline_sps / gpu_count,2)} ({round(hivemind_sps,1)}, {round(baseline_sps,1)})\")\n",
    "\n",
    "\n",
    "TBS = 32768\n",
    "gpu_count = 8\n",
    "cv_8x_a10_mean_sps = full_cv_a10_runs_df.query(f\"samples_per_sec.notna() and TBS=={TBS} and model=='ConvNextLarge' and gpu_count=={gpu_count}\")[\"samples_per_sec\"].mean()\n",
    "nlp_8x_a10_mean_sps = full_nlp_a10_runs_df.query(f\"samples_per_sec.notna() and TBS=={TBS} and model=='RoBERTaXLM' and gpu_count=={gpu_count}\")[\"samples_per_sec\"].mean()\n",
    "cv_8x_t4_mean_sps = get_mean_throughput(model=\"ConvNextLarge\", gpu_count=gpu_count)\n",
    "nlp_8x_t4_mean_sps = get_mean_throughput(model=\"RoBERTaXLM\", gpu_count=gpu_count)\n",
    "print(\"A10 comparison:\")\n",
    "print(f\" - Speedup for model ConvNextLarge for 8xA10 GPUS : {round(cv_8x_a10_mean_sps / cv_8x_t4_mean_sps,2)}x, ({round(cv_8x_a10_mean_sps,1)}, {round(cv_8x_t4_mean_sps,1)})\")\n",
    "print(f\" - Speedup for model RoBERTaXLM    for 8xA10 GPUS : {round(nlp_8x_a10_mean_sps / nlp_8x_t4_mean_sps,2)}x, ({round(nlp_8x_a10_mean_sps,1)}, {round(nlp_8x_t4_mean_sps,1)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "277177a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_t4_granulartiy_df = pd.concat(objs=[\n",
    "    cv_2xUS_T4_gran_df,\n",
    "    cv_3xUS_T4_gran_df,\n",
    "    cv_4xUS_T4_gran_df,\n",
    "    cv_6xUS_T4_gran_df,\n",
    "    cv_8xUS_T4_gran_df,\n",
    "    nlp_2xUS_T4_gran_df,\n",
    "    nlp_3xUS_T4_gran_df,\n",
    "    nlp_4xUS_T4_gran_df,\n",
    "    nlp_6xUS_T4_gran_df,\n",
    "    nlp_8xUS_T4_gran_df,\n",
    "])\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "plt.figure(figsize=(8,8))\n",
    "ax = us_t4_granulartiy_df[[\"calc_time_s\",\"comm_time_s\",\"name\"]].set_index('name').plot(kind='bar', stacked=True, figsize=(8,3))\n",
    "plt.ylabel(\"Time in Seconds\")\n",
    "plt.xlabel(\"\")\n",
    "plt.legend(labels=[\"Calculation\", \"Communication\"])\n",
    "plt.ylim(0,600)\n",
    "ax.axvline(4.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "#ax.axvline(8.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "\n",
    "show_granulartiy_values(ax=ax, df=us_t4_granulartiy_df)\n",
    "save_fig(\"geo-distributed-performance-US-granularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9a3c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_8xA10_32768_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0222554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_t4_granulartiy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e96f18fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_t4_granulartiy_df = pd.concat(objs=[\n",
    "    cv_2xUS_T4_gran_df,\n",
    "    cv_4xUS_T4_gran_df,\n",
    "    cv_6xUS_T4_gran_df,\n",
    "    cv_8xUS_T4_gran_df,\n",
    "    nlp_2xUS_T4_gran_df,\n",
    "    nlp_4xUS_T4_gran_df,\n",
    "    nlp_6xUS_T4_gran_df,\n",
    "    nlp_8xUS_T4_gran_df,\n",
    "])\n",
    "us_t4_granulartiy_df[\"comm_time_s_norm\"] = us_t4_granulartiy_df[\"comm_time_s\"] / 2\n",
    "\n",
    "temp_df = pd.concat(objs=[cv_2xUS_T4_gran_df, conv_2xA10_32768_df])\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "plt.figure(figsize=(8,8))\n",
    "ax = us_t4_granulartiy_df[[\"comm_time_s_norm\",\"name\"]].set_index('name').plot(kind='bar', figsize=(8,3))\n",
    "plt.ylabel(\"Time in Seconds\")\n",
    "plt.xlabel(\"\")\n",
    "#plt.legend(labels=[\"Calculation\", \"Communication\"])\n",
    "plt.ylim(0,25)\n",
    "ax.axvline(4.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "#ax.axvline(8.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "sns.move_legend(ax, \"upper left\")#, bbox_to_anchor=(0, -0.15))\n",
    "round_bar_value_multi(ax, decimals=2)\n",
    "#show_granulartiy_values(ax=ax, df=us_t4_granulartiy_df)\n",
    "#save_fig(\"geo-distributed-performance-US-granularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a90f1004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_comm_time(model, gpu_count):\n",
    "    temp_df = us_t4_granulartiy_df.query(f\"model=='{model}' and gpu_count=={gpu_count}\")\n",
    "    return temp_df[\"comm_time_s\"].mean()\n",
    "\n",
    "for model in us_t4_granulartiy_df[\"model\"].unique():\n",
    "    print(f\"Model {model}:\")\n",
    "    baseline_comm_time_s = get_mean_comm_time(model=model, gpu_count=2)\n",
    "    prev_comm_time_s = baseline_comm_time_s\n",
    "    for gpu_count in [2,4,6,8]:\n",
    "        hivemind_comm_time_s = get_mean_comm_time(model=model, gpu_count=gpu_count)\n",
    "        print(f\" - Communication time on {gpu_count} GPUS: {round(hivemind_comm_time_s,1)}s, Delta: {round(hivemind_comm_time_s - prev_comm_time_s,2)}s\")\n",
    "        prev_comm_time_s = hivemind_comm_time_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2429a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_1xus_1xeu_df = get_hivemind_runs(run_names=[\"hivemind-348\"], drop_first_epoch=True)\n",
    "cv_1xus_1xeu_df = rename_models(df = cv_1xus_1xeu_df)\n",
    "cv_1xus_1xeu_df[\"gpu_count\"] = int(2)\n",
    "cv_1xus_1xeu_df[\"gpu_type\"] = \"EU/US T4\"\n",
    "\n",
    "cv_2xus_2xeu_df = get_hivemind_runs(run_names=[\"hivemind-83\"], drop_first_epoch=True)\n",
    "cv_2xus_2xeu_df = rename_models(df = cv_2xus_2xeu_df)\n",
    "cv_2xus_2xeu_df[\"gpu_count\"] = int(4)\n",
    "cv_2xus_2xeu_df[\"gpu_type\"] = \"EU/US T4\"\n",
    "\n",
    "cv_4xus_2xeu_df = get_hivemind_runs(run_names=[\"hivemind-85\"], drop_first_epoch=True)\n",
    "cv_4xus_2xeu_df = rename_models(df = cv_4xus_2xeu_df)\n",
    "cv_4xus_2xeu_df[\"gpu_count\"] = int(6)\n",
    "cv_4xus_2xeu_df[\"gpu_type\"] = \"EU/US T4\"\n",
    "\n",
    "cv_4xus_4xeu_df = get_hivemind_runs(run_names=[\"hivemind-87\"], drop_first_epoch=True)\n",
    "cv_4xus_4xeu_df = rename_models(df = cv_4xus_4xeu_df)\n",
    "cv_4xus_4xeu_df[\"gpu_count\"] = int(8)\n",
    "cv_4xus_4xeu_df[\"gpu_type\"] = \"EU/US T4\"\n",
    "\n",
    "nlp_1xus_1xeu_df = get_hivemind_runs(run_names=[\"hivemind-349\"], drop_first_epoch=True)\n",
    "nlp_1xus_1xeu_df = rename_models(df = nlp_1xus_1xeu_df)\n",
    "nlp_1xus_1xeu_df[\"gpu_count\"] = int(2)\n",
    "nlp_1xus_1xeu_df[\"gpu_type\"] = \"EU/US T4\"\n",
    "\n",
    "nlp_2xus_2xeu_df = get_hivemind_runs(run_names=[\"hivemind-84\"], drop_first_epoch=True)\n",
    "nlp_2xus_2xeu_df = rename_models(df = nlp_2xus_2xeu_df)\n",
    "nlp_2xus_2xeu_df[\"gpu_count\"] = int(4)\n",
    "nlp_2xus_2xeu_df[\"gpu_type\"] = \"EU/US T4\"\n",
    "\n",
    "nlp_4xus_2xeu_df = get_hivemind_runs(run_names=[\"hivemind-86\"], drop_first_epoch=True)\n",
    "nlp_4xus_2xeu_df = rename_models(df = nlp_4xus_2xeu_df)\n",
    "nlp_4xus_2xeu_df[\"gpu_count\"] = int(6)\n",
    "nlp_4xus_2xeu_df[\"gpu_type\"] = \"EU/US T4\"\n",
    "\n",
    "nlp_4xus_4xeu_df = get_hivemind_runs(run_names=[\"hivemind-88\"], drop_first_epoch=True)\n",
    "nlp_4xus_4xeu_df = rename_models(df = nlp_4xus_4xeu_df)\n",
    "nlp_4xus_4xeu_df[\"gpu_count\"] = int(8)\n",
    "nlp_4xus_4xeu_df[\"gpu_type\"] = \"EU/US T4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "055b49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_us_eu_t4_runs_df = pd.concat(objs=[\n",
    "    cv_us_1xt4_df,\n",
    "    cv_1xus_1xeu_df,\n",
    "    cv_2xus_2xeu_df,\n",
    "    cv_4xus_2xeu_df,\n",
    "    cv_4xus_4xeu_df,\n",
    "    nlp_us_1xt4_df,\n",
    "    nlp_1xus_1xeu_df,\n",
    "    nlp_2xus_2xeu_df,\n",
    "    nlp_4xus_2xeu_df,\n",
    "    nlp_4xus_4xeu_df\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc7d3efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter local sps out\n",
    "temp_df = full_us_eu_t4_runs_df.query(\"samples_per_sec.notna()\")\n",
    "\n",
    "sns.set(palette=cv_nlp_palette)\n",
    "plt.figure(figsize=(8,3))\n",
    "ax = sns.lineplot(\n",
    "    data=temp_df,\n",
    "    x=\"gpu_count\", y=\"samples_per_sec\", hue=\"model\", style=\"model\",\n",
    "    errorbar=('se', 1), dashes=True,\n",
    "    markers=True,\n",
    "    linewidth=2.0,\n",
    "    markersize=10.0\n",
    ")\n",
    "#plt.title(\"Geo-distributed Performance EU-US\")\n",
    "plt.xlabel(\"T4 Count\")\n",
    "plt.legend(title=\"Model\")\n",
    "plt.ylabel(\"Samples per Second\")\n",
    "plt.ylim(0,700)\n",
    "plt.xlim(0,10)\n",
    "sns.move_legend(ax, \"upper left\")#, bbox_to_anchor=(0, -0.15))\n",
    "save_fig(\"geo-distributed-performance-US-EU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd2a41",
   "metadata": {},
   "source": [
    "## US (B) Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb3e5516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_throughput(model, gpu_count, df):\n",
    "    TBS = 32768\n",
    "    temp_df = df.query(f\"samples_per_sec.notna() and TBS=={TBS} and model=='{model}' and gpu_count=={gpu_count}\")\n",
    "    return temp_df[\"samples_per_sec\"].mean()\n",
    "\n",
    "print(\"> US-EU Experiments\\n\")\n",
    "\n",
    "for model in full_us_eu_t4_runs_df[\"model\"].unique():\n",
    "    print(f\"Model {model}:\")\n",
    "    baseline_sps = get_mean_throughput(model=model, gpu_count=1, df=full_us_eu_t4_runs_df)\n",
    "    \n",
    "    last_us_norm = 0\n",
    "    last_us_eu_norm = 0\n",
    "    \n",
    "    for gpu_count in [2,4,6,8]:\n",
    "        hivemind_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_us_eu_t4_runs_df)\n",
    "        hivemind_local_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_us_t4_runs_df)\n",
    "        \n",
    "        us_baseline_speedup    = hivemind_local_sps / baseline_sps\n",
    "        us_norm_baseline_speedup = us_baseline_speedup / gpu_count\n",
    "        \n",
    "        us_eu_baseline_speedup = hivemind_sps / baseline_sps\n",
    "        us_eu_norm_baseline_speedup = us_eu_baseline_speedup / gpu_count\n",
    "        \n",
    "        us_vs_us_eu_speedup    = hivemind_sps / hivemind_local_sps\n",
    "        \n",
    "        print(f\" - US    Speedup vs baseline on {gpu_count} GPUS: {round(us_baseline_speedup,2)}x, Norm: {round(us_norm_baseline_speedup,2)}, Norm delta: {round(last_us_norm - us_norm_baseline_speedup,4)}, ({round(hivemind_local_sps,1)}, {round(baseline_sps,1)})\")\n",
    "        print(f\" - US-EU Speedup vs baseline on {gpu_count} GPUS: {round(us_eu_baseline_speedup,2)}x, Norm: {round(us_eu_norm_baseline_speedup,2)}, Norm delta: {round(last_us_eu_norm - us_eu_norm_baseline_speedup,4)}, ({round(hivemind_sps,1)}, {round(baseline_sps,1)})\")\n",
    "        print(f\" - Speedup vs US-run         on {gpu_count} GPUS: {round(us_vs_us_eu_speedup,2)}x,            ({round(hivemind_sps,1)}, {round(hivemind_local_sps,1)})\")\n",
    "        print(\"\")\n",
    "        \n",
    "        last_us_norm = us_norm_baseline_speedup\n",
    "        last_us_eu_norm = us_eu_norm_baseline_speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0bf1d03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "cv_1xUS_1xEU_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-348\", name=\"CV B-2\", debug=debug)\n",
    "cv_2xUS_2xEU_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-83\", name=\"CV B-4\", debug=debug)\n",
    "cv_4xUS_2xEU_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-85\", name=\"CV B-6\", debug=debug)\n",
    "cv_4xUS_4xEU_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-87\", name=\"CV B-8\", debug=debug)\n",
    "\n",
    "nlp_1xUS_1xEU_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-349\", name=\"NLP B-2\", debug=debug)\n",
    "nlp_2xUS_2xEU_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-84\", name=\"NLP B-4\", debug=debug)\n",
    "nlp_4xUS_2xEU_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-86\", name=\"NLP B-6\", debug=debug)\n",
    "nlp_4xUS_4xEU_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-88\", name=\"NLP B-8\", debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4b3af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_eu_t4_granulartiy_df = pd.concat(objs=[\n",
    "    cv_2xUS_T4_gran_df,\n",
    "    cv_1xUS_1xEU_T4_gran_df,\n",
    "    cv_4xUS_T4_gran_df,\n",
    "    cv_2xUS_2xEU_T4_gran_df,\n",
    "    cv_6xUS_T4_gran_df,\n",
    "    cv_4xUS_2xEU_T4_gran_df,\n",
    "    cv_8xUS_T4_gran_df,\n",
    "    cv_4xUS_4xEU_T4_gran_df,\n",
    "    nlp_2xUS_T4_gran_df,\n",
    "    nlp_1xUS_1xEU_T4_gran_df,\n",
    "    nlp_4xUS_T4_gran_df,\n",
    "    nlp_2xUS_2xEU_T4_gran_df,\n",
    "    nlp_6xUS_T4_gran_df,\n",
    "    nlp_4xUS_2xEU_T4_gran_df,\n",
    "    nlp_8xUS_T4_gran_df,\n",
    "    nlp_4xUS_4xEU_T4_gran_df,\n",
    "])\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "plt.figure(figsize=(8,3))\n",
    "ax = us_eu_t4_granulartiy_df[[\"calc_time_s\",\"comm_time_s\",\"name\"]].set_index('name').plot(kind='bar', stacked=True, figsize=(8,3))\n",
    "plt.ylabel(\"Time in Seconds\")\n",
    "plt.xlabel(\"\")\n",
    "plt.legend(labels=[\"Calculation\", \"Communication\"])\n",
    "plt.ylim(0,650)\n",
    "ax.axvline(1.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(3.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(5.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(7.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(9.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(11.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(13.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "\n",
    "show_granulartiy_values(ax=ax, df=us_eu_t4_granulartiy_df)\n",
    "save_fig(\"geo-distributed-performance-US-EU-granularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57040185",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_1xus_1xeu_1xasia_df = get_hivemind_runs(run_names=[\"hivemind-350\"], drop_first_epoch=True)\n",
    "cv_1xus_1xeu_1xasia_df = rename_models(df = cv_1xus_1xeu_1xasia_df)\n",
    "cv_1xus_1xeu_1xasia_df[\"gpu_count\"] = int(3)\n",
    "cv_1xus_1xeu_1xasia_df[\"gpu_type\"] = \"EU/US/ASIA T4\"\n",
    "\n",
    "cv_1xus_1xeu_1xasia_1xaus_df = get_hivemind_runs(run_names=[\"hivemind-352\"], drop_first_epoch=True)\n",
    "cv_1xus_1xeu_1xasia_1xaus_df = rename_models(df = cv_1xus_1xeu_1xasia_1xaus_df)\n",
    "cv_1xus_1xeu_1xasia_1xaus_df[\"gpu_count\"] = int(4)\n",
    "cv_1xus_1xeu_1xasia_1xaus_df[\"gpu_type\"] = \"EU/US/ASIA/AUS T4\"\n",
    "\n",
    "cv_2xus_2xeu_2xasia_df = get_hivemind_runs(run_names=[\"hivemind-89\"], drop_first_epoch=True)\n",
    "cv_2xus_2xeu_2xasia_df = rename_models(df = cv_2xus_2xeu_2xasia_df)\n",
    "cv_2xus_2xeu_2xasia_df[\"gpu_count\"] = int(6)\n",
    "cv_2xus_2xeu_2xasia_df[\"gpu_type\"] = \"EU/US/ASIA T4\"\n",
    "\n",
    "cv_2xus_2xeu_2xasia_2xaus_df = get_hivemind_runs(run_names=[\"hivemind-92\"], drop_first_epoch=True)\n",
    "cv_2xus_2xeu_2xasia_2xaus_df = rename_models(df = cv_2xus_2xeu_2xasia_2xaus_df)\n",
    "cv_2xus_2xeu_2xasia_2xaus_df[\"gpu_count\"] = int(8)\n",
    "cv_2xus_2xeu_2xasia_2xaus_df[\"gpu_type\"] = \"EU/US/ASIA/AUS T4\"\n",
    "\n",
    "nlp_1xus_1xeu_1xasia_df = get_hivemind_runs(run_names=[\"hivemind-351\"], drop_first_epoch=True)\n",
    "nlp_1xus_1xeu_1xasia_df = rename_models(df = nlp_1xus_1xeu_1xasia_df)\n",
    "nlp_1xus_1xeu_1xasia_df[\"gpu_count\"] = int(3)\n",
    "nlp_1xus_1xeu_1xasia_df[\"gpu_type\"] = \"EU/US/ASIA T4\"\n",
    "\n",
    "nlp_1xus_1xeu_1xasia_1xaus_df = get_hivemind_runs(run_names=[\"hivemind-353\"], drop_first_epoch=True)\n",
    "nlp_1xus_1xeu_1xasia_1xaus_df = rename_models(df = nlp_1xus_1xeu_1xasia_1xaus_df)\n",
    "nlp_1xus_1xeu_1xasia_1xaus_df[\"gpu_count\"] = int(4)\n",
    "nlp_1xus_1xeu_1xasia_1xaus_df[\"gpu_type\"] = \"EU/US/ASIA/AUS T4\"\n",
    "\n",
    "nlp_2xus_2xeu_2xasia_df = get_hivemind_runs(run_names=[\"hivemind-90\"], drop_first_epoch=True)\n",
    "nlp_2xus_2xeu_2xasia_df = rename_models(df = nlp_2xus_2xeu_2xasia_df)\n",
    "nlp_2xus_2xeu_2xasia_df[\"gpu_count\"] = int(6)\n",
    "nlp_2xus_2xeu_2xasia_df[\"gpu_type\"] = \"EU/US/ASIA T4\"\n",
    "\n",
    "nlp_2xus_2xeu_2xasia_2xaus_df = get_hivemind_runs(run_names=[\"hivemind-93\"], drop_first_epoch=True)\n",
    "nlp_2xus_2xeu_2xasia_2xaus_df = rename_models(df = nlp_2xus_2xeu_2xasia_2xaus_df)\n",
    "nlp_2xus_2xeu_2xasia_2xaus_df[\"gpu_count\"] = int(8)\n",
    "nlp_2xus_2xeu_2xasia_2xaus_df[\"gpu_type\"] = \"EU/US/ASIA/AUS T4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a5fdd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_us_eu_asia_aus_t4_runs_df = pd.concat(objs=[\n",
    "    cv_us_1xt4_df,\n",
    "    cv_1xus_1xeu_1xasia_df,\n",
    "    cv_2xus_2xeu_2xasia_df,\n",
    "    cv_1xus_1xeu_1xasia_1xaus_df,\n",
    "    cv_2xus_2xeu_2xasia_2xaus_df,\n",
    "    nlp_us_1xt4_df,\n",
    "    nlp_1xus_1xeu_1xasia_df,\n",
    "    nlp_2xus_2xeu_2xasia_df,\n",
    "    nlp_1xus_1xeu_1xasia_1xaus_df,\n",
    "    nlp_2xus_2xeu_2xasia_2xaus_df,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "53d52cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter local sps out\n",
    "temp_df = full_us_eu_asia_aus_t4_runs_df.query(\"samples_per_sec.notna()\")\n",
    "\n",
    "sns.set(palette=cv_nlp_palette)\n",
    "plt.figure(figsize=(8,3))\n",
    "ax = sns.lineplot(\n",
    "    data=temp_df,\n",
    "    x=\"gpu_count\", y=\"samples_per_sec\", hue=\"model\", style=\"model\",\n",
    "    errorbar=('se', 1), dashes=True,\n",
    "    markers=True,\n",
    "    linewidth=2.0,\n",
    "    markersize=10.0\n",
    ")\n",
    "#plt.title(\"Geo-distributed Performance EU-US-ASIA-AUS\")\n",
    "plt.xlabel(\"T4 Count\")\n",
    "plt.legend(title=\"Model\")\n",
    "plt.ylabel(\"Samples per Second\")\n",
    "plt.ylim(0,700)\n",
    "plt.xlim(0,10)\n",
    "sns.move_legend(ax, \"upper left\")#, bbox_to_anchor=(0, -0.15))\n",
    "save_fig(\"geo-distributed-performance-US-EU-ASIA-AUS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb90d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_throughput(model, gpu_count, df):\n",
    "    TBS = 32768\n",
    "    temp_df = df.query(f\"samples_per_sec.notna() and TBS=={TBS} and model=='{model}' and gpu_count=={gpu_count}\")\n",
    "    return temp_df[\"samples_per_sec\"].mean()\n",
    "\n",
    "print(\"> US-EU-ASIA-AUS Experiments\\n\")\n",
    "\n",
    "for model in full_us_eu_asia_aus_t4_runs_df[\"model\"].unique():\n",
    "    print(f\"Model {model}:\")\n",
    "    # baseline from the A-experiments\n",
    "    baseline_sps = get_mean_throughput(model=model, gpu_count=1, df=full_us_eu_t4_runs_df)\n",
    "    \n",
    "    last_a_norm = 0\n",
    "    last_b_norm = 0\n",
    "    last_c_norm = 0\n",
    "    \n",
    "    for gpu_count in [3,4,6,8]:\n",
    "        c_hivemind_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_us_eu_asia_aus_t4_runs_df)\n",
    "        b_hivemind_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_us_eu_t4_runs_df)\n",
    "        a_hivemind_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_us_t4_runs_df)\n",
    "        \n",
    "        a_baseline_speedup    = a_hivemind_sps / baseline_sps\n",
    "        a_norm_baseline_speedup = a_baseline_speedup / gpu_count\n",
    "        \n",
    "        b_baseline_speedup = b_hivemind_sps / baseline_sps\n",
    "        b_norm_baseline_speedup = b_baseline_speedup / gpu_count\n",
    "        \n",
    "        c_baseline_speedup = c_hivemind_sps / baseline_sps\n",
    "        c_norm_baseline_speedup = c_baseline_speedup / gpu_count\n",
    "        \n",
    "        #us_vs_us_eu_speedup    = hivemind_sps / hivemind_local_sps\n",
    "        \n",
    "        print(f\" - US             Speedup vs baseline on {gpu_count} GPUS: {round(a_baseline_speedup,2)}x, Norm: {round(a_norm_baseline_speedup,2)}, Norm delta: {round(last_a_norm - a_norm_baseline_speedup,4)}, ({round(a_hivemind_sps,1)}, {round(baseline_sps,1)})\")\n",
    "        print(f\" - US-EU          Speedup vs baseline on {gpu_count} GPUS: {round(b_baseline_speedup,2)}x, Norm: {round(b_norm_baseline_speedup,2)}, Norm delta: {round(last_b_norm - b_norm_baseline_speedup,4)}, ({round(b_hivemind_sps,1)}, {round(baseline_sps,1)})\")\n",
    "        print(f\" - US-EU-ASIA-AUS Speedup vs baseline on {gpu_count} GPUS: {round(c_baseline_speedup,2)}x, Norm: {round(c_norm_baseline_speedup,2)}, Norm delta: {round(last_c_norm - c_norm_baseline_speedup,4)}, ({round(c_hivemind_sps,1)}, {round(baseline_sps,1)})\")\n",
    "        print(f\" - US-EU-ASIA-AUS is slower than US-EU by: {round(c_hivemind_sps / b_hivemind_sps,2)}x\")\n",
    "        print(f\" - US-EU-ASIA-AUS is slower than US by: {round(c_hivemind_sps / a_hivemind_sps,2)}x\")\n",
    "\n",
    "        \n",
    "        print(\"\")\n",
    "        \n",
    "        last_a_norm = a_norm_baseline_speedup\n",
    "        last_b_norm = b_norm_baseline_speedup\n",
    "        last_c_norm = c_norm_baseline_speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01153b26",
   "metadata": {},
   "source": [
    "### Full Intercontinental Comparison\n",
    "\n",
    "**Story**\n",
    "\n",
    "**1. Adding more regions with 1 GPUs**\n",
    "- A-3 (single region) vs. C-3 (three regions)\n",
    "- A-4 (single region) vs. C-4 (four regions)\n",
    "- A-4 (single region) vs. B-4 (two regions) vs. C-4 (four regions)\n",
    "\n",
    "**2. Adding more regions with 2 GPUs**\n",
    "- A-6 (single region) vs. B-6 (two regions, unbalanced) vs. C-6 (three regions)\n",
    "- A-8 (single region) vs. B-8 (two regions, balanced) vs. C-8 (four regions)\n",
    "\n",
    "**3. Comparing scalability when adding regions**\n",
    "- C-3 (three regions) vs. C-4 (four regions) + 1xT4 (compare to A-3 and A-4)\n",
    "\n",
    "**4. Comparing scalability when adding more compute**\n",
    "- C-3 (three regions) vs. C-6 (three regions) + 3xT4 (compare to A-3 and A-6)\n",
    "- C-4 (four regions)  vs. C-8 (four regions)  + 4xT4 (doubling local compute, comparing to A-4 and A-8)\n",
    "\n",
    "**5. Comparing scalability when adding regions and more compute**\n",
    "- C-6 (three regions) vs. C-8 (four regions) + 2xT4 (compare to A-6 and A-8)\n",
    "    - Also compare to C-3 -> C-4 per-GPU contribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d6234",
   "metadata": {},
   "source": [
    "**1. Adding more regions with 1 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25785e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_throughput(model, gpu_count, df):\n",
    "    TBS = 32768\n",
    "    temp_df = df.query(f\"samples_per_sec.notna() and TBS=={TBS} and model=='{model}' and gpu_count=={gpu_count}\")\n",
    "    return temp_df[\"samples_per_sec\"].mean()\n",
    "\n",
    "models = full_us_eu_t4_runs_df[\"model\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1e06d515",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A-3 vs C-3 (single region vs three regions):\")\n",
    "print(\"-----------------------\")\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    gpu_count = 3\n",
    "    a_1_sps = get_mean_throughput(model=model, gpu_count=1, df=full_us_eu_t4_runs_df)\n",
    "\n",
    "    a_3_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_us_t4_runs_df)\n",
    "    a_3_norm_sps = a_3_sps / gpu_count\n",
    "    a_3_speedup = a_3_sps / a_1_sps\n",
    "    a_3_norm_speedup = a_3_speedup / gpu_count\n",
    "\n",
    "    c_3_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_us_eu_asia_aus_t4_runs_df)\n",
    "    c_3_norm_sps = c_3_sps / gpu_count\n",
    "    c_3_speedup = c_3_sps / a_1_sps\n",
    "    c_3_norm_speedup = c_3_speedup / gpu_count\n",
    "\n",
    "    \n",
    "    print(f\"{model:>15}, A-{gpu_count}. SPS: {round(a_3_sps,2):>6}, Norm SPS: {round(a_3_norm_sps,2):>6}, Speedup: {round(a_3_speedup,2):>6}, Norm Speedup: {round(a_3_norm_speedup,2):>6}\")\n",
    "    print(f\"{model:>15}, C-{gpu_count}. SPS: {round(c_3_sps,2):>6}, Norm SPS: {round(c_3_norm_sps,2):>6}, Speedup: {round(c_3_speedup,2):>6}, Norm Speedup: {round(c_3_norm_speedup,2):>6}\")\n",
    "    print(f\"   C-3 vs A-3: {round(c_3_sps / a_3_sps,2)}x\")\n",
    "    print(\"-----------------------\")\n",
    "    \n",
    "print(\"A-4 vs C-4 (single region vs four regions):\")\n",
    "print(\"-----------------------\")\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    gpu_count = 4\n",
    "    a_1_sps = get_mean_throughput(model=model, gpu_count=1, df=full_us_eu_t4_runs_df)\n",
    "\n",
    "    a_4_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_us_t4_runs_df)\n",
    "    a_4_norm_sps = a_4_sps / gpu_count\n",
    "    a_4_speedup = a_4_sps / a_1_sps\n",
    "    a_4_norm_speedup = a_4_speedup / gpu_count\n",
    "\n",
    "    b_4_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_us_eu_t4_runs_df)\n",
    "    b_4_norm_sps = b_4_sps / gpu_count\n",
    "    b_4_speedup = b_4_sps / a_1_sps\n",
    "    b_4_norm_speedup = b_4_speedup / gpu_count\n",
    "    \n",
    "    c_4_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_us_eu_asia_aus_t4_runs_df)\n",
    "    c_4_norm_sps = c_4_sps / gpu_count\n",
    "    c_4_speedup = c_4_sps / a_1_sps\n",
    "    c_4_norm_speedup = c_4_speedup / gpu_count\n",
    "\n",
    "    \n",
    "    print(f\"{model:>15}, A-{gpu_count}. SPS: {round(a_4_sps,2):>6}, Norm SPS: {round(a_4_norm_sps,2):>6}, Speedup: {round(a_4_speedup,2):>6}, Norm Speedup: {round(a_4_norm_speedup,2):>6}\")\n",
    "    print(f\"{model:>15}, B-{gpu_count}. SPS: {round(b_4_sps,2):>6}, Norm SPS: {round(b_4_norm_sps,2):>6}, Speedup: {round(b_4_speedup,2):>6}, Norm Speedup: {round(b_4_norm_speedup,2):>6}\")\n",
    "    print(f\"{model:>15}, C-{gpu_count}. SPS: {round(c_4_sps,2):>6}, Norm SPS: {round(c_4_norm_sps,2):>6}, Speedup: {round(c_4_speedup,2):>6}, Norm Speedup: {round(c_4_norm_speedup,2):>6}\")\n",
    "    print(f\"   B-4 vs A-4: {round(b_4_sps / a_4_sps,2)}x\")\n",
    "    print(f\"   C-4 vs A-4: {round(c_4_sps / a_4_sps,2)}x\")\n",
    "    print(\"-----------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a6fccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A-6 vs B-6 vs C-6 (single region vs three regions):\")\n",
    "print(\"-----------------------\")\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    gpu_count = 6\n",
    "    a_1_sps = get_mean_throughput(model=model, gpu_count=1, df=full_us_eu_t4_runs_df)\n",
    "\n",
    "    a_6_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_us_t4_runs_df)\n",
    "    a_6_norm_sps = a_6_sps / gpu_count\n",
    "    a_6_speedup = a_6_sps / a_1_sps\n",
    "    a_6_norm_speedup = a_6_speedup / gpu_count\n",
    "\n",
    "    b_6_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_us_eu_t4_runs_df)\n",
    "    b_6_norm_sps = b_6_sps / gpu_count\n",
    "    b_6_speedup = b_6_sps / a_1_sps\n",
    "    b_6_norm_speedup = b_6_speedup / gpu_count\n",
    "    \n",
    "    c_6_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_us_eu_asia_aus_t4_runs_df)\n",
    "    c_6_norm_sps = c_6_sps / gpu_count\n",
    "    c_6_speedup = c_6_sps / a_1_sps\n",
    "    c_6_norm_speedup = c_6_speedup / gpu_count\n",
    "\n",
    "    \n",
    "    print(f\"{model:>15}, A-{gpu_count}. SPS: {round(a_6_sps,2):>6}, Norm SPS: {round(a_6_norm_sps,2):>6}, Speedup: {round(a_4_speedup,2):>6}, Norm Speedup: {round(a_6_norm_speedup,2):>6}\")\n",
    "    print(f\"{model:>15}, B-{gpu_count}. SPS: {round(b_6_sps,2):>6}, Norm SPS: {round(b_6_norm_sps,2):>6}, Speedup: {round(b_4_speedup,2):>6}, Norm Speedup: {round(b_6_norm_speedup,2):>6}\")\n",
    "    print(f\"{model:>15}, C-{gpu_count}. SPS: {round(c_6_sps,2):>6}, Norm SPS: {round(c_6_norm_sps,2):>6}, Speedup: {round(c_4_speedup,2):>6}, Norm Speedup: {round(c_6_norm_speedup,2):>6}\")\n",
    "    print(f\"   B-6 vs A-6: {round(b_6_sps / a_6_sps,2)}x\")\n",
    "    print(f\"   C-6 vs A-6: {round(c_6_sps / a_6_sps,2)}x\")\n",
    "    print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2a6f2954",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A-8 vs B-8 vs C-8 (single region vs four regions):\")\n",
    "print(\"-----------------------\")\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    gpu_count = 8\n",
    "    a_1_sps = get_mean_throughput(model=model, gpu_count=1, df=full_us_eu_t4_runs_df)\n",
    "\n",
    "    a_8_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_us_t4_runs_df)\n",
    "    a_8_norm_sps = a_8_sps / gpu_count\n",
    "    a_8_speedup = a_8_sps / a_1_sps\n",
    "    a_8_norm_speedup = a_8_speedup / gpu_count\n",
    "\n",
    "    b_8_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_us_eu_t4_runs_df)\n",
    "    b_8_norm_sps = b_8_sps / gpu_count\n",
    "    b_8_speedup = b_8_sps / a_1_sps\n",
    "    b_8_norm_speedup = b_8_speedup / gpu_count\n",
    "    \n",
    "    c_8_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_us_eu_asia_aus_t4_runs_df)\n",
    "    c_8_norm_sps = c_8_sps / gpu_count\n",
    "    c_8_speedup = c_8_sps / a_1_sps\n",
    "    c_8_norm_speedup = c_8_speedup / gpu_count\n",
    "\n",
    "    \n",
    "    print(f\"{model:>15}, A-{gpu_count}. SPS: {round(a_8_sps,2):>6}, Norm SPS: {round(a_8_norm_sps,2):>6}, Speedup: {round(a_8_speedup,2):>6}, Norm Speedup: {round(a_8_norm_speedup,2):>6}\")\n",
    "    print(f\"{model:>15}, B-{gpu_count}. SPS: {round(b_8_sps,2):>6}, Norm SPS: {round(b_8_norm_sps,2):>6}, Speedup: {round(b_8_speedup,2):>6}, Norm Speedup: {round(b_8_norm_speedup,2):>6}\")\n",
    "    print(f\"{model:>15}, C-{gpu_count}. SPS: {round(c_8_sps,2):>6}, Norm SPS: {round(c_8_norm_sps,2):>6}, Speedup: {round(c_8_speedup,2):>6}, Norm Speedup: {round(c_8_norm_speedup,2):>6}\")\n",
    "    print(f\"   B-8 vs A-8: {round(b_8_sps / a_8_sps,2)}x\")\n",
    "    print(f\"   C-8 vs A-8: {round(c_8_sps / a_8_sps,2)}x\")\n",
    "    print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b06f2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "debug = False\n",
    "cv_1xUS_1xEU_1xASIA_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-350\", name=\"CV C-3\", debug=debug)\n",
    "cv_1xUS_1xEU_1xASIA_1xAUS_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-352\", name=\"CV C-4\", debug=debug)\n",
    "\n",
    "nlp_1xUS_1xEU_1xASIA_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-351\", name=\"NLP C-3\", debug=debug)\n",
    "nlp_1xUS_1xEU_1xASIA_1xAUS_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-353\", name=\"NLP C-4\", debug=debug)\n",
    "\n",
    "cv_2xUS_2xEU_2xASIA_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-89\", name=\"CV C-6\", debug=debug)\n",
    "cv_2xUS_2xEU_2xASIA_2xAUS_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-92\", name=\"CV C-8\", debug=debug)\n",
    "\n",
    "nlp_2xUS_2xEU_2xASIA_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-90\", name=\"NLP C-6\", debug=debug)\n",
    "nlp_2xUS_2xEU_2xASIA_2xAUS_T4_gran_df = get_granularity_cumulated(run_name=\"hivemind-93\", name=\"NLP C-8\", debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "903034dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_zone_granularity_df = pd.concat(objs=[\n",
    "    cv_3xUS_T4_gran_df,\n",
    "    cv_1xUS_1xEU_1xASIA_T4_gran_df,\n",
    "    cv_4xUS_T4_gran_df,\n",
    "    cv_1xUS_1xEU_1xASIA_1xAUS_T4_gran_df,\n",
    "    cv_6xUS_T4_gran_df,\n",
    "    cv_2xUS_2xEU_2xASIA_T4_gran_df,\n",
    "    cv_8xUS_T4_gran_df,\n",
    "    cv_2xUS_2xEU_2xASIA_2xAUS_T4_gran_df,\n",
    "    nlp_3xUS_T4_gran_df,\n",
    "    nlp_1xUS_1xEU_1xASIA_T4_gran_df,\n",
    "    nlp_4xUS_T4_gran_df,\n",
    "    nlp_1xUS_1xEU_1xASIA_1xAUS_T4_gran_df,\n",
    "    nlp_6xUS_T4_gran_df,\n",
    "    nlp_2xUS_2xEU_2xASIA_T4_gran_df,\n",
    "    nlp_8xUS_T4_gran_df,\n",
    "    nlp_2xUS_2xEU_2xASIA_2xAUS_T4_gran_df\n",
    "])\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "plt.figure(figsize=(8,3))\n",
    "ax = multi_zone_granularity_df[[\"calc_time_s\",\"comm_time_s\",\"name\"]].set_index('name').plot(kind='bar', stacked=True, figsize=(8,3))\n",
    "plt.ylabel(\"Time in Seconds\")\n",
    "plt.ylim(0,600)\n",
    "plt.legend(labels=[\"Calculation\", \"Communication\"])\n",
    "plt.xlabel(\"\")\n",
    "ax.axvline(1.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(3.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(5.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(7.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(9.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(11.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(13.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "show_granulartiy_values(ax=ax, df=multi_zone_granularity_df)\n",
    "save_fig(\"geo-distributed-performance-US-EU-ASIA-AUS-granularity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af1ae38",
   "metadata": {},
   "source": [
    "# Multi-Cloud Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e6d2e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_4xgc_df = get_hivemind_runs(run_names=[\"hivemind-77\"], drop_first_epoch=True)\n",
    "cv_4xgc_df = rename_models(df = cv_4xgc_df)\n",
    "cv_4xgc_df[\"gpu_count\"] = int(4)\n",
    "cv_4xgc_df[\"gpu_type\"] = \"US T4\"\n",
    "cv_4xgc_df[\"provider\"] = \"D-1\" #\"4xGC\"\n",
    "\n",
    "cv_2xgc_2xaws_df = get_hivemind_runs(run_names=[\"hivemind-113\"], drop_first_epoch=True)\n",
    "cv_2xgc_2xaws_df = rename_models(df = cv_2xgc_2xaws_df)\n",
    "cv_2xgc_2xaws_df[\"gpu_count\"] = int(4)\n",
    "cv_2xgc_2xaws_df[\"gpu_type\"] = \"US T4\"\n",
    "cv_2xgc_2xaws_df[\"provider\"] = \"D-2\" # \"2xGC + 2xAWS\"\n",
    "\n",
    "cv_2xgc_2xazure_df = get_hivemind_runs(run_names=[\"hivemind-117\"], drop_first_epoch=True)\n",
    "cv_2xgc_2xazure_df = rename_models(df = cv_2xgc_2xazure_df)\n",
    "cv_2xgc_2xazure_df[\"gpu_count\"] = int(4)\n",
    "cv_2xgc_2xazure_df[\"gpu_type\"] = \"US T4\"\n",
    "cv_2xgc_2xazure_df[\"provider\"] = \"D-3\" #\"2xGC + 2xAzure\"\n",
    "\n",
    "nlp_4xgc_df = get_hivemind_runs(run_names=[\"hivemind-78\"], drop_first_epoch=True)\n",
    "nlp_4xgc_df = rename_models(df = nlp_4xgc_df)\n",
    "nlp_4xgc_df[\"gpu_count\"] = int(4)\n",
    "nlp_4xgc_df[\"gpu_type\"] = \"US T4\"\n",
    "nlp_4xgc_df[\"provider\"] = \"D-1\" #\"4xGC\"\n",
    "\n",
    "nlp_2xgc_2xaws_df = get_hivemind_runs(run_names=[\"hivemind-112\"], drop_first_epoch=True)\n",
    "nlp_2xgc_2xaws_df = rename_models(df = nlp_2xgc_2xaws_df)\n",
    "nlp_2xgc_2xaws_df[\"gpu_count\"] = int(4)\n",
    "nlp_2xgc_2xaws_df[\"gpu_type\"] = \"US T4\"\n",
    "nlp_2xgc_2xaws_df[\"provider\"] = \"D-2\"#\"2xGC + 2xAWS\"\n",
    "\n",
    "nlp_2xgc_2xazure_df = get_hivemind_runs(run_names=[\"hivemind-118\"], drop_first_epoch=True)\n",
    "nlp_2xgc_2xazure_df = rename_models(df = nlp_2xgc_2xazure_df)\n",
    "nlp_2xgc_2xazure_df[\"gpu_count\"] = int(4)\n",
    "nlp_2xgc_2xazure_df[\"gpu_type\"] = \"US T4\"\n",
    "nlp_2xgc_2xazure_df[\"provider\"] = \"D-3\" #\"2xGC + 2xAzure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c6293c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_multicloud_t4_runs_df = pd.concat(objs=[\n",
    "    cv_4xgc_df,\n",
    "    cv_2xgc_2xaws_df,\n",
    "    cv_2xgc_2xazure_df,\n",
    "    nlp_4xgc_df,\n",
    "    nlp_2xgc_2xaws_df,\n",
    "    nlp_2xgc_2xazure_df\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d1e9eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter local sps out\n",
    "temp_df = full_multicloud_t4_runs_df.query(\"samples_per_sec.notna()\")\n",
    "\n",
    "sns.set(palette=cv_nlp_palette)\n",
    "plt.figure(figsize=(8,3))\n",
    "ax = sns.lineplot(\n",
    "    data=temp_df,\n",
    "    x=\"provider\", y=\"samples_per_sec\", hue=\"model\", style=\"model\",\n",
    "    errorbar=('se', 1), dashes=True,\n",
    "    markers=True,\n",
    "    linewidth=2.0,\n",
    "    markersize=10.0\n",
    ")\n",
    "#plt.title(\"Multicloud Performance\")\n",
    "plt.xlabel(\"T4 Count\")\n",
    "plt.ylabel(\"Samples per Second\")\n",
    "plt.legend(title=\"Model\")\n",
    "#plt.ylim(0,700)\n",
    "#plt.xlim(0,10)\n",
    "#sns.move_legend(ax, \"upper left\", bbox_to_anchor=(0, -0.15))\n",
    "save_fig(\"multi-cloud-performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "831797f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_multicloud_t4_runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a711995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_throughput(model, gpu_count, df, provider):\n",
    "    TBS = 32768\n",
    "    temp_df = df.query(f\"samples_per_sec.notna() and TBS=={TBS} and model=='{model}' and gpu_count=={gpu_count} and provider=='{provider}'\")\n",
    "    return temp_df[\"samples_per_sec\"].mean()\n",
    "\n",
    "print(\"> Multi-Cloud Experiments\\n\")\n",
    "\n",
    "for model in full_multicloud_t4_runs_df[\"model\"].unique():\n",
    "    print(f\"Model {model}:\")\n",
    "    gpu_count = 4\n",
    "    baseline_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_multicloud_t4_runs_df, provider=\"D-1\")\n",
    "    gc_aws_sps   = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_multicloud_t4_runs_df, provider=\"D-2\")\n",
    "    gc_azure_sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=full_multicloud_t4_runs_df, provider=\"D-3\")\n",
    "    \n",
    "    gc_aws_speedup = gc_aws_sps / baseline_sps\n",
    "    gc_azure_speedup = gc_azure_sps / baseline_sps\n",
    "    \n",
    "    print(f\"- 4xGC:         {round(baseline_sps,2)}sps\")\n",
    "    print(f\"- 2xGC+2xAWS:   {round(gc_aws_sps,2)}sps\")\n",
    "    print(f\"   - Speedup vs 4xGC:   {round(gc_aws_speedup,2)}x\")\n",
    "    print(f\"- 2xGC+2xAzure: {round(gc_azure_sps,2)}sps\")\n",
    "    print(f\"   - Speedup vs 4xGC:   {round(gc_azure_speedup,2)}x\")\n",
    "    #us_baseline_speedup    = hivemind_local_sps / baseline_sps\n",
    "    #us_norm_baseline_speedup = us_baseline_speedup / gpu_count\n",
    "\n",
    "    #us_eu_baseline_speedup = hivemind_sps / baseline_sps\n",
    "    #us_eu_norm_baseline_speedup = us_eu_baseline_speedup / gpu_count\n",
    "\n",
    "    #us_vs_us_eu_speedup    = hivemind_sps / hivemind_local_sps\n",
    "\n",
    "    #print(f\" - US    Speedup vs baseline on {gpu_count} GPUS: {round(us_baseline_speedup,2)}x, Norm: {round(us_norm_baseline_speedup,2)}, Norm delta: {round(last_us_norm - us_norm_baseline_speedup,4)}, ({round(hivemind_local_sps,1)}, {round(baseline_sps,1)})\")\n",
    "    #print(f\" - US-EU Speedup vs baseline on {gpu_count} GPUS: {round(us_eu_baseline_speedup,2)}x, Norm: {round(us_eu_norm_baseline_speedup,2)}, Norm delta: {round(last_us_eu_norm - us_eu_norm_baseline_speedup,4)}, ({round(hivemind_sps,1)}, {round(baseline_sps,1)})\")\n",
    "    #print(f\" - Speedup vs US-run         on {gpu_count} GPUS: {round(us_vs_us_eu_speedup,2)}x,            ({round(hivemind_sps,1)}, {round(hivemind_local_sps,1)})\")\n",
    "    #print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cf5590ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "cv_4xGC_gran_df = get_granularity_cumulated(run_name=\"hivemind-77\", name=\"CV D-1\", debug=debug)\n",
    "cv_2xGC_2xAWS_gran_df = get_granularity_cumulated(run_name=\"hivemind-113\", name=\"CV D-2\", debug=debug)\n",
    "cv_2xGC_2xAZU_gran_df = get_granularity_cumulated(run_name=\"hivemind-117\", name=\"CV D-3\", debug=debug)\n",
    "\n",
    "nlp_4xGC_gran_df = get_granularity_cumulated(run_name=\"hivemind-78\", name=\"NLP D-1\", debug=debug)\n",
    "nlp_2xGC_2xAWS_gran_df = get_granularity_cumulated(run_name=\"hivemind-112\", name=\"NLP D-2\", debug=debug)\n",
    "nlp_2xGC_2xAZU_gran_df = get_granularity_cumulated(run_name=\"hivemind-118\", name=\"NLP D-3\", debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "518e56f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_cloud_granularity_df = pd.concat(objs=[\n",
    "    cv_4xGC_gran_df,\n",
    "    cv_2xGC_2xAWS_gran_df,\n",
    "    cv_2xGC_2xAZU_gran_df,\n",
    "    nlp_4xGC_gran_df,\n",
    "    nlp_2xGC_2xAWS_gran_df,\n",
    "    nlp_2xGC_2xAZU_gran_df,\n",
    "])\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "plt.figure(figsize=(8,3))\n",
    "ax = multi_cloud_granularity_df[[\"calc_time_s\",\"comm_time_s\",\"name\"]].set_index('name').plot(kind='bar', stacked=True, figsize=(8,3))\n",
    "plt.ylabel(\"Time in Seconds\")\n",
    "plt.legend(labels=[\"Calculation\", \"Communication\"])\n",
    "plt.xlabel(\"\")\n",
    "plt.ylim(0,600)\n",
    "ax.axvline(2.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "show_granulartiy_values(ax=ax, df=multi_cloud_granularity_df)\n",
    "save_fig(\"multi-cloud-performance-granularity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354ce9a5",
   "metadata": {},
   "source": [
    "# Hybrid Cloud Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d5973",
   "metadata": {},
   "source": [
    "## Startup Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bb52e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_base_rtx8000_df = get_baseline_runs(run_names=[\"baseline-94-torchvision.models.convnext_large\"])\n",
    "cv_base_rtx8000_df = rename_models(df = cv_base_rtx8000_df)\n",
    "cv_base_rtx8000_df[\"gpu_count\"] = int(1)\n",
    "cv_base_rtx8000_df[\"name\"] = \"RTX8000\"\n",
    "cv_base_rtx8000_df[\"gpu_type\"] = \"RTX8000\"\n",
    "\n",
    "cv_eu_1xt4_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-102\"], drop_first_epoch=True)\n",
    "cv_eu_1xt4_rtx8000_df = rename_models(df = cv_eu_1xt4_rtx8000_df)\n",
    "cv_eu_1xt4_rtx8000_df[\"gpu_count\"] = int(2)\n",
    "cv_eu_1xt4_rtx8000_df[\"name\"] = \"E-A-1\"\n",
    "cv_eu_1xt4_rtx8000_df[\"gpu_type\"] = \"EU T4 (E-A)\"\n",
    "\n",
    "cv_eu_2xt4_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-100\"], drop_first_epoch=True)\n",
    "cv_eu_2xt4_rtx8000_df = rename_models(df = cv_eu_2xt4_rtx8000_df)\n",
    "cv_eu_2xt4_rtx8000_df[\"gpu_count\"] = int(3)\n",
    "cv_eu_2xt4_rtx8000_df[\"name\"] = \"E-A-2\"\n",
    "cv_eu_2xt4_rtx8000_df[\"gpu_type\"] = \"EU T4 (E-A)\"\n",
    "\n",
    "cv_eu_4xt4_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-98\"], drop_first_epoch=True)\n",
    "cv_eu_4xt4_rtx8000_df = rename_models(df = cv_eu_4xt4_rtx8000_df)\n",
    "cv_eu_4xt4_rtx8000_df[\"gpu_count\"] = int(5)\n",
    "cv_eu_4xt4_rtx8000_df[\"name\"] = \"E-A-4\"\n",
    "cv_eu_4xt4_rtx8000_df[\"gpu_type\"] = \"EU T4 (E-A)\"\n",
    "\n",
    "cv_eu_8xt4_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-96\"], drop_first_epoch=True)\n",
    "cv_eu_8xt4_rtx8000_df = rename_models(df = cv_eu_8xt4_rtx8000_df)\n",
    "cv_eu_8xt4_rtx8000_df[\"gpu_count\"] = int(9)\n",
    "cv_eu_8xt4_rtx8000_df[\"name\"] = \"E-A-8\"\n",
    "cv_eu_8xt4_rtx8000_df[\"gpu_type\"] = \"EU T4 (E-A)\"\n",
    "\n",
    "nlp_base_rtx8000_df = get_baseline_runs(run_names=[\"baseline-95-roberta_mlm_xlm\"])\n",
    "nlp_base_rtx8000_df = rename_models(df = nlp_base_rtx8000_df)\n",
    "nlp_base_rtx8000_df[\"gpu_count\"] = int(1)\n",
    "nlp_base_rtx8000_df[\"name\"] = \"RTX8000\"\n",
    "nlp_base_rtx8000_df[\"gpu_type\"] = \"RTX8000\"\n",
    "\n",
    "nlp_eu_1xt4_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-103\"], drop_first_epoch=True)\n",
    "nlp_eu_1xt4_rtx8000_df = rename_models(df = nlp_eu_1xt4_rtx8000_df)\n",
    "nlp_eu_1xt4_rtx8000_df[\"gpu_count\"] = int(2)\n",
    "nlp_eu_1xt4_rtx8000_df[\"name\"] = \"E-A-1\"\n",
    "nlp_eu_1xt4_rtx8000_df[\"gpu_type\"] = \"EU T4 (E-A)\"\n",
    "\n",
    "nlp_eu_2xt4_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-101\"], drop_first_epoch=True)\n",
    "nlp_eu_2xt4_rtx8000_df = rename_models(df = nlp_eu_2xt4_rtx8000_df)\n",
    "nlp_eu_2xt4_rtx8000_df[\"gpu_count\"] = int(3)\n",
    "nlp_eu_2xt4_rtx8000_df[\"name\"] = \"E-A-2\"\n",
    "nlp_eu_2xt4_rtx8000_df[\"gpu_type\"] = \"EU T4 (E-A)\"\n",
    "\n",
    "nlp_eu_4xt4_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-99\"], drop_first_epoch=True)\n",
    "nlp_eu_4xt4_rtx8000_df = rename_models(df = nlp_eu_4xt4_rtx8000_df)\n",
    "nlp_eu_4xt4_rtx8000_df[\"gpu_count\"] = int(5)\n",
    "nlp_eu_4xt4_rtx8000_df[\"name\"] = \"E-A-4\"\n",
    "nlp_eu_4xt4_rtx8000_df[\"gpu_type\"] = \"EU T4 (E-A)\"\n",
    "\n",
    "nlp_eu_8xt4_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-97\"], drop_first_epoch=True)\n",
    "nlp_eu_8xt4_rtx8000_df = rename_models(df = nlp_eu_8xt4_rtx8000_df)\n",
    "nlp_eu_8xt4_rtx8000_df[\"gpu_count\"] = int(9)\n",
    "nlp_eu_8xt4_rtx8000_df[\"name\"] = \"E-A-8\"\n",
    "nlp_eu_8xt4_rtx8000_df[\"gpu_type\"] = \"EU T4 (E-A)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e925cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtx8000_eu_runs_df = pd.concat(objs=[\n",
    "    cv_base_rtx8000_df,\n",
    "    cv_eu_1xt4_rtx8000_df,\n",
    "    cv_eu_2xt4_rtx8000_df,\n",
    "    cv_eu_4xt4_rtx8000_df,\n",
    "    cv_eu_8xt4_rtx8000_df,\n",
    "    nlp_base_rtx8000_df,\n",
    "    nlp_eu_1xt4_rtx8000_df,\n",
    "    nlp_eu_2xt4_rtx8000_df,\n",
    "    nlp_eu_4xt4_rtx8000_df,\n",
    "    nlp_eu_8xt4_rtx8000_df\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "66b50c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_us_1xt4_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-135\"], drop_first_epoch=True)\n",
    "cv_us_1xt4_rtx8000_df = rename_models(df = cv_us_1xt4_rtx8000_df)\n",
    "cv_us_1xt4_rtx8000_df[\"gpu_count\"] = int(2)\n",
    "cv_us_1xt4_rtx8000_df[\"name\"] = \"E-B-1\"\n",
    "cv_us_1xt4_rtx8000_df[\"gpu_type\"] = \"US T4 (E-B)\"\n",
    "\n",
    "cv_us_2xt4_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-129\"], drop_first_epoch=True)\n",
    "cv_us_2xt4_rtx8000_df = rename_models(df = cv_us_2xt4_rtx8000_df)\n",
    "cv_us_2xt4_rtx8000_df[\"gpu_count\"] = int(3)\n",
    "cv_us_2xt4_rtx8000_df[\"name\"] = \"E-B-2\"\n",
    "cv_us_2xt4_rtx8000_df[\"gpu_type\"] = \"US T4 (E-B)\"\n",
    "\n",
    "cv_us_4xt4_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-127\"], drop_first_epoch=True)\n",
    "cv_us_4xt4_rtx8000_df = rename_models(df = cv_us_4xt4_rtx8000_df)\n",
    "cv_us_4xt4_rtx8000_df[\"gpu_count\"] = int(5)\n",
    "cv_us_4xt4_rtx8000_df[\"name\"] = \"E-B-4\"\n",
    "cv_us_4xt4_rtx8000_df[\"gpu_type\"] = \"US T4 (E-B)\"\n",
    "\n",
    "cv_us_8xt4_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-121\"], drop_first_epoch=True)\n",
    "cv_us_8xt4_rtx8000_df = rename_models(df = cv_us_8xt4_rtx8000_df)\n",
    "cv_us_8xt4_rtx8000_df[\"gpu_count\"] = int(9)\n",
    "cv_us_8xt4_rtx8000_df[\"name\"] = \"E-B-8\"\n",
    "cv_us_8xt4_rtx8000_df[\"gpu_type\"] = \"US T4 (E-B)\"\n",
    "\n",
    "nlp_us_1xt4_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-136\"], drop_first_epoch=True)\n",
    "nlp_us_1xt4_rtx8000_df = rename_models(df = nlp_us_1xt4_rtx8000_df)\n",
    "nlp_us_1xt4_rtx8000_df[\"gpu_count\"] = int(2)\n",
    "nlp_us_1xt4_rtx8000_df[\"name\"] = \"E-B-1\"\n",
    "nlp_us_1xt4_rtx8000_df[\"gpu_type\"] = \"US T4 (E-B)\"\n",
    "\n",
    "nlp_us_2xt4_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-130\"], drop_first_epoch=True)\n",
    "nlp_us_2xt4_rtx8000_df = rename_models(df = nlp_us_2xt4_rtx8000_df)\n",
    "nlp_us_2xt4_rtx8000_df[\"gpu_count\"] = int(3)\n",
    "nlp_us_2xt4_rtx8000_df[\"name\"] = \"E-B-2\"\n",
    "nlp_us_2xt4_rtx8000_df[\"gpu_type\"] = \"US T4 (E-B)\"\n",
    "\n",
    "nlp_us_4xt4_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-128\"], drop_first_epoch=True)\n",
    "nlp_us_4xt4_rtx8000_df = rename_models(df = nlp_us_4xt4_rtx8000_df)\n",
    "nlp_us_4xt4_rtx8000_df[\"gpu_count\"] = int(5)\n",
    "nlp_us_4xt4_rtx8000_df[\"name\"] = \"E-B-4\"\n",
    "nlp_us_4xt4_rtx8000_df[\"gpu_type\"] = \"US T4 (E-B)\"\n",
    "\n",
    "nlp_us_8xt4_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-122\"], drop_first_epoch=True)\n",
    "nlp_us_8xt4_rtx8000_df = rename_models(df = nlp_us_8xt4_rtx8000_df)\n",
    "nlp_us_8xt4_rtx8000_df[\"gpu_count\"] = int(9)\n",
    "nlp_us_8xt4_rtx8000_df[\"name\"] = \"E-B-8\"\n",
    "nlp_us_8xt4_rtx8000_df[\"gpu_type\"] = \"US T4 (E-B)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "741cabea",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtx8000_us_runs_df = pd.concat(objs=[\n",
    "    cv_us_1xt4_rtx8000_df,\n",
    "    cv_us_2xt4_rtx8000_df,\n",
    "    cv_us_4xt4_rtx8000_df,\n",
    "    cv_us_8xt4_rtx8000_df,\n",
    "    nlp_us_1xt4_rtx8000_df,\n",
    "    nlp_us_2xt4_rtx8000_df,\n",
    "    nlp_us_4xt4_rtx8000_df,\n",
    "    nlp_us_8xt4_rtx8000_df\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4bf51e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_us_1xa10_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-151\"], drop_first_epoch=True)\n",
    "cv_us_1xa10_rtx8000_df = rename_models(df = cv_us_1xa10_rtx8000_df)\n",
    "cv_us_1xa10_rtx8000_df[\"gpu_count\"] = int(2)\n",
    "cv_us_1xa10_rtx8000_df[\"name\"] = \"E-C-1\"\n",
    "cv_us_1xa10_rtx8000_df[\"gpu_type\"] = \"US A10 (E-C)\"\n",
    "\n",
    "cv_us_2xa10_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-147\"], drop_first_epoch=True)\n",
    "cv_us_2xa10_rtx8000_df = rename_models(df = cv_us_2xa10_rtx8000_df)\n",
    "cv_us_2xa10_rtx8000_df[\"gpu_count\"] = int(3)\n",
    "cv_us_2xa10_rtx8000_df[\"name\"] = \"E-C-2\"\n",
    "cv_us_2xa10_rtx8000_df[\"gpu_type\"] = \"US A10 (E-C)\"\n",
    "\n",
    "cv_us_4xa10_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-338\"], drop_first_epoch=True)\n",
    "cv_us_4xa10_rtx8000_df = rename_models(df = cv_us_4xa10_rtx8000_df)\n",
    "cv_us_4xa10_rtx8000_df[\"gpu_count\"] = int(5)\n",
    "cv_us_4xa10_rtx8000_df[\"name\"] = \"E-C-4\"\n",
    "cv_us_4xa10_rtx8000_df[\"gpu_type\"] = \"US A10 (E-C)\"\n",
    "\n",
    "cv_us_8xa10_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-139\"], drop_first_epoch=True)\n",
    "cv_us_8xa10_rtx8000_df = rename_models(df = cv_us_8xa10_rtx8000_df)\n",
    "cv_us_8xa10_rtx8000_df[\"gpu_count\"] = int(9)\n",
    "cv_us_8xa10_rtx8000_df[\"name\"] = \"E-C-8\"\n",
    "cv_us_8xa10_rtx8000_df[\"gpu_type\"] = \"US A10 (E-C)\"\n",
    "\n",
    "nlp_us_1xa10_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-152\"], drop_first_epoch=True)\n",
    "nlp_us_1xa10_rtx8000_df = rename_models(df = nlp_us_1xa10_rtx8000_df)\n",
    "nlp_us_1xa10_rtx8000_df[\"gpu_count\"] = int(2)\n",
    "nlp_us_1xa10_rtx8000_df[\"name\"] = \"E-C-1\"\n",
    "nlp_us_1xa10_rtx8000_df[\"gpu_type\"] = \"US A10 (E-C)\"\n",
    "\n",
    "nlp_us_2xa10_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-148\"], drop_first_epoch=True)\n",
    "nlp_us_2xa10_rtx8000_df = rename_models(df = nlp_us_2xa10_rtx8000_df)\n",
    "nlp_us_2xa10_rtx8000_df[\"gpu_count\"] = int(3)\n",
    "nlp_us_2xa10_rtx8000_df[\"name\"] = \"E-C-2\"\n",
    "nlp_us_2xa10_rtx8000_df[\"gpu_type\"] = \"US A10 (E-C)\"\n",
    "\n",
    "nlp_us_4xa10_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-144\"], drop_first_epoch=True)\n",
    "nlp_us_4xa10_rtx8000_df = rename_models(df = nlp_us_4xa10_rtx8000_df)\n",
    "nlp_us_4xa10_rtx8000_df[\"gpu_count\"] = int(5)\n",
    "nlp_us_4xa10_rtx8000_df[\"name\"] = \"E-C-4\"\n",
    "nlp_us_4xa10_rtx8000_df[\"gpu_type\"] = \"US A10 (E-C)\"\n",
    "\n",
    "nlp_us_8xa10_rtx8000_df = get_hivemind_runs(run_names=[\"hivemind-140\"], drop_first_epoch=True)\n",
    "nlp_us_8xa10_rtx8000_df = rename_models(df = nlp_us_8xa10_rtx8000_df)\n",
    "nlp_us_8xa10_rtx8000_df[\"gpu_count\"] = int(9)\n",
    "nlp_us_8xa10_rtx8000_df[\"name\"] = \"E-C-8\"\n",
    "nlp_us_8xa10_rtx8000_df[\"gpu_type\"] = \"US A10 (E-C)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e5eca718",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtx8000_us_a10_runs_df = pd.concat(objs=[\n",
    "    cv_us_1xa10_rtx8000_df,\n",
    "    cv_us_2xa10_rtx8000_df,\n",
    "    cv_us_4xa10_rtx8000_df,\n",
    "    cv_us_8xa10_rtx8000_df,\n",
    "    nlp_us_1xa10_rtx8000_df,\n",
    "    nlp_us_2xa10_rtx8000_df,\n",
    "    nlp_us_4xa10_rtx8000_df,\n",
    "    nlp_us_8xa10_rtx8000_df\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8720bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtx8000_all_runs_df = pd.concat(objs=[rtx8000_eu_runs_df, rtx8000_us_runs_df, rtx8000_us_a10_runs_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c1fc3365",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# filter local sps out\n",
    "temp_df = rtx8000_all_runs_df.query(\"samples_per_sec.notna()\")\n",
    "temp_df = temp_df.query(\"model=='ConvNextLarge'\")\n",
    "\n",
    "sns.set(palette=cv_palette)\n",
    "plt.figure(figsize=(8,3))\n",
    "ax = sns.lineplot(\n",
    "    data=temp_df,\n",
    "    x=\"gpu_count\", y=\"samples_per_sec\", hue=\"gpu_type\", style=\"gpu_type\",\n",
    "    errorbar=('se', 1), dashes=True,\n",
    "    markers=True,\n",
    "    linewidth=1.0,\n",
    "    markersize=10.0\n",
    ")\n",
    "#plt.title(\"Private Setting\")\n",
    "plt.xlabel(\"GPU Count\")\n",
    "plt.ylim(0,900)\n",
    "plt.ylabel(\"Samples per Second\")\n",
    "plt.legend(title=\"GPU Type\")\n",
    "ax.axhline(195, color=\"#ffe793\", linestyle=\"-\")\n",
    "#plt.xlim(0,10)\n",
    "#sns.move_legend(ax, \"upper left\", bbox_to_anchor=(0, -0.15))\n",
    "save_fig(\"cv-private-setting-performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f10a4196",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtx8000_all_runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bb421d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_throughput(model, gpu_count, df, gpu_type):\n",
    "    TBS = 32768\n",
    "    temp_df = df.query(f\"samples_per_sec.notna() and TBS=={TBS} and model=='{model}' and gpu_count=={gpu_count} and gpu_type=='{gpu_type}'\")\n",
    "    return temp_df[\"samples_per_sec\"].mean()\n",
    "\n",
    "\n",
    "#print(\"CV, EU T4 vs. US T4 vs. US A10\")\n",
    "#print(\"-----------------------\")\n",
    "\n",
    "# rtx8000_all_runs_df = pd.concat(objs=[rtx8000_eu_runs_df, rtx8000_us_runs_df, rtx8000_us_a10_runs_df])\n",
    "\n",
    "for model in rtx8000_all_runs_df[\"model\"].unique():\n",
    "    for gpu_type in [\"EU T4 (E-A)\", \"US T4 (E-B)\", \"US A10 (E-C)\"]:\n",
    "        print(f\"Model: {model}, Type: {gpu_type}\")\n",
    "        baseline_sps = get_mean_throughput(model=model, gpu_count=1, df=rtx8000_all_runs_df, gpu_type=\"RTX8000\")\n",
    "        print(f\"   1 GPU, SPS: {round(baseline_sps,2):>6}\")\n",
    "        for gpu_count in [2,3,5,9]:\n",
    "            sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=rtx8000_all_runs_df, gpu_type=gpu_type)\n",
    "            norm_sps = sps / gpu_count\n",
    "            speedup = sps / baseline_sps\n",
    "            norm_speedup = speedup / gpu_count\n",
    "            \n",
    "            print(f\"  {gpu_count} GPUs, SPS: {round(sps,2):>6}, Norm SPS: {round(norm_sps,2):>6}, Speedup: {round(speedup,2):>6}, Norm Speedup: {round(norm_speedup,2):>6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5f409425",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = rtx8000_all_runs_df.query(\"samples_per_sec.notna()\")\n",
    "temp_df = temp_df.query(\"model=='RoBERTaXLM'\")\n",
    "\n",
    "sns.set(palette=nlp_ext_palette)\n",
    "plt.figure(figsize=(8,3))\n",
    "ax = sns.lineplot(\n",
    "    data=temp_df,\n",
    "    x=\"gpu_count\", y=\"samples_per_sec\", hue=\"gpu_type\", style=\"gpu_type\",\n",
    "    errorbar=('se', 1), dashes=True,\n",
    "    markers=True,\n",
    "    linewidth=1.0,\n",
    "    markersize=10.0\n",
    ")\n",
    "#plt.title(\"Private Setting\")\n",
    "plt.xlabel(\"GPU Count\")\n",
    "plt.ylim(0,700)\n",
    "plt.ylabel(\"Samples per Second\")\n",
    "plt.legend(title=\"GPU Type\")\n",
    "ax.axhline(430, color=\"#d6efb3\", linestyle=\"-\")\n",
    "#plt.xlim(0,10)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(0.062, 1))\n",
    "save_fig(\"nlp-private-setting-performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "56f473ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "cv_eu_1xt4_rtx8000_gran_df   = get_granularity_cumulated(run_name=\"hivemind-102\", name=\"E-A-1\", debug=debug)\n",
    "cv_eu_2xt4_rtx8000_gran_df   = get_granularity_cumulated(run_name=\"hivemind-100\", name=\"E-A-2\", debug=debug)\n",
    "cv_eu_4xt4_rtx8000_gran_df   = get_granularity_cumulated(run_name=\"hivemind-98\",  name=\"E-A-4\", debug=debug)\n",
    "cv_eu_8xt4_rtx8000_gran_df   = get_granularity_cumulated(run_name=\"hivemind-96\",  name=\"E-A-8\", debug=debug)\n",
    "nlp_eu_1xt4_rtx8000_gran_df  = get_granularity_cumulated(run_name=\"hivemind-103\", name=\"E-A-1\", debug=debug)\n",
    "nlp_eu_2xt4_rtx8000_gran_df  = get_granularity_cumulated(run_name=\"hivemind-101\", name=\"E-A-2\", debug=debug)\n",
    "nlp_eu_4xt4_rtx8000_gran_df  = get_granularity_cumulated(run_name=\"hivemind-99\",  name=\"E-A-4\", debug=debug)\n",
    "nlp_eu_8xt4_rtx8000_gran_df  = get_granularity_cumulated(run_name=\"hivemind-97\",  name=\"E-A-8\", debug=debug)\n",
    "cv_us_1xt4_rtx8000_gran_df   = get_granularity_cumulated(run_name=\"hivemind-135\", name=\"E-B-1\", debug=debug)\n",
    "cv_us_2xt4_rtx8000_gran_df   = get_granularity_cumulated(run_name=\"hivemind-129\", name=\"E-B-2\", debug=debug)\n",
    "cv_us_4xt4_rtx8000_gran_df   = get_granularity_cumulated(run_name=\"hivemind-127\", name=\"E-B-4\", debug=debug)\n",
    "cv_us_8xt4_rtx8000_gran_df   = get_granularity_cumulated(run_name=\"hivemind-121\", name=\"E-B-8\", debug=debug)\n",
    "nlp_us_1xt4_rtx8000_gran_df  = get_granularity_cumulated(run_name=\"hivemind-136\", name=\"E-B-1\", debug=debug)\n",
    "nlp_us_2xt4_rtx8000_gran_df  = get_granularity_cumulated(run_name=\"hivemind-130\", name=\"E-B-2\", debug=debug)\n",
    "nlp_us_4xt4_rtx8000_gran_df  = get_granularity_cumulated(run_name=\"hivemind-128\", name=\"E-B-4\", debug=debug)\n",
    "nlp_us_8xt4_rtx8000_gran_df  = get_granularity_cumulated(run_name=\"hivemind-122\", name=\"E-B-8\", debug=debug)\n",
    "cv_us_1xa10_rtx8000_gran_df  = get_granularity_cumulated(run_name=\"hivemind-151\", name=\"E-C-1\", debug=debug)\n",
    "cv_us_2xa10_rtx8000_gran_df  = get_granularity_cumulated(run_name=\"hivemind-147\", name=\"E-C-2\", debug=debug)\n",
    "cv_us_4xa10_rtx8000_gran_df  = get_granularity_cumulated(run_name=\"hivemind-338\", name=\"E-C-4\", debug=debug)\n",
    "cv_us_8xa10_rtx8000_gran_df  = get_granularity_cumulated(run_name=\"hivemind-139\", name=\"E-C-8\", debug=debug)\n",
    "nlp_us_1xa10_rtx8000_gran_df = get_granularity_cumulated(run_name=\"hivemind-152\", name=\"E-C-1\", debug=debug)\n",
    "nlp_us_2xa10_rtx8000_gran_df = get_granularity_cumulated(run_name=\"hivemind-148\", name=\"E-C-2\", debug=debug)\n",
    "nlp_us_4xa10_rtx8000_gran_df = get_granularity_cumulated(run_name=\"hivemind-144\", name=\"E-C-4\", debug=debug)\n",
    "nlp_us_8xa10_rtx8000_gran_df = get_granularity_cumulated(run_name=\"hivemind-140\", name=\"E-C-8\", debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "490e931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_us_4xa10_rtx8000_gran_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cf67db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_private_cloud_granularity_df = pd.concat(objs=[\n",
    "    cv_eu_1xt4_rtx8000_gran_df,   \n",
    "    cv_eu_2xt4_rtx8000_gran_df,   \n",
    "    cv_eu_4xt4_rtx8000_gran_df,   \n",
    "    cv_eu_8xt4_rtx8000_gran_df,    \n",
    "    cv_us_1xt4_rtx8000_gran_df,   \n",
    "    cv_us_2xt4_rtx8000_gran_df,   \n",
    "    cv_us_4xt4_rtx8000_gran_df,   \n",
    "    cv_us_8xt4_rtx8000_gran_df,   \n",
    "    cv_us_1xa10_rtx8000_gran_df,  \n",
    "    cv_us_2xa10_rtx8000_gran_df,  \n",
    "    cv_us_4xa10_rtx8000_gran_df,  \n",
    "    cv_us_8xa10_rtx8000_gran_df\n",
    "])\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "plt.figure(figsize=(8,3))\n",
    "ax = cv_private_cloud_granularity_df[[\"calc_time_s\",\"comm_time_s\",\"name\"]].set_index('name').plot(kind='bar', stacked=True, figsize=(8,3))\n",
    "plt.ylabel(\"Time in Seconds\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylim(0,600)\n",
    "plt.legend(labels=[\"Calculation\", \"Communication\"])\n",
    "ax.axvline(3.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(7.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "show_granulartiy_values(ax=ax, df=cv_private_cloud_granularity_df)\n",
    "save_fig(\"cv-private-cloud-performance-granularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dc381ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_private_cloud_granularity_df = pd.concat(objs=[  \n",
    "    nlp_eu_1xt4_rtx8000_gran_df,  \n",
    "    nlp_eu_2xt4_rtx8000_gran_df,  \n",
    "    nlp_eu_4xt4_rtx8000_gran_df,  \n",
    "    nlp_eu_8xt4_rtx8000_gran_df,  \n",
    "    nlp_us_1xt4_rtx8000_gran_df,  \n",
    "    nlp_us_2xt4_rtx8000_gran_df,  \n",
    "    nlp_us_4xt4_rtx8000_gran_df,  \n",
    "    nlp_us_8xt4_rtx8000_gran_df,  \n",
    "    nlp_us_1xa10_rtx8000_gran_df, \n",
    "    nlp_us_2xa10_rtx8000_gran_df, \n",
    "    nlp_us_4xa10_rtx8000_gran_df, \n",
    "    nlp_us_8xa10_rtx8000_gran_df, \n",
    "])\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "plt.figure(figsize=(8,3))\n",
    "ax = nlp_private_cloud_granularity_df[[\"calc_time_s\",\"comm_time_s\",\"name\"]].set_index('name').plot(kind='bar', stacked=True, figsize=(8,3))\n",
    "plt.ylabel(\"Time in Seconds\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylim(0,600)\n",
    "plt.legend(labels=[\"Calculation\", \"Communication\"])\n",
    "ax.axvline(3.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(7.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "show_granulartiy_values(ax=ax, df=nlp_private_cloud_granularity_df)\n",
    "save_fig(\"nlp-private-cloud-performance-granularity\")\n",
    "sns.move_legend(ax, \"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c73f6",
   "metadata": {},
   "source": [
    "### Research Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6a305a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_base_v100_df = get_baseline_runs(run_names=[\"baseline-119-torchvision.models.convnext_large\"])\n",
    "cv_base_v100_df = rename_models(df = cv_base_v100_df)\n",
    "cv_base_v100_df[\"gpu_count\"] = int(1)\n",
    "cv_base_v100_df[\"name\"] = \"Baseline\"\n",
    "cv_base_v100_df[\"gpu_type\"] = \"8xV100\"\n",
    "\n",
    "cv_eu_1xt4_v100_df = get_hivemind_runs(run_names=[\"hivemind-159\"], drop_first_epoch=True)\n",
    "cv_eu_1xt4_v100_df = rename_models(df = cv_eu_1xt4_v100_df)\n",
    "cv_eu_1xt4_v100_df[\"gpu_count\"] = int(2)\n",
    "cv_eu_1xt4_v100_df[\"name\"] = \"F-A-1\"\n",
    "cv_eu_1xt4_v100_df[\"gpu_type\"] = \"EU T4 (F-A)\"\n",
    "\n",
    "cv_eu_2xt4_v100_df = get_hivemind_runs(run_names=[\"hivemind-157\"], drop_first_epoch=True)\n",
    "cv_eu_2xt4_v100_df = rename_models(df = cv_eu_2xt4_v100_df)\n",
    "cv_eu_2xt4_v100_df[\"gpu_count\"] = int(3)\n",
    "cv_eu_2xt4_v100_df[\"name\"] = \"F-A-2\"\n",
    "cv_eu_2xt4_v100_df[\"gpu_type\"] = \"EU T4 (F-A)\"\n",
    "\n",
    "cv_eu_4xt4_v100_df = get_hivemind_runs(run_names=[\"hivemind-155\"], drop_first_epoch=True)\n",
    "cv_eu_4xt4_v100_df = rename_models(df = cv_eu_4xt4_v100_df)\n",
    "cv_eu_4xt4_v100_df[\"gpu_count\"] = int(5)\n",
    "cv_eu_4xt4_v100_df[\"name\"] = \"F-A-4\"\n",
    "cv_eu_4xt4_v100_df[\"gpu_type\"] = \"EU T4 (F-A)\"\n",
    "\n",
    "cv_eu_8xt4_v100_df = get_hivemind_runs(run_names=[\"hivemind-153\"], drop_first_epoch=True)\n",
    "cv_eu_8xt4_v100_df = rename_models(df = cv_eu_8xt4_v100_df)\n",
    "cv_eu_8xt4_v100_df[\"gpu_count\"] = int(9)\n",
    "cv_eu_8xt4_v100_df[\"name\"] = \"F-A-8\"\n",
    "cv_eu_8xt4_v100_df[\"gpu_type\"] = \"EU T4 (F-A)\"\n",
    "\n",
    "nlp_base_v100_df = get_baseline_runs(run_names=[\"baseline-120-roberta_mlm_xlm\"])\n",
    "nlp_base_v100_df = rename_models(df = nlp_base_v100_df)\n",
    "nlp_base_v100_df[\"gpu_count\"] = int(1)\n",
    "nlp_base_v100_df[\"name\"] = \"Baseline\"\n",
    "nlp_base_v100_df[\"gpu_type\"] = \"8xV100\"\n",
    "\n",
    "nlp_eu_1xt4_v100_df = get_hivemind_runs(run_names=[\"hivemind-160\"], drop_first_epoch=True)\n",
    "nlp_eu_1xt4_v100_df = rename_models(df = nlp_eu_1xt4_v100_df)\n",
    "nlp_eu_1xt4_v100_df[\"gpu_count\"] = int(2)\n",
    "nlp_eu_1xt4_v100_df[\"name\"] = \"F-A-1\"\n",
    "nlp_eu_1xt4_v100_df[\"gpu_type\"] = \"EU T4 (F-A)\"\n",
    "\n",
    "nlp_eu_2xt4_v100_df = get_hivemind_runs(run_names=[\"hivemind-158\"], drop_first_epoch=True)\n",
    "nlp_eu_2xt4_v100_df = rename_models(df = nlp_eu_2xt4_v100_df)\n",
    "nlp_eu_2xt4_v100_df[\"gpu_count\"] = int(3)\n",
    "nlp_eu_2xt4_v100_df[\"name\"] = \"F-A-2\"\n",
    "nlp_eu_2xt4_v100_df[\"gpu_type\"] = \"EU T4 (F-A)\"\n",
    "\n",
    "nlp_eu_4xt4_v100_df = get_hivemind_runs(run_names=[\"hivemind-156\"], drop_first_epoch=True)\n",
    "nlp_eu_4xt4_v100_df = rename_models(df = nlp_eu_4xt4_v100_df)\n",
    "nlp_eu_4xt4_v100_df[\"gpu_count\"] = int(5)\n",
    "nlp_eu_4xt4_v100_df[\"name\"] = \"F-A-4\"\n",
    "nlp_eu_4xt4_v100_df[\"gpu_type\"] = \"EU T4 (F-A)\"\n",
    "\n",
    "nlp_eu_8xt4_v100_df = get_hivemind_runs(run_names=[\"hivemind-154\"], drop_first_epoch=True)\n",
    "nlp_eu_8xt4_v100_df = rename_models(df = nlp_eu_8xt4_v100_df)\n",
    "nlp_eu_8xt4_v100_df[\"gpu_count\"] = int(9)\n",
    "nlp_eu_8xt4_v100_df[\"name\"] = \"F-A-8\"\n",
    "nlp_eu_8xt4_v100_df[\"gpu_type\"] = \"EU T4 (F-A)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "99f89c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "v100_eu_runs_df = pd.concat(objs=[\n",
    "    cv_base_v100_df,\n",
    "    cv_eu_1xt4_v100_df,\n",
    "    cv_eu_2xt4_v100_df,\n",
    "    cv_eu_4xt4_v100_df,\n",
    "    cv_eu_8xt4_v100_df,\n",
    "    nlp_base_v100_df,\n",
    "    nlp_eu_1xt4_v100_df,\n",
    "    nlp_eu_2xt4_v100_df,\n",
    "    nlp_eu_4xt4_v100_df,\n",
    "    nlp_eu_8xt4_v100_df\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b4b6853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_us_1xt4_v100_df = get_hivemind_runs(run_names=[\"hivemind-133\"], drop_first_epoch=True)\n",
    "cv_us_1xt4_v100_df = rename_models(df = cv_us_1xt4_v100_df)\n",
    "cv_us_1xt4_v100_df[\"gpu_count\"] = int(2)\n",
    "cv_us_1xt4_v100_df[\"name\"] = \"F-B-1\"\n",
    "cv_us_1xt4_v100_df[\"gpu_type\"] = \"US T4 (F-B)\"\n",
    "\n",
    "cv_us_2xt4_v100_df = get_hivemind_runs(run_names=[\"hivemind-131\"], drop_first_epoch=True)\n",
    "cv_us_2xt4_v100_df = rename_models(df = cv_us_2xt4_v100_df)\n",
    "cv_us_2xt4_v100_df[\"gpu_count\"] = int(3)\n",
    "cv_us_2xt4_v100_df[\"name\"] = \"F-B-2\"\n",
    "cv_us_2xt4_v100_df[\"gpu_type\"] = \"US T4 (F-B)\"\n",
    "\n",
    "cv_us_4xt4_v100_df = get_hivemind_runs(run_names=[\"hivemind-125\"], drop_first_epoch=True)\n",
    "cv_us_4xt4_v100_df = rename_models(df = cv_us_4xt4_v100_df)\n",
    "cv_us_4xt4_v100_df[\"gpu_count\"] = int(5)\n",
    "cv_us_4xt4_v100_df[\"name\"] = \"F-B-4\"\n",
    "cv_us_4xt4_v100_df[\"gpu_type\"] = \"US T4 (F-B)\"\n",
    "\n",
    "cv_us_8xt4_v100_df = get_hivemind_runs(run_names=[\"hivemind-123\"], drop_first_epoch=True)\n",
    "cv_us_8xt4_v100_df = rename_models(df = cv_us_8xt4_v100_df)\n",
    "cv_us_8xt4_v100_df[\"gpu_count\"] = int(9)\n",
    "cv_us_8xt4_v100_df[\"name\"] = \"F-B-8\"\n",
    "cv_us_8xt4_v100_df[\"gpu_type\"] = \"US T4 (F-B)\"\n",
    "\n",
    "nlp_us_1xt4_v100_df = get_hivemind_runs(run_names=[\"hivemind-134\"], drop_first_epoch=True)\n",
    "nlp_us_1xt4_v100_df = rename_models(df = nlp_us_1xt4_v100_df)\n",
    "nlp_us_1xt4_v100_df[\"gpu_count\"] = int(2)\n",
    "nlp_us_1xt4_v100_df[\"name\"] = \"F-B-1\"\n",
    "nlp_us_1xt4_v100_df[\"gpu_type\"] = \"US T4 (F-B)\"\n",
    "\n",
    "nlp_us_2xt4_v100_df = get_hivemind_runs(run_names=[\"hivemind-132\"], drop_first_epoch=True)\n",
    "nlp_us_2xt4_v100_df = rename_models(df = nlp_us_2xt4_v100_df)\n",
    "nlp_us_2xt4_v100_df[\"gpu_count\"] = int(3)\n",
    "nlp_us_2xt4_v100_df[\"name\"] = \"F-B-2\"\n",
    "nlp_us_2xt4_v100_df[\"gpu_type\"] = \"US T4 (F-B)\"\n",
    "\n",
    "nlp_us_4xt4_v100_df = get_hivemind_runs(run_names=[\"hivemind-126\"], drop_first_epoch=True)\n",
    "nlp_us_4xt4_v100_df = rename_models(df = nlp_us_4xt4_v100_df)\n",
    "nlp_us_4xt4_v100_df[\"gpu_count\"] = int(5)\n",
    "nlp_us_4xt4_v100_df[\"name\"] = \"F-B-4\"\n",
    "nlp_us_4xt4_v100_df[\"gpu_type\"] = \"US T4 (F-B)\"\n",
    "\n",
    "nlp_us_8xt4_v100_df = get_hivemind_runs(run_names=[\"hivemind-124\"], drop_first_epoch=True)\n",
    "nlp_us_8xt4_v100_df = rename_models(df = nlp_us_8xt4_v100_df)\n",
    "nlp_us_8xt4_v100_df[\"gpu_count\"] = int(9)\n",
    "nlp_us_8xt4_v100_df[\"name\"] = \"F-B-8\"\n",
    "nlp_us_8xt4_v100_df[\"gpu_type\"] = \"US T4 (F-B)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "16a4749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v100_us_runs_df = pd.concat(objs=[\n",
    "    cv_us_1xt4_v100_df,\n",
    "    cv_us_2xt4_v100_df,\n",
    "    cv_us_4xt4_v100_df,\n",
    "    cv_us_8xt4_v100_df,\n",
    "    nlp_us_1xt4_v100_df,\n",
    "    nlp_us_2xt4_v100_df,\n",
    "    nlp_us_4xt4_v100_df,\n",
    "    nlp_us_8xt4_v100_df\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cddf1acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_us_1xa10_v100_df = get_hivemind_runs(run_names=[\"hivemind-149\"], drop_first_epoch=True)\n",
    "cv_us_1xa10_v100_df = rename_models(df = cv_us_1xa10_v100_df)\n",
    "cv_us_1xa10_v100_df[\"gpu_count\"] = int(2)\n",
    "cv_us_1xa10_v100_df[\"gpu_type\"] = \"US A10 (F-C)\"\n",
    "cv_us_1xa10_v100_df[\"name\"] = \"F-C-1\"\n",
    "\n",
    "cv_us_2xa10_v100_df = get_hivemind_runs(run_names=[\"hivemind-145\"], drop_first_epoch=True)\n",
    "cv_us_2xa10_v100_df = rename_models(df = cv_us_2xa10_v100_df)\n",
    "cv_us_2xa10_v100_df[\"gpu_count\"] = int(3)\n",
    "cv_us_2xa10_v100_df[\"gpu_type\"] = \"US A10 (F-C)\"\n",
    "cv_us_2xa10_v100_df[\"name\"] = \"F-C-2\"\n",
    "\n",
    "cv_us_4xa10_v100_df = get_hivemind_runs(run_names=[\"hivemind-141\"], drop_first_epoch=True)\n",
    "cv_us_4xa10_v100_df = rename_models(df = cv_us_4xa10_v100_df)\n",
    "cv_us_4xa10_v100_df[\"gpu_count\"] = int(5)\n",
    "cv_us_4xa10_v100_df[\"gpu_type\"] = \"US A10 (F-C)\"\n",
    "cv_us_4xa10_v100_df[\"name\"] = \"F-C-4\"\n",
    "\n",
    "cv_us_8xa10_v100_df = get_hivemind_runs(run_names=[\"hivemind-137\"], drop_first_epoch=True)\n",
    "cv_us_8xa10_v100_df = rename_models(df = cv_us_8xa10_v100_df)\n",
    "cv_us_8xa10_v100_df[\"gpu_count\"] = int(9)\n",
    "cv_us_8xa10_v100_df[\"gpu_type\"] = \"US A10 (F-C)\"\n",
    "cv_us_8xa10_v100_df[\"name\"] = \"F-C-8\"\n",
    "\n",
    "nlp_us_1xa10_v100_df = get_hivemind_runs(run_names=[\"hivemind-150\"], drop_first_epoch=True)\n",
    "nlp_us_1xa10_v100_df = rename_models(df = nlp_us_1xa10_v100_df)\n",
    "nlp_us_1xa10_v100_df[\"gpu_count\"] = int(2)\n",
    "nlp_us_1xa10_v100_df[\"gpu_type\"] = \"US A10 (F-C)\"\n",
    "nlp_us_1xa10_v100_df[\"name\"] = \"F-C-1\"\n",
    "\n",
    "nlp_us_2xa10_v100_df = get_hivemind_runs(run_names=[\"hivemind-146\"], drop_first_epoch=True)\n",
    "nlp_us_2xa10_v100_df = rename_models(df = nlp_us_2xa10_v100_df)\n",
    "nlp_us_2xa10_v100_df[\"gpu_count\"] = int(3)\n",
    "nlp_us_2xa10_v100_df[\"gpu_type\"] = \"US A10 (F-C)\"\n",
    "nlp_us_2xa10_v100_df[\"name\"] = \"F-C-2\"\n",
    "\n",
    "nlp_us_4xa10_v100_df = get_hivemind_runs(run_names=[\"hivemind-142\"], drop_first_epoch=True)\n",
    "nlp_us_4xa10_v100_df = rename_models(df = nlp_us_4xa10_v100_df)\n",
    "nlp_us_4xa10_v100_df[\"gpu_count\"] = int(5)\n",
    "nlp_us_4xa10_v100_df[\"gpu_type\"] = \"US A10 (F-C)\"\n",
    "nlp_us_4xa10_v100_df[\"name\"] = \"F-C-4\"\n",
    "\n",
    "nlp_us_8xa10_v100_df = get_hivemind_runs(run_names=[\"hivemind-138\"], drop_first_epoch=True)\n",
    "nlp_us_8xa10_v100_df = rename_models(df = nlp_us_8xa10_v100_df)\n",
    "nlp_us_8xa10_v100_df[\"gpu_count\"] = int(9)\n",
    "nlp_us_8xa10_v100_df[\"gpu_type\"] = \"US A10 (F-C)\"\n",
    "nlp_us_8xa10_v100_df[\"name\"] = \"F-C-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "24adadb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "v100_us_a10_runs_df = pd.concat(objs=[\n",
    "    cv_us_1xa10_v100_df,\n",
    "    cv_us_2xa10_v100_df,\n",
    "    cv_us_4xa10_v100_df,\n",
    "    cv_us_8xa10_v100_df,\n",
    "    nlp_us_1xa10_v100_df,\n",
    "    nlp_us_2xa10_v100_df,\n",
    "    nlp_us_4xa10_v100_df,\n",
    "    nlp_us_8xa10_v100_df\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c5aba1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "v100_all_runs_df = pd.concat(objs=[v100_eu_runs_df, v100_us_runs_df, v100_us_a10_runs_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3150c764",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# filter local sps out\n",
    "temp_df = v100_all_runs_df.query(\"samples_per_sec.notna()\")\n",
    "temp_df = temp_df.query(\"model=='ConvNextLarge'\")\n",
    "\n",
    "sns.set(palette=cv_palette)\n",
    "plt.figure(figsize=(8,3))\n",
    "ax = sns.lineplot(\n",
    "    data=temp_df,\n",
    "    x=\"gpu_count\", y=\"samples_per_sec\", hue=\"gpu_type\", style=\"gpu_type\",\n",
    "    errorbar=('se', 1), dashes=True,\n",
    "    markers=True,\n",
    "    linewidth=1.0,\n",
    "    markersize=10.0\n",
    ")\n",
    "#plt.title(\"Research Setting\")\n",
    "plt.xlabel(\"GPU Count\")\n",
    "plt.ylim(0,900)\n",
    "plt.ylabel(\"Samples per Second\")\n",
    "plt.legend(title=\"GPU Type\")\n",
    "ax.axhline(412, color=\"#ffe793\", linestyle=\"-\")\n",
    "#plt.xlim(0,10)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(0.1, 1.01))\n",
    "save_fig(\"cv-research-setting-performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b1e1b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter local sps out\n",
    "temp_df = v100_all_runs_df.query(\"samples_per_sec.notna()\")\n",
    "# only show NLP\n",
    "temp_df = temp_df.query(\"model=='RoBERTaXLM'\")\n",
    "\n",
    "sns.set(palette=nlp_ext_palette)\n",
    "plt.figure(figsize=(8,3))\n",
    "ax = sns.lineplot(\n",
    "    data=temp_df,\n",
    "    x=\"gpu_count\", y=\"samples_per_sec\", hue=\"gpu_type\", style=\"gpu_type\",\n",
    "    errorbar=('se', 1), dashes=True,\n",
    "    markers=True,\n",
    "    linewidth=1.0,\n",
    "    markersize=10.0\n",
    ")\n",
    "#plt.title(\"Research Setting\")\n",
    "plt.xlabel(\"GPU Count\")\n",
    "plt.ylabel(\"Samples per Second\")\n",
    "#plt.ylim(0,600)\n",
    "#plt.xlim(0,10)\n",
    "plt.legend(title=\"GPU Type\")\n",
    "ax.axhline(1812, color=\"#d6efb3\", linestyle=\"-\")\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(0.1, 0.95))\n",
    "save_fig(\"nlp-research-setting-performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fe643c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_throughput(model, gpu_count, df, gpu_type):\n",
    "    TBS = 32768\n",
    "    temp_df = df.query(f\"samples_per_sec.notna() and TBS=={TBS} and model=='{model}' and gpu_count=={gpu_count} and gpu_type=='{gpu_type}'\")\n",
    "    return temp_df[\"samples_per_sec\"].mean()\n",
    "\n",
    "for model in v100_all_runs_df[\"model\"].unique():\n",
    "    for gpu_type in [\"EU T4 (F-A)\", \"US T4 (F-B)\", \"US A10 (F-C)\"]:\n",
    "        print(f\"Model: {model}, Type: {gpu_type}\")\n",
    "        baseline_sps = get_mean_throughput(model=model, gpu_count=1, df=v100_all_runs_df, gpu_type=\"8xV100\")\n",
    "        print(f\"   1 GPU, SPS: {round(baseline_sps,2):>6}\")\n",
    "        for gpu_count in [2,3,5,9]:\n",
    "            sps = get_mean_throughput(model=model, gpu_count=gpu_count, df=v100_all_runs_df, gpu_type=gpu_type)\n",
    "            norm_sps = sps / gpu_count\n",
    "            speedup = sps / baseline_sps\n",
    "            norm_speedup = speedup / gpu_count\n",
    "            \n",
    "            print(f\"  {gpu_count} GPUs, SPS: {round(sps,2):>6}, Norm SPS: {round(norm_sps,2):>6}, Speedup: {round(speedup,2):>6}, Norm Speedup: {round(norm_speedup,2):>6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cbfb041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "cv_eu_1xt4_v100_gran_df   = get_granularity_cumulated(run_name=\"hivemind-159\", name=\"F-A-1\", debug=debug)\n",
    "cv_eu_2xt4_v100_gran_df   = get_granularity_cumulated(run_name=\"hivemind-157\", name=\"F-A-2\", debug=debug)\n",
    "cv_eu_4xt4_v100_gran_df   = get_granularity_cumulated(run_name=\"hivemind-155\", name=\"F-A-4\", debug=debug)\n",
    "cv_eu_8xt4_v100_gran_df   = get_granularity_cumulated(run_name=\"hivemind-153\", name=\"F-A-8\", debug=debug)\n",
    "nlp_eu_1xt4_v100_gran_df  = get_granularity_cumulated(run_name=\"hivemind-160\", name=\"F-A-1\", debug=debug)\n",
    "nlp_eu_2xt4_v100_gran_df  = get_granularity_cumulated(run_name=\"hivemind-158\", name=\"F-A-2\", debug=debug)\n",
    "nlp_eu_4xt4_v100_gran_df  = get_granularity_cumulated(run_name=\"hivemind-156\", name=\"F-A-4\", debug=debug)\n",
    "nlp_eu_8xt4_v100_gran_df  = get_granularity_cumulated(run_name=\"hivemind-154\", name=\"F-A-8\", debug=debug)\n",
    "cv_us_1xt4_v100_gran_df   = get_granularity_cumulated(run_name=\"hivemind-133\", name=\"F-B-1\", debug=debug)\n",
    "cv_us_2xt4_v100_gran_df   = get_granularity_cumulated(run_name=\"hivemind-131\", name=\"F-B-2\", debug=debug)\n",
    "cv_us_4xt4_v100_gran_df   = get_granularity_cumulated(run_name=\"hivemind-125\", name=\"F-B-4\", debug=debug)\n",
    "cv_us_8xt4_v100_gran_df   = get_granularity_cumulated(run_name=\"hivemind-123\", name=\"F-B-8\", debug=debug)\n",
    "nlp_us_1xt4_v100_gran_df  = get_granularity_cumulated(run_name=\"hivemind-134\", name=\"F-B-1\", debug=debug)\n",
    "nlp_us_2xt4_v100_gran_df  = get_granularity_cumulated(run_name=\"hivemind-132\", name=\"F-B-2\", debug=debug)\n",
    "nlp_us_4xt4_v100_gran_df  = get_granularity_cumulated(run_name=\"hivemind-126\", name=\"F-B-4\", debug=debug)\n",
    "nlp_us_8xt4_v100_gran_df  = get_granularity_cumulated(run_name=\"hivemind-124\", name=\"F-B-8\", debug=debug)\n",
    "cv_us_1xa10_v100_gran_df  = get_granularity_cumulated(run_name=\"hivemind-149\", name=\"F-C-1\", debug=debug)\n",
    "cv_us_2xa10_v100_gran_df  = get_granularity_cumulated(run_name=\"hivemind-145\", name=\"F-C-2\", debug=debug)\n",
    "cv_us_4xa10_v100_gran_df  = get_granularity_cumulated(run_name=\"hivemind-141\", name=\"F-C-4\", debug=debug)\n",
    "cv_us_8xa10_v100_gran_df  = get_granularity_cumulated(run_name=\"hivemind-137\", name=\"F-C-8\", debug=debug)\n",
    "nlp_us_1xa10_v100_gran_df = get_granularity_cumulated(run_name=\"hivemind-150\", name=\"F-C-1\", debug=debug)\n",
    "nlp_us_2xa10_v100_gran_df = get_granularity_cumulated(run_name=\"hivemind-146\", name=\"F-C-2\", debug=debug)\n",
    "nlp_us_4xa10_v100_gran_df = get_granularity_cumulated(run_name=\"hivemind-142\", name=\"F-C-4\", debug=debug)\n",
    "nlp_us_8xa10_v100_gran_df = get_granularity_cumulated(run_name=\"hivemind-138\", name=\"F-C-8\", debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "04b638d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_research_cloud_granularity_df = pd.concat(objs=[\n",
    "    cv_eu_1xt4_v100_gran_df,\n",
    "    cv_eu_2xt4_v100_gran_df,\n",
    "    cv_eu_4xt4_v100_gran_df,\n",
    "    cv_eu_8xt4_v100_gran_df,\n",
    "    cv_us_1xt4_v100_gran_df,\n",
    "    cv_us_2xt4_v100_gran_df,\n",
    "    cv_us_4xt4_v100_gran_df,\n",
    "    cv_us_8xt4_v100_gran_df,\n",
    "    cv_us_1xa10_v100_gran_df,\n",
    "    cv_us_2xa10_v100_gran_df,\n",
    "    cv_us_4xa10_v100_gran_df,\n",
    "    cv_us_8xa10_v100_gran_df\n",
    "])\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "plt.figure(figsize=(8,3))\n",
    "ax = cv_research_cloud_granularity_df[[\"calc_time_s\",\"comm_time_s\",\"name\"]].set_index('name').plot(kind='bar', stacked=True, figsize=(8,3))\n",
    "plt.ylabel(\"Time in Seconds\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylim(0,650)\n",
    "plt.legend(labels=[\"Calculation\", \"Communication\"])\n",
    "ax.axvline(3.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(7.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "show_granulartiy_values(ax=ax, df=cv_research_cloud_granularity_df)\n",
    "save_fig(\"cv-research-cloud-performance-granularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b56e50a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_research_cloud_granularity_df = pd.concat(objs=[\n",
    "    nlp_eu_1xt4_v100_gran_df,\n",
    "    nlp_eu_2xt4_v100_gran_df,\n",
    "    nlp_eu_4xt4_v100_gran_df,\n",
    "    nlp_eu_8xt4_v100_gran_df,\n",
    "    nlp_us_1xt4_v100_gran_df,\n",
    "    nlp_us_2xt4_v100_gran_df,\n",
    "    nlp_us_4xt4_v100_gran_df,\n",
    "    nlp_us_8xt4_v100_gran_df,\n",
    "    nlp_us_1xa10_v100_gran_df,\n",
    "    nlp_us_2xa10_v100_gran_df,\n",
    "    nlp_us_4xa10_v100_gran_df,\n",
    "    nlp_us_8xa10_v100_gran_df,\n",
    "])\n",
    "\n",
    "sns.set(palette=granularity_palette)\n",
    "plt.figure(figsize=(8,3))\n",
    "ax = nlp_research_cloud_granularity_df[[\"calc_time_s\",\"comm_time_s\",\"name\"]].set_index('name').plot(kind='bar', stacked=True, figsize=(8,3))\n",
    "plt.ylabel(\"Time in Seconds\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylim(0,650)\n",
    "plt.legend(labels=[\"Calculation\", \"Communication\"])\n",
    "ax.axvline(3.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "ax.axvline(7.5, color=\"#6c6c6c\", linestyle=\"--\")\n",
    "show_granulartiy_values(ax=ax, df=nlp_research_cloud_granularity_df)\n",
    "save_fig(\"nlp-research-cloud-performance-granularity\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
